This file is a merged representation of the entire codebase, combined into a single document by Repomix.

# File Summary

## Purpose
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

## Additional Info

# Directory Structure
```
.github/
  workflows/
    workflow.yml
  FUNDING.yml
bin/
  feeds.py
  optimize.py
  versions.py
docs/
  services/
    adguard-home.md
    apisix-dashboard.md
    apisix-gateway.md
    appsmith.md
    authelia.md
    authentik.md
    auxiliary.md
    backup-borg.md
    calibre-web.md
    changedetection.md
    clickhouse.md
    collabora-online.md
    couchdb.md
    docker-registry-browser.md
    docker-registry-proxy.md
    docker-registry-purger.md
    docker-registry.md
    docker.md
    dokuwiki.md
    echoip.md
    endlessh.md
    etcd.md
    etherpad.md
    exim-relay.md
    firezone.md
    focalboard.md
    forgejo-runner.md
    forgejo.md
    freescout.md
    freshrss.md
    funkwhale.md
    gitea.md
    gotosocial.md
    grafana-loki.md
    grafana.md
    headscale.md
    healthchecks.md
    hubsite.md
    ilmo.md
    infisical.md
    influxdb.md
    jackett.md
    jitsi.md
    keycloak.md
    keydb.md
    labelstudio.md
    lago.md
    languagetool.md
    linkding.md
    mariadb.md
    matterbridge.md
    miniflux.md
    mobilizon.md
    mongodb.md
    mosquitto.md
    mrs.md
    n8n.md
    navidrome.md
    neko.md
    netbox.md
    nextcloud.md
    notfellchen.md
    ntfy.md
    oauth2-proxy.md
    outline.md
    overseerr.md
    owncast.md
    oxitraffic.md
    paperless-ngx.md
    peertube.md
    plausible.md
    postgis.md
    postgres-backup.md
    postgres.md
    prometheus-blackbox-exporter.md
    prometheus-node-exporter.md
    prometheus-postgres-exporter.md
    prometheus-ssh-exporter.md
    prometheus.md
    promtail.md
    qbittorrent.md
    radarr.md
    radicale.md
    readeck.md
    redis.md
    redmine.md
    roundcube.md
    rumqttd.md
    searxng.md
    semaphore.md
    soft-serve.md
    sonarr.md
    stirling-pdf.md
    syncthing.md
    system.md
    tandoor.md
    telegraf.md
    traefik.md
    tsdproxy.md
    uptime-kuma.md
    valkey.md
    vaultwarden.md
    versatiles.md
    wetty.md
    wg-easy.md
    woodpecker-ci.md
    wordpress.md
    writefreely.md
  alternative-architectures.md
  ansible.md
  configuring-dns.md
  configuring-ipv6.md
  configuring-playbook.md
  developer-documentation.md
  getting-the-playbook.md
  installing.md
  interoperability.md
  just.md
  maintenance-and-troubleshooting.md
  maintenance-upgrading-services.md
  playbook-tags.md
  prerequisites.md
  README.md
  running-multiple-instances.md
  self-building.md
  setting-up-services-on-mdad-server.md
  supported-services.md
  uninstalling.md
examples/
  mash-for-matrix-docker-ansible-deploy-users/
    vars.yml
  hosts
  vars.yml
roles/
  mash/
    playbook_base/
      defaults/
        main.yml
      tasks/
        main.yml
        setup_base_dir.yml
        setup_user.yml
        validate_config.yml
    playbook_migration/
      defaults/
        main.yml
      tasks/
        debian_docker_trusted_gpg_d_migration_migration.yml
        docker_daemon_options_file_cleanup.yml
        main.yml
        validate_config.yml
templates/
  group_vars_mash_servers
  requirements.yml
  setup.yml
.dockerignore
.editorconfig
.gitignore
ansible.cfg
CHANGELOG.md
Dockerfile
justfile
LICENSE
README.md
releases.opml
VERSIONS.md
```

# Files

## File: .github/workflows/workflow.yml
````yaml
name: CI (main and tags)
on:
  push:
    branches: [ "main" ]
    tags: [ "v*" ]
permissions:
  checks: write
  contents: write
  packages: write
  pull-requests: read
jobs:
  build-publish:
    name: Build and Publish
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Login to ghcr.io
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=raw,value=latest,enable=${{ github.ref_name == 'main' }}
            type=semver,pattern={{raw}}
      - name: Build and push
        uses: docker/build-push-action@v6
        with:
          platforms: linux/amd64,linux/arm64
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
````

## File: .github/FUNDING.yml
````yaml
---
# These are supported funding model platforms

# https://liberapay.com/mother-of-all-self-hosting/
liberapay: mother-of-all-self-hosting
````

## File: bin/feeds.py
````python
import os
import sys
import argparse
from urllib.parse import urlparse
import xml.etree.ElementTree as ET

parser = argparse.ArgumentParser(description='Extracts release feeds from roles')
parser.add_argument('root_dir', help='Root dir which to traverse recursively for defaults/main.yml roles files')
parser.add_argument('action', help='Pass "check" to list roles with missing feeds or "dump" to dump an OPML file')
args = parser.parse_args()
if args.action not in ['check', 'dump']:
    sys.exit('Error: possible arguments are "check" or "dump"')

excluded_paths = [
    # appservice-kakaotalk defines a Project URL, but that Gitea repository does not have an Atom/RSS feed.
    # It doesn't have any tags anyway.
    './upstream/roles/custom/matrix-bridge-appservice-kakaotalk/defaults',
]
project_source_url_str = '# Project source code URL:'

def get_roles_files_from_dir(root_dir):
    file_paths = []
    for dir_name, sub_dur_list, file_list in os.walk(root_dir):
        for file_name in file_list:
            if not dir_name.endswith('defaults') or file_name != 'main.yml':
                continue
            if dir_name in excluded_paths:
                continue
            file_paths.append(os.path.join(dir_name, file_name))
    return file_paths

def get_git_repos_from_files(file_paths, break_on_missing_repos=False):
    git_repos = {}
    missing_repos = []

    for file in file_paths:
        file_lines = open(file, 'r').readlines()
        found_project_repo = False
        for line in file_lines:
            project_repo_val = ''
            if project_source_url_str in line:
                # extract the value from a line like this:
                # Project source code URL: https://github.com/mautrix/signal
                project_repo_val = line.split(project_source_url_str)[1].strip()
                if not validate_url(project_repo_val):
                    print('Invalid url for line ', line)
                    break
            if project_repo_val != '':
                if file not in git_repos:
                    git_repos[file] = []

                git_repos[file].append(project_repo_val)
                found_project_repo = True

        if not found_project_repo:
            missing_repos.append(file)

    if break_on_missing_repos and len(missing_repos) > 0:
        print('Missing `{0}` comment for:\n{1}'.format(project_source_url_str, '\n'.join(missing_repos)))

    return git_repos

def validate_url(text):
    if text == '':
        return False
    try:
        result = urlparse(text)
        return all([result.scheme, result.netloc])
    except:
        return False


def format_feeds_from_git_repos(git_repos):
    feeds = {}
    for role, git_repos in git_repos.items():
        for idx, git_repo in enumerate(git_repos):
            if 'github' in git_repo:
                atomFilePath = git_repo.replace('.git', '') + '/releases.atom'
            elif ('gitlab' in git_repo or 'mau.dev' in git_repo):
                atomFilePath = git_repo.replace('.git', '') + '/-/tags?format=atom'
            elif 'git.zx2c4.com' in git_repo: # cgit
                atomFilePath = git_repo + '/atom/'
            elif 'framagit.org' in git_repo: # gitlab
                atomFilePath = git_repo.replace('.git', '') + '/-/tags?format=atom'
            elif 'git.osgeo.org' in git_repo: # gitea
                atomFilePath = git_repo.replace('.git', '') + '.atom'
            elif 'dev.funkwhale.audio' in git_repo: # gitlab
                atomFilePath = git_repo.replace('.git', '') + '/-/tags?format=atom'
            elif 'codeberg.org' in git_repo:
                atomFilePath = git_repo.replace('.git', '') + '/releases.atom'
            elif 'code.forgejo.org' in git_repo:
                atomFilePath = git_repo.replace('.git', '') + '/releases.atom'
            else:
                print('Unrecognized git repository: %s' % git_repo)
                continue

            role_name = role.split('/')[4]
            if role_name == 'defaults':
                role_name = role.split('/')[3]
            role_name = role_name.removeprefix('matrix-bot-').removeprefix('matrix-bridge-').removeprefix('matrix-client-').removeprefix('matrix-')
            if idx > 0:
                # there is more than 1 project source code for this role
                role_name += '-' + str(idx+1)

            feeds[role_name] = {
                'text': role_name,
                'title': role_name,
                'type': 'rss',
                'htmlUrl': git_repo,
                'xmlUrl': atomFilePath
            }

    feeds = {key: val for key, val in sorted(feeds.items(), key = lambda item: item[0])}
    return feeds

def dump_opml_file_from_feeds(feeds):
    tree = ET.ElementTree('tree')

    opml = ET.Element('opml', {'version': '1.0'})
    head = ET.SubElement(opml, 'head')

    title = ET.SubElement(head, 'title')
    title.text = 'Release feeds for roles'

    body = ET.SubElement(opml, 'body')
    for role, feed_dict in feeds.items():
        outline = ET.SubElement(body, 'outline', feed_dict)

    ET.indent(opml)
    tree._setroot(opml)
    file_name = 'releases.opml'
    tree.write(file_name, encoding = 'UTF-8', xml_declaration = True)
    print('Generated %s' % file_name)

if __name__ == '__main__':
    file_paths = get_roles_files_from_dir(root_dir=args.root_dir)
    break_on_missing = args.action == 'check'
    git_repos = get_git_repos_from_files(file_paths=file_paths, break_on_missing_repos=break_on_missing)
    feeds = format_feeds_from_git_repos(git_repos)

    if args.action == 'dump':
        dump_opml_file_from_feeds(feeds)
````

## File: bin/optimize.py
````python
import argparse
import regex
import sys
import yaml

parser = argparse.ArgumentParser(description='Optimizes the playbook based on enabled components found in vars.yml files')
parser.add_argument('--vars-paths', help='Path to vars.yml configuration files to process', required=True)
parser.add_argument('--src-requirements-yml-path', help='Path to source requirements.yml file with all role definitions', required=True)
parser.add_argument('--src-setup-yml-path', help='Path to source setup.yml file', required=True)
parser.add_argument('--src-group-vars-yml-path', help='Path to source group vars file', required=True)
parser.add_argument('--dst-requirements-yml-path', help='Path to destination requirements.yml file, where role definitions will be saved', required=True)
parser.add_argument('--dst-setup-yml-path', help='Path to destination setup.yml file', required=True)
parser.add_argument('--dst-group-vars-yml-path', help='Path to destination group vars file', required=True)

args = parser.parse_args()

def load_combined_variable_names_from_files(vars_yml_file_paths):
    variable_names = set({})
    for vars_path in vars_yml_file_paths:
        with open(vars_path, 'r') as file:
            yaml_data = yaml.safe_load(file)

            variable_names = variable_names | set(yaml_data.keys())
    return variable_names

def load_yaml_file(path):
    with open(path, 'r') as file:
        return yaml.safe_load(file)

def is_role_definition_in_use(role_definition, used_variable_names):
    for variable_name in used_variable_names:
        if 'activation_prefix' in role_definition:
            if role_definition['activation_prefix'] == '':
                # Special value indicating "always activate".
                # We don't really need this dedicated if, but it's more obvious with it.
                return True
            if variable_name.startswith(role_definition['activation_prefix']):
                return True
    return False

def write_yaml_to_file(definitions, path):
    with open(path, 'w') as file:
        yaml.dump(definitions, file)

def read_file(path):
    with open(path, 'r') as file:
        return file.read()

def write_to_file(contents, path):
    with open(path, 'w') as file:
        file.write(contents)

# Matches the beginning of role-specific blocks.
# Example: `# role-specific:playbook_help`
regex_role_specific_block_start = regex.compile('^\\s*#\\s*role-specific:\\s*([^\\s]+)$')

# Matches the end of role-specific blocks.
# Example: `# /role-specific:playbook_help`
regex_role_specific_block_end = regex.compile('^\\s*#\\s*/role-specific:\\s*([^\\s]+)$')

def process_file_contents(file_name, enabled_role_names, known_role_names):
    contents = read_file(file_name)

    lines_preserved = []
    role_specific_stack = []

    for line_number, line in enumerate(contents.split("\n")):
        # Stage 1: looking for a role-specific starting block
        start_role_matches = regex_role_specific_block_start.match(line)
        if start_role_matches is not None:
            role_name = start_role_matches.group(1)
            if role_name not in known_role_names:
                raise Exception('Found start block for role {0} on line {1} in file {2}, but it is not a known role name found among: {3}'.format(
                    role_name,
                    line_number,
                    file_name,
                    known_role_names,
                ))
            role_specific_stack.append(role_name)
            continue

        # Stage 2: looking for role-specific closing blocks
        end_role_matches = regex_role_specific_block_end.match(line)
        if end_role_matches is not None:
            role_name = end_role_matches.group(1)
            if role_name not in known_role_names:
                raise Exception('Found end block for role {0} on line {1} in file {2}, but it is not a known role name found among: {3}'.format(
                    role_name,
                    line_number,
                    file_name,
                    known_role_names,
                ))

            if len(role_specific_stack) == 0:
                raise Exception('Found end block for role {0} on line {1} in file {2}, but there is no opening statement for it'.format(
                    role_name,
                    line_number,
                    file_name,
                ))

            last_role_name = role_specific_stack[len(role_specific_stack) - 1]
            if role_name != last_role_name:
                raise Exception('Found end block for role {0} on line {1} in file {2}, but the last starting block was for role {3}'.format(
                    role_name,
                    line_number,
                    file_name,
                    last_role_name,
                ))

            role_specific_stack.pop()

            continue

        # Stage 3: regular line
        all_roles_allowed = True
        for role_name in role_specific_stack:
            if role_name not in enabled_role_names:
                all_roles_allowed = False
                break

        if all_roles_allowed:
            lines_preserved.append(line)

    if len(role_specific_stack) != 0:
        raise Exception('Expected one or more closing block for role-specific tags in file {0}: {1}'.format(file_name, role_specific_stack))

    lines_final = []
    sequential_blank_lines_count = 0
    for line in lines_preserved:
        if line != "":
            lines_final.append(line)
            sequential_blank_lines_count = 0
            continue

        if sequential_blank_lines_count <= 1:
            lines_final.append(line)
            sequential_blank_lines_count += 1
            continue

    return "\n".join(lines_final)

vars_paths = args.vars_paths.split(' ')
used_variable_names = load_combined_variable_names_from_files(vars_paths)

all_role_definitions = load_yaml_file(args.src_requirements_yml_path)

enabled_role_definitions = []
for role_definition in all_role_definitions:
    if 'name' not in role_definition:
        raise Exception('Role definition does not have a name and should be adjusted to have one: {0}'.format(role_definition))
    if is_role_definition_in_use(role_definition, used_variable_names):
        enabled_role_definitions.append(role_definition)

write_yaml_to_file(enabled_role_definitions, args.dst_requirements_yml_path)

known_role_names = tuple(map(lambda definition: definition['name'], all_role_definitions))
enabled_role_names = tuple(map(lambda definition: definition['name'], enabled_role_definitions))

setup_yml_processed = process_file_contents(args.src_setup_yml_path, enabled_role_names, known_role_names)
write_to_file(setup_yml_processed, args.dst_setup_yml_path)

group_vars_yml_processed = process_file_contents(args.src_group_vars_yml_path, enabled_role_names, known_role_names)
write_to_file(group_vars_yml_processed, args.dst_group_vars_yml_path)
````

## File: bin/versions.py
````python
#!/usr/bin/env python3

import os
import re
import yaml

ignored = [
    'matrix_synapse_default_room_version',
]
prefixes = [
    'matrix_',
    'custom_',
    'int_',
    'synapse_default_',
    'synapse_ext_',
    'mailer_container_',
    'bot_',
    'client_',
    'mautrix_',
    'devture_',
    'beeper_',
    'backup_borg_',
]
suffixes = [
    '_version',
]


def find_versions():
    matches = {}
    for root, dirs, files in os.walk('.'):
        if root.endswith('defaults'):
            for file in files:
                if file.endswith('main.yml'):
                    path = os.path.join(root, file)
                    with open(path, 'r') as f:
                        data = yaml.safe_load(f)
                        for key, value in data.items():
                            if key.endswith('_version') and value and not re.search(r'{{|master|main|""', str(value)) and key not in ignored:
                                sanitized_key = sanitize_key(key)
                                matches[sanitized_key] = value
    return matches


def sanitize_key(key):
    for prefix in prefixes:
        key = key.removeprefix(prefix)
    for suffix in suffixes:
        key = key.removesuffix(suffix)
    return key.replace('_', ' ').title()


def generate_versions():
    versions = find_versions()
    with open(os.path.join(os.getcwd(), 'VERSIONS.md'), 'w') as f:
        for key, value in sorted(versions.items()):
            f.write(f'* {key}: {value}\n')


if __name__ == "__main__":
    generate_versions()
````

## File: docs/services/adguard-home.md
````markdown
# AdGuard Home

[AdGuard Home](https://adguard.com/en/adguard-home/overview.html/) is a network-wide DNS software for blocking ads & tracking.

> [!WARNING]
> Running a public DNS server is not advisable. You'd better install AdGuard Home in a trusted local network, or adjust its network interfaces and port exposure (via the variables in the [Networking](#networking) configuration section below) so that you don't expose your DNS server publicly to the whole world. If you're exposing your DNS server publicly, consider restricting who can use it by adjusting the **Allowed clients** setting in the **Access settings** section of **Settings** -> **DNS settings**.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# adguard-home                                                         #
#                                                                      #
########################################################################

adguard_home_enabled: true

adguard_home_hostname: mash.example.com

# Hosting under a subpath sort of works, but is not ideal
# (see the URL section below for details).
# Consider using a dedicated hostname and removing the line below.
adguard_home_path_prefix: /adguard-home

########################################################################
#                                                                      #
# /adguard-home                                                        #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/adguard-home`.

You can remove the `adguard_home_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

When **hosting under a subpath**, you may hit [this bug](https://github.com/AdguardTeam/AdGuardHome/issues/5478), which causes these **annoyances**:

- upon initial usage, you will be redirected to `/install.html` and would need to manually adjust this URL to something like `/adguard-home/install.html` (depending on your `adguard_home_path_prefix`). After the installation wizard completes, you'd be redirected to `/index.html` incorrectly as well.

- every time you hit the homepage and you're not logged in, you will be redirected to `/login.html` and would need to manually adjust this URL to something like `/adguard-home/login.html` (depending on your `adguard_home_path_prefix`)


### Networking

By default, the following ports will be exposed by the container on **all network interfaces**:

- `53` over **TCP**, controlled by `adguard_home_container_dns_tcp_bind_port` — used for DNS over TCP
- `53` over **UDP**, controlled by `adguard_home_container_dns_udp_bind_port` — used for DNS over UDP

Docker automatically opens these ports in the server's firewall, so you **likely don't need to do anything**. If you use another firewall in front of the server, you may need to adjust it.

To expose these ports only on **some** network interfaces, you can use additional configuration like this:

```yaml
# Expose only on 192.168.1.15
adguard_home_container_dns_tcp_bind_port: '192.168.1.15:53'
adguard_home_container_dns_udp_bind_port: '192.168.1.15:53'
```

## Usage

After installation, you can go to the AdGuard Home URL, as defined in `adguard_home_hostname` and `adguard_home_path_prefix`.

As mentioned in the [URL](#url) section above, you may hit some annoyances when hosting under a subpath.

The first time you visit the AdGuard Home pages, you'll go through a setup wizard **make sure to set the HTTP port under "Admin Web Interface" to `3000`**. This is the in-container port that our Traefik setup expects and uses for serving the install wizard to begin with. If you go with the default (`80`), the web UI will stop working after the installation wizard completes.

Things you should consider doing later:

- increasing the per-client Rate Limit (from the default of `20`) in the **DNS server configuration** section in **Settings** -> **DNS Settings**
- enabling caching in the **DNS cache configuration** section in **Settings** -> **DNS Settings**
- adding additional blocklists by discovering them on [Firebog](https://firebog.net/) or other sources and importing them from **Filters** -> **DNS blocklists**
- reading the AdGuard Home [README](https://github.com/AdguardTeam/AdGuardHome/blob/master/README.md) and [Wiki](https://github.com/AdguardTeam/AdGuardHome/wiki)


## Troubleshooting and workaround

Adguard Home does not currently support being setup with a non-`root` account (see [issue](https://github.com/AdguardTeam/AdGuardHome/issues/4714)). As the playbook uses the user `mash` when starting services, you will likely encounter the following error when `adguard-home.service` tries to start for the first time:

```
mar 02 19:11:59 $hostname mash-adguard-home[872496]: 2024/03/02 18:11:59.706251 [info] Checking if AdGuard Home has necessary permissions
mar 02 19:11:59 $hostname mash-adguard-home[872496]: 2024/03/02 18:11:59.706257 [fatal] This is the first launch of AdGuard Home. You must run it as Administrator.
```

You can workaround this issue by editing `mash-adguard-home.service` and temporarily make it start Adguard Home as the `root` user for the first time, and then revert it back to using a regular user afterwards. Follow the steps below, which require you to be `root` to execute the commands:

1. Run `systemctl edit --full mash-adguard-home.service` to edit Adguard Home's service file and remove or comment out the line starting with `--user` (e.g. `--user=996:3992 \` — the numbers represent the uid/gid of the `mash` user, so your values may be different):

	```
	ExecStartPre=/usr/bin/env docker create \
	                        --rm \
	                        --name=mash-adguard-home \
	                        --log-driver=none \
	                        --user=996:3992 \  <--- remove temporarily
	```

2. Run `systemctl restart mash-adguard-home.service` to restart the service.
3. Perform the first time setup as documented under [usage](https://github.com/mother-of-all-self-hosting/mash-playbook/blob/main/docs/services/adguard-home.md#usage).
4. Run `systemctl stop mash-adguard-home.service` to stop the service.
5. Run `chown -R mash:mash /mash/adguard-home/workdir` to change ownership of the files created during the first-time setup from `root` to `mash`. Optionally, use `ls -ll /mash/adguard-home/workdir` to check the file ownership before and after running `chown`.
6. Run the playbook again to rebuild `/etc/systemd/system/mash-adguard-home.service` and start AdGuard Home again: `just install-service adguard-home.service`.
7. If you didn't get any errors, Adguard Home should be running correctly. You can also check on the service with: `journalctl -fu mash-adguard-home.service`.
````

## File: docs/services/apisix-dashboard.md
````markdown
# APISIX Dashboard

[APISIX Dashboard](https://apisix.apache.org/docs/dashboard/USER_GUIDE/) is a web UI for [APISIX Gateway](./apisix-gateway.md).

It works by directly editing the [etcd](./etcd.md) database that APISIX Gateway stores its data in.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server
- an [etcd](etcd.md) key-value store
- (optional) [APISIX Gateway](./apisix-gateway.md) — there's no point in administrating APISIX Gateway configuration stored in etcd without having an APISIX Gateway instance to initialize and consume it


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# apisix_dashboard                                                     #
#                                                                      #
########################################################################

apisix_dashboard_enabled: true

apisix_dashboard_hostname: dashboard.api.example.com

# A strong secret for JWT authentication
apisix_dashboard_config_authentication_secret: ''

apisix_dashboard_config_authentication_users:
  - username: admin
    password: password-here

########################################################################
#                                                                      #
# /apisix_dashboard                                                    #
#                                                                      #
########################################################################
```

If you'd like to do something more advanced, the [`ansible-role-apisix-dashboard` Ansible role](https://github.com/mother-of-all-self-hosting/ansible-role-apisix-dashboard) is very configurable and should not get in your way of exposing ports or configuring arbitrary settings.

Take a look at [its `default/main.yml` file](https://github.com/mother-of-all-self-hosting/ansible-role-apisix-dashboard/blob/main/defaults/main.yml) for available Ansible variables you can use in your own `vars.yml` configuration file.

### URL

In the example configuration above, we configure APISIX Dashboard to expose itself at: `https://dashboard.api.example.com`

### Authentication

The example above uses the built-in login page of APISIX Dashboard with a list of users is defined via `apisix_dashboard_config_authentication_users`.

APISIX Dashboard also supports OpenID Connect providers. It can be enabled and configured via various `apisix_dashboard_config_oidc_*` Ansible variables.


## Usage

After installation, you can visit the APISIX Dashboard URL and authenticate with a credential as specified in `apisix_dashboard_config_authentication_users`. If you've enabled OpenID Connect, you may also be able to authenticate with that.
````

## File: docs/services/apisix-gateway.md
````markdown
# APISIX Gateway

[APISIX Gateway](https://apisix.apache.org/docs/apisix/getting-started/README/) is an [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) and Ingress Controller.

APISIX Gateway has a complex [architecture](https://apisix.apache.org/docs/apisix/architecture-design/apisix/) in which APISIX can serve multiple roles (data plane, control plane). There are different [deployment modes](https://apisix.apache.org/docs/apisix/deployment-modes/) for achieving a more decoupled setup.

What we're configuring here is a `traditional` deployment in which one APISIX instance acts as both the data plane and the control plane.
By tweaking the configuration, you may be able to install multiple instances (on separate machines), each serving a different role. This is beyond the scope of this documentation page.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server
- an [etcd](etcd.md) key-value store


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# apisix_gateway                                                       #
#                                                                      #
########################################################################

apisix_gateway_enabled: true

# Configure the hostname and path at which the API would be exposed
apisix_gateway_hostname: api.example.com
apisix_gateway_path_prefix: /api

apisix_gateway_config_deployment_admin_admin_key:
  - name: admin1
    key: secret-api-key-here
    role: admin
  - name: viewer1
    key: secret-api-key-here
    role: viewer

# You may also wish to enable the Admin API.
#
# If you'd be administrating APISIX via another service
# (e.g. APISIX Dashboard, which manipulates the etcd database directly),
# then enabling this Admin API is not strictly required.
apisix_gateway_container_labels_admin_enabled: true
apisix_gateway_container_labels_admin_hostname: admin.api.example.com
apisix_gateway_container_labels_admin_path_prefix: /

########################################################################
#                                                                      #
# /apisix_gateway                                                      #
#                                                                      #
########################################################################
```

If you'd like to do something more advanced, the [`ansible-role-apisix-gateway` Ansible role](https://github.com/mother-of-all-self-hosting/ansible-role-apisix-gateway) is very configurable and should not get in your way of exposing ports or configuring arbitrary settings.

Take a look at [its `default/main.yml` file](https://github.com/mother-of-all-self-hosting/ansible-role-apisix-gateway/blob/main/defaults/main.yml) for available Ansible variables you can use in your own `vars.yml` configuration file.

### URL

In the example configuration above, we configure APISIX to expose 2 services:

- Gateway API, to be reachable at `https://api.example.com/api`
- [Admin API](https://apisix.apache.org/docs/apisix/admin-api/), to be reachable at `https://api.example.com/api`

Path prefixes default to `/` for all services, so if you don't like the example above (using `/api`), consider removing the path prefix variables.

## Usage

After installation, you can send API requests to your API gateway (as specified in `apisix_gateway_hostname` and `apisix_gateway_path_prefix`).

Example: `curl https://api.example.com/api`

Since no routes are configured by default, you'd receive 404 requests. To configure routes, either use the Admin API (described below) or install [APISIX dashboard](./apisix-dashboard.md) to administrate APISIX using a web UI.

If you've enabled the [Admin API](https://apisix.apache.org/docs/apisix/admin-api/) (`apisix_gateway_container_labels_admin_enabled: true`), you will also be able to manage the APISIX configuration (managing routes, upstreams, etc.) by sending API requests to the Admin API URL (as specified in `apisix_gateway_container_labels_admin_hostname` and `apisix_gateway_container_labels_admin_path_prefix`).

Example: `curl -H 'X-API-KEY: YOUR_SECRET_API_KEY_HERE' https://admin.api.example.com/apisix/admin/routes`

## Recommended other services

- [APISIX dashboard](apisix-dashboard.md) — a dashboard (web UI) for APISIX
````

## File: docs/services/appsmith.md
````markdown
# Appsmith

[Appsmith](https://www.appsmith.com/) is an open-source platform that enables developers to build and deploy custom internal tools and applications without writing code.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# appsmith                                                             #
#                                                                      #
########################################################################

appsmith_enabled: true

appsmith_hostname: appsmith.example.com

# WARNING: remove this after you create your user account,
# unless you'd like to run a server with public registration enabled.
appsmith_environment_variable_appsmith_signup_disabled: false

########################################################################
#                                                                      #
# /appsmith                                                            #
#                                                                      #
########################################################################
```


### URL

In the example configuration above, we configure the service to be hosted at `https://appsmith.example.com`.

Hosting Appsmith under a subpath (by configuring the `appsmith_path_prefix` variable) does not seem to be possible right now, due to Appsmith limitations..


### Authentication

Public registration can be enabled/disabled using the `appsmith_environment_variable_appsmith_signup_disabled` variable.

We recommend installing with public registration enabled at first, creating your first user account, and then disabling public registration (unless you need it).


## Usage

After installation, you can go to the Appsmith URL, as defined in `appsmith_hostname`.

As mentioned in [Authentication](#authentication) above, you can create the first user from the web interface.

If you'd like to prevent other users from registering, consider disabling public registration by removing the `appsmith_environment_variable_appsmith_signup_disabled` references from your configuration and re-running the playbook (`just install-service appsmith`).
````

## File: docs/services/authelia.md
````markdown
# Authelia

[Authelia](https://www.authelia.com/) is an open-source [authentication](https://www.authelia.com/overview/authentication/introduction/) and [authorization](https://www.authelia.com/overview/authorization/access-control/) server and portal fulfilling the identity and access management (IAM) role of information security in providing [multi-factor authentication](https://www.authelia.com/overview/authentication/introduction/) and single sign-on (SSO) for your applications via a web portal.

Authelia has 2 [modes of operation](#modes-of-operation) (Forward-Auth and OpenID Connect). Read below for more information.

> [!WARNING]
> This service is a new addition to the playbook. It may not fully work or be configured in a suboptimal manner.


## Dependencies

This service requires the following other services:

- a database
  - (optional) a [Postgres](postgres.md) database — if enabled for your Ansible inventory host, Authelia will be connected to the Postgres server automatically
  - (optional) a MySQL / [MariaDB](mariadb.md) database — if enabled for your Ansible inventory host (and you don't also enable Postgres), Authelia will be connected to the MariaDB server automatically
  - or SQLite, used by default when none of the above database choices is enabled for your Ansible inventory host

- (optional, but recommended) [Valkey](valkey.md)
  - for storing session information in a persistent manner
  - if Valkey is not enabled, session information is stored in-memory and restarting Authelia destroys user sessions

- a [Traefik](traefik.md) reverse-proxy server
  - for serving the Authelia portal website
  - for protecting other Traefik-based services by adding the Authelia forward-auth middleware to them when [Protecting services with Authelia's forward-auth](#protecting-a-service-with-authelias-forward-auth)


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# authelia                                                             #
#                                                                      #
########################################################################

authelia_enabled: true

authelia_hostname: authelia.example.com

# The base domain that session cookies related to authentication will be set on.
#
# Forward-auth services that you will protect need to be hosted on subdomains under this base domain.
# (e.g. service1.example.com, service2.example.com)
#
# For OpenID Connect-protected services, this value does not matter.
#
# In any case, `authelia_config_session_domain` needs to be a subdomain of `authelia_hostname`.
authelia_config_session_domain: example.com

# The database encryption password.
# Changing this subsequently will cause trouble unless you use the CLI to migrate.
# Generating a strong password (e.g. `pwgen -s 64 1`) is recommended.
authelia_config_storage_encryption_key: ''

# Authelia supports either LDAP or a file-based authentication (user) database.
#
# Using the file-based database is easiest and it's what we do by default here
# by defining `authelia_config_authentication_backend_file_content`.
#
# To use LDAP, remove `authelia_config_authentication_backend_file_content` and define various
# `authelia_config_authentication_backend_ldap_*` variables.
authelia_config_authentication_backend_file_content: |
  ---
  users:
    john:
      disabled: false
      displayname: John Doe
      password: PASSWORD_HASH_HERE
      email: john@example.com
      groups:
        - admins
        - dev
    peter:
      disabled: false
      displayname: Peter Johnson
      password: PASSWORD_HASH_HERE
      email: peter@example.com
      groups:
        - dev
  ...

# You can define various authentication rules for each protected service here.
authelia_config_access_control_rules:
 - domain: 'service1.example.com'
   policy: one_factor

# The configuration below connects Authelia to the Valkey instance, for session storage purposes.
# You may wish to run a separate Valkey instance for Authelia, because Valkey is not multi-tenant.
# Read more in docs/services/redis.md.
# If Valkey is not available, session data will be stored in memory and will be lost on container restart.
authelia_config_session_redis_host: "{{ valkey_identifier if valkey_enabled else '' }}"

########################################################################
#                                                                      #
# /authelia                                                            #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://authelia.example.com`.

While the Authelia Ansible role provides an `authelia_path_prefix` variable, Authelia does not support being hosted at a subpath right now.

On the Authelia base URL, there's a portal website where you can log in and manage your user account.


### Session storage

As mentioned in the default configuration above (see `authelia_config_session_redis_host`), you may wish to run [Valkey](valkey.md) for storing session data.

You may wish to run a separate Valkey instance for Authelia, because Valkey is not multi-tenant. See [our Valkey documentation page](valkey.md) for additional details. When running a separate instance of Valkey, you may need to connect Authelia to the Valkey instance's container network via the `authelia_container_additional_networks_custom` variable.


### Authentication storage providers

Authelia supports [LDAP](https://www.authelia.com/configuration/first-factor/ldap/) and [file-based](https://www.authelia.com/configuration/first-factor/file/) storage providers for the user database.

The default configuration above enables the file-based provider with the `authelia_config_authentication_backend_file_content` variable.

To use LDAP, remove the `authelia_config_authentication_backend_file_content` variable and define a bunch of `authelia_config_authentication_backend_ldap_*` variables.


### Modes of operation

Authelia has 2 [modes of operation](#modes-of-operation) which can be enabled simultaneously:

- **Forward-Auth**: [Forward-Auth](https://doc.traefik.io/traefik/middlewares/http/forwardauth/) is useful for protecting services which are not aware of authentication at all or which can receive authentication/authorization data via [HTTP headers](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers). Forward-Auth can act as a replacement for [HTTP Basic Authentication](https://en.wikipedia.org/wiki/Basic_access_authentication). It does this by acting as a companion to [common reverse proxies](https://www.authelia.com/overview/prologue/supported-proxies/), like [Traefik](traefik.md) which is frequently used by this playbook. To learn more, see [Protecting a service with Authelia's forward-auth](#protecting-a-service-with-authelias-forward-auth)

- **OpenID Connect**: experimental OpenID Connect support support, so that services which are OpenID Connect-compatible can use Authelia as an identity provider. To learn more, see [Protecting a service with OpenID Connect](#protecting-a-service-with-openid-connect)


#### Protecting a service with Authelia's forward-auth

If you're using [Traefik](traefik.md), you can easily protect services running on the same host by adding additional Traefik labels to them.

[Forward-Auth](https://doc.traefik.io/traefik/middlewares/http/forwardauth/) is useful for protecting services which are not aware of authentication at all or which can receive authentication/authorization data via [HTTP headers](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers).

Here's an example configuration for [Hubsite](hubsite.md) (a service which does not support authentication at all):

```yaml
hubsite_container_labels_additional_labels: |
  traefik.http.routers.{{ hubsite_identifier }}.middlewares={{ authelia_identifier }}@docker
```

The Hubsite component does not use any Traefik middlewares, so defining a `.middlewares` configuration key and pointing it to the Authelia middleware works well.

For most other components, middlewares are in use in their default Traefik labels, so adding an additional `.middlewares` key will not work. You may need to inject additional middlewares on top of the default ones. Not all components may (yet) have a variable for doing so. Consider contributing to various roles to allow additional middlewares to be injected dynamically!


#### Protecting a service with OpenID Connect

For services which support OpenID Connect, you can enable the [experimental OpenID Connect identity provider](https://www.authelia.com/configuration/identity-providers/open-id-connect/) in Authelia.

You will need some additional configuration like this:

```yaml
authelia_config_identity_providers_oidc_clients:
  - id: grafana
    description: Grafana
    secret: HASH_OF_THE_SHARED_SECRET
    public: false
    authorization_policy: one_factor
    redirect_uris:
      - https://mash.example.com/grafana/login/generic_oauth
    scopes:
      - openid
      - profile
      - groups
      - email
    userinfo_signing_algorithm: none

authelia_config_identity_providers_oidc_issuer_private_key: |
  KEY CONTENT GOES HERE.
  TO GENERATE A KEY, USE: `openssl genpkey -algorithm RSA -out FILE_NAME`
```

The example configuration above configures a single OpenID Connect client (application) called `grafana` (see the [Grafana](grafana.md) service supported by this playbook), which supposedly lives at the base URL of `https://mash.example.com/grafana`.

You will need to create a shared secret and hash its value (e.g. `php -r 'echo password_hash("PASSWORD_HERE",  PASSWORD_ARGON2ID);'`). Feel free to use another language (or tool) for creating a hash as well. A few different hash algorithms are supported besides Argon2id.

Finally, configure your application, hooking it to Authelia's OpenID Connect identity provider.
You can get inspired by the [sample configuration](grafana.md#single-sign-on--authelia) we have created for [Grafana](grafana.md).

## Extending the Authelia configuration

The Authelia Ansible role provides various variables for configuring Authelia. You can see their default values in the [`defaults/main.yml` file](https://github.com/mother-of-all-self-hosting/ansible-role-authelia/blob/main/defaults/main.yml) of the Authelia role.

If a dedicated variable is not available for you to use or if you wish to override some hardcoded default, you can always use the `authelia_configuration_extension_yaml` Ansible variable for extending/overriding the default configuration.

## Related services

- [authentik](authentik.md) — An open-source Identity Provider focused on flexibility and versatility.
- [Keycloak](keycloak.md) — An open source identity and access management solution
- [OAuth2-Proxy](oauth2-proxy.md) — A reverse proxy and static file server that provides authentication using OpenID Connect Providers (Google, GitHub, [Authentik](authentik.md), [Keycloak](keycloak.md), and others) to SSO-protect services which do not support SSO natively
````

## File: docs/services/authentik.md
````markdown
# Authentik

[authentik](https://goauthentik.io/) is an open-source Identity Provider focused on flexibility and versatility. MASH can install authentik with the [`mother-of-all-self-hosting/ansible-role-authentik`](https://github.com/mother-of-all-self-hosting/ansible-role-authentik) ansible role.


> [!WARNING]
> SSO is pretty complex and while this role will install authentik for you we only tested OIDC and OAUTH integration. There is a high probability that using outposts/LDAP would need further configuration efforts. Make sure you test before using this in production and feel free to provide feedback!

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Valkey](valkey.md) data-store, installation details [below](#valkey)
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# authentik                                                            #
#                                                                      #
########################################################################

authentik_enabled: true

authentik_hostname: authentik.example.com

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
authentik_secret_key: ''

# Valkey configuration, as described below

########################################################################
#                                                                      #
# /authentik                                                           #
#                                                                      #
########################################################################
```

### Valkey

As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require KeyDB on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to authentik](#creating-a-valkey-instance-dedicated-to-authentik).

If you're only running authentik on this server and don't need to use KeyDB for anything else, you can [use a single Valkey instance](#using-the-shared-valkey-instance-for-authentik).

#### Using the shared Valkey instance for authentik

To install a single (non-dedicated) Valkey instance (`mash-valkey`) and hook authentik to it, add the following **additional** configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# authentik                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point authentik to the shared Valkey instance
authentik_config_redis_hostname: "{{ valkey_identifier }}"

# Make sure the authentik service (mash-authentik.service) starts after the shared KeyDB service (mash-valkey.service)
authentik_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the authentik container is connected to the container network of the shared KeyDB service (mash-valkey)
authentik_container_additional_networks_custom:
  - "{{ valkey_identifier }}"

########################################################################
#                                                                      #
# /authentik                                                           #
#                                                                      #
########################################################################
```

This will create a `mash-valkey` Valkey instance on this host.

This is only recommended if you won't be installing other services which require KeyDB. Alternatively, go for [Creating a Valkey instance dedicated to authentik](#creating-a-valkey-instance-dedicated-to-authentik).


#### Creating a Valkey instance dedicated to authentik

The following instructions are based on the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `authentik.example.com` is your main one, create `authentik.example.com-deps`).

Then, create a new `vars.yml` file for the

`inventory/host_vars/authentik.example.com-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-authentik-'
mash_playbook_service_base_directory_name_prefix: 'authentik-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-authentik-valkey` instance on this host with its data in `/mash/authentik-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/authentik.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# authentik                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point authentik to its dedicated Valkey instance
authentik_config_redis_hostname: mash-authentik-valkey

# Make sure the authentik service (mash-authentik.service) starts after its dedicated KeyDB service (mash-authentik-valkey.service)
authentik_systemd_required_services_list_custom:
  - "mash-authentik-valkey.service"

# Make sure the authentik container is connected to the container network of its dedicated KeyDB service (mash-authentik-valkey)
authentik_container_additional_networks_custom:
  - "mash-authentik-valkey"

########################################################################
#                                                                      #
# /authentik                                                           #
#                                                                      #
########################################################################
```


## Installation

If you've decided to install a dedicated Valkey instance for authentik, make sure to first do [installation](../installing.md) for the supplementary inventory host (e.g. `authentik.example.com-deps`), before running installation for the main one (e.g. `authentik.example.com`).


## Usage

After installation, you can set the admin password at `https://<authentik_hostname>/if/flow/initial-setup/`. Set the admin password there and start adding applications and users! Refer to the [official documentation](https://goauthentik.io/docs/) to learn how to integrate services. For this playbook tested examples are described in the respective service documentation. See

* [Grafana](./grafana.md#single-sign-on-authentik)
* [Nextcloud](./nextcloud.md#single-sign-on-authentik)
````

## File: docs/services/auxiliary.md
````markdown
# AUX

The [AUX](https://github.com/mother-of-all-self-hosting/ansible-role-aux) Ansible role can help you manage auxiliary files and directoris on your server.

It's useful for when you'd like to use Ansible to drop additional files on the server.

Consult the role's documentation for learning more.
````

## File: docs/services/backup-borg.md
````markdown
<!--
SPDX-FileCopyrightText: 2022 - 2025 Nikita Chernyi
SPDX-FileCopyrightText: 2022 - 2024 Slavi Pantaleev
SPDX-FileCopyrightText: 2022 MDAD project contributors
SPDX-FileCopyrightText: 2022 Julian-Samuel Gebühr
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# BorgBackup

The playbook can install and configure [BorgBackup](https://www.borgbackup.org/) (short: Borg) with [borgmatic](https://torsion.org/borgmatic/) for you.

BorgBackup is a deduplicating backup program with optional compression and encryption. That means your daily incremental backups can be stored in a fraction of the space and is safe whether you store it at home or on a cloud service.

The [Ansible role for BorgBackup](https://github.com/mother-of-all-self-hosting/ansible-role-backup_borg) is developed and maintained by the MASH project. For details about configuring BorgBackup, you can check them via:
- 🌐 [the role's documentation](https://github.com/mother-of-all-self-hosting/ansible-role-backup_borg/blob/main/docs/configuring-backup-borg.md) online
- 📁 `roles/galaxy/backup_borg/docs/configuring-backup-borg.md` locally, if you have [fetched the Ansible roles](../installing.md)
````

## File: docs/services/calibre-web.md
````markdown
# Calibre-Web

[Calibre-Web](https://github.com/janeczku/calibre-web) is a web app that offers a clean and intuitive interface for browsing, reading, and downloading eBooks using a valid [Calibre](https://calibre-ebook.com/) database.

> [!WARNING]
> Calibre-Web currently does not support running the container rootless, therefore the role has not the usual security features of other services provided by this playbook. This put your system more at higher risk as vulnerabilities can have a higher impact.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# calibre-web                                                            #
#                                                                      #
########################################################################

calibre_web_enabled: true

calibre_web_hostname: mash.example.com
calibre_web_path_prefix: /calibre-web

# By default, calibre_web will look at the /books directory for your Calibre database.
#
# You'd need to mount some book directory into the calibre_web container, like shown below.
# The "Syncthing integration" section below may be relevant.
# calibre_web_container_additional_volumes:
#   - type: bind
#     src: /on-host/path/to/books
#     dst: /books

#
#
#
# Enable this extension explicitly to add the Calibre ebook-convert binary (x64 only). Omit this variable for a lightweight image.
# The path to the binary is /usr/bin/ebook-convert (has to be specified in the web interface — also specify the path to Calibre binaries as well; usr/bin)
#calibre_web_environment_variables_extension: |
#  DOCKER_MODS=linuxserver/mods:universal-calibre

########################################################################
#                                                                      #
# /calibre-web                                                           #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/calibre-web`.

You can remove the `calibre_web_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

### Authentication

The default username is `admin` and the default password is `admin123`.
You'll be able to change the username and password, and add additional users in the web UI.

On the initial setup screen, enter /books as your calibre library location.
If you haven't placed a Calibre database in that directory on the host yet, it will error as an invalid location.

### Syncthing integration

If you've got a [Syncthing](syncthing.md) service running, you can use it to synchronize your books directory onto the server and then mount it as read-only into the calibre_web container.

We recommend that you make use of the [aux](auxiliary.md) role to create some shared directory like this:

```yaml
########################################################################
#                                                                      #
# aux                                                                  #
#                                                                      #
########################################################################

aux_directory_definitions:
  - dest: "{{ mash_playbook_base_path }}/storage"
  - dest: "{{ mash_playbook_base_path }}/storage/books"

########################################################################
#                                                                      #
# /aux                                                                 #
#                                                                      #
########################################################################
```

You can then mount this `{{ mash_playbook_base_path }}/storage/books` directory into the Syncthing container and synchronize it with some other computer:

```yaml
########################################################################
#                                                                      #
# syncthing                                                            #
#                                                                      #
########################################################################

# Other Syncthing configuration..

syncthing_container_additional_volumes:
  - type: bind
    src: "{{ mash_playbook_base_path }}/storage/books"
    dst: /books

########################################################################
#                                                                      #
# /syncthing                                                           #
#                                                                      #
########################################################################
```

Finally, mount the `{{ mash_playbook_base_path }}/storage/books` directory into the calibre-web container as read-only:

```yaml
########################################################################
#                                                                      #
# calibre-web                                                            #
#                                                                      #
########################################################################

# Other calibre-web configuration..

calibre_web_container_additional_volumes:
  - type: bind
    src: "{{ mash_playbook_base_path }}/storage/books"
    dst: /books

########################################################################
#                                                                      #
# /calibre-web                                                           #
#                                                                      #
########################################################################
```

## Usage

After installation, you can go to the calibre-web URL, as defined in `calibre_web_hostname` and `calibre_web_path_prefix`.

## Recommended other services

- [Syncthing](syncthing.md) — a continuous file synchronization program which synchronizes files between two or more computers in real time. See [Syncthing integration](#syncthing-integration)
````

## File: docs/services/changedetection.md
````markdown
# Changedetection.io

[Changedetection.io](https://github.com/dgtlmoon/changedetection.io) is a simple **website change detection and restock monitoring** solution.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# Changedetection.io                                                   #
#                                                                      #
########################################################################

changedetection_enabled: true

changedetection_hostname: mash.example.com

changedetection_path_prefix: /changedetection

########################################################################
#                                                                      #
# /Changedetection.io                                                  #
#                                                                      #
########################################################################
```

### Playwright webdriver

Some advanced options like using javascript or using the Visual Selector tool use an additional playwright webdriver. To enable this driver, add the following **additional**  configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
changedetection_playwright_driver_enabled: true
```


### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/changedetection`.

You can remove the `changedetection_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

## Usage

After installation, you can go to your given URL and start setting up Changedetection.io
````

## File: docs/services/clickhouse.md
````markdown
# ClickHouse

[ClickHouse](https://clickhouse.com/) is an open-source column-oriented DBMS for online analytical processing (OLAP) that allows users to generate analytical reports using SQL queries in real-time.

Some of the services installed by this playbook require a ClickHouse database.

Enabling the ClickHouse database service will automatically wire all other services which require such a database to use it.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# clickhouse                                                           #
#                                                                      #
########################################################################

clickhouse_enabled: true

# Put a strong password below, generated with `pwgen -s 64 1` or in another way
clickhouse_root_passsword: ''

########################################################################
#                                                                      #
# /clickhouse                                                          #
#                                                                      #
########################################################################
```

### Getting a database terminal

You can use the `/mash/clickhouse/bin/cli` tool to get interactive terminal access to the ClickHouse server.

## Upgrading ClickHouse

ClickHouse is supposed to auto-upgrade its data as you upgrade to a newer version. There's nothing special that needs to be done.

## Backing up ClickHouse

The `/mash/clickhouse/backups` directory is mounted as `/backups` into the container and is an allowed disk for backups called `backups`.

You can export a single database table by using [the CLI](#getting-a-database-terminal) and running a command like this:

```sql
BACKUP TABLE test TO Disk('backups', 'test.zip');
```

Read the [Backup and Restore](https://clickhouse.com/docs/en/operations/backup) article in the official documentation to learn more.

For better (more en-masse) exporting, it may be beneficial to use the 3rd party [clickhouse-backup](https://github.com/AlexAkulov/clickhouse-backup) tool, but this is not supported by the playbook yet.
````

## File: docs/services/collabora-online.md
````markdown
# Collabora Online

The [Collabora Online Development Edition (CODE)](https://www.collaboraoffice.com/) office suite, together with the [Office App](https://apps.nextcloud.com/apps/richdocuments) for [Nextcloud](nextcloud.md) enables you to edit office documents online.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# collabora-online                                                     #
#                                                                      #
########################################################################

collabora_online_enabled: true

collabora_online_hostname: collabora.example.com

# A password for the admin interface, available at: https://COLLABORA_ONLINE_DOMAIN/browser/dist/admin/admin.html
# Use only alpha-numeric characters
collabora_online_env_variable_password: ''

collabora_online_aliasgroup: "https://{{ nextcloud_hostname | replace('.', '\\.') }}:443"

########################################################################
#                                                                      #
# /collabora-online                                                    #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://collabora.example.com`.


## Integrating with Nextcloud

To learn how to integrate Collabora Online with Nextcloud, see the [Collabora Online section](nextcloud.md#collabora-online) in our Nextcloud documentation.


## Admin Interface

There's an admin interface with various statistics and information at: `https://COLLABORA_ONLINE_DOMAIN/browser/dist/admin/admin.html`

Use your admin credentials for logging in:

- the default username is `admin`, as specified in `collabora_online_env_variable_username`
- the password is the one you've specified in `collabora_online_env_variable_password`
````

## File: docs/services/couchdb.md
````markdown
# CouchDB

[CouchDB](https://couchdb.apache.org/) is a NoSQL database that uses JSON for documents.
This Ansible role is designed to install and configure CouchDB for using the [official CouchDB Docker image](https://github.com/apache/couchdb-docker) via the [ansible-role-couchdb](https://github.com/Bergruebe/ansible-role-couchdb).

> [!WARNING]
> - This role will not delete or modify existing databases or users. It will only create new databases and users if they do not already exist.
> - This role **does not automatically integrate with Traefik** yet (see details below). PRs are welcome!

## Features

- Sets up CouchDB in a Docker container.
- Creates necessary system tables, if `couchdb_config_single_node: true.
- Adds users as specified in the playbook.
- Sets database permissions.
- Integrates with the MASH playbook for easy deployment.

## Usage

To use this role with the MASH playbook, add following lines to your inventory file of your MASH playbook:

```yaml
########################################################################
#                                                                      #
# couchdb                                                              #
#                                                                      #
########################################################################

couchdb_enabled: true

couchdb_hostname: couchdb.example.com

# enable couchdb single node mode, to automatically create databases and users
couchdb_config_single_node: true

couchdb_admins_custom:
  - name: admin
    password: UseASecurePassword

couchdb_users_custom:
  - name: user1
    password: UseASecurePassword
    roles: []
    type: user

couchdb_tables_custom:
    - name: my_custom_table
      permission:
        admin:
          names:
            - user1
          roles: []
        member:
          names: []
          roles: []

########################################################################
#                                                                      #
# /couchdb                                                             #
#                                                                      #
########################################################################
```

You can customize the behavior of the role by setting the following variables in your playbook:

- `couchdb_environment_variables_extension`: to add additional environment variables to the CouchDB container.
- `couchdb_config_extension`: to add additional configuration to the CouchDB configuration
- `couchdb_config_peruser_enabled`: to enable per-user configuration in CouchDB | default is `true`.
- `couchdb_config_require_valid_user_except_for_up`: to require a valid user for all requests except for the `_up` endpoint | default is `true`.
- `couchdb_container_additional_networks_custom`: to add additional networks to the CouchDB container.
- `couchdb_version`: to specify the version of the CouchDB Docker image to use

For more information on possible configuration, refer to the comments in the [`defaults/main.yml`](https://github.com/Bergruebe/ansible-role-couchdb/blob/master/defaults/main.yml) file.

By default, this role **will not expose the CouchDB port** to the host machine. If you want to access CouchDB from outside the Docker container, you will need to expose the port in your playbook via the `couchdb_container_http_host_bind_port` variable. Or you can just add the container to another docker network via the `couchdb_container_additional_networks_custom` variable.
Please consider the use of a reverse proxy for secure access to CouchDB.

Currently, the [ansible-role-couchdb](https://github.com/Bergruebe/ansible-role-couchdb) Ansible role **does not automatically integrate with Traefik**. PRs are welcome!

## Contributing

Contributions are welcome! Please feel free to review the [ansible-role-couchdb](https://github.com/Bergruebe/ansible-role-couchdb) repository and submit a Pull Request.
````

## File: docs/services/docker-registry-browser.md
````markdown
# Docker Registry Browser

[Docker Registry Browser](https://github.com/klausmeyer/docker-registry-browser) is a Web Interface for the Docker Registry HTTP API V2 written in Ruby on Rails.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# docker-registry-browser                                              #
#                                                                      #
########################################################################

docker_registry_browser_enabled: true

# Hosting under a subpath (such as `/browser`) allows the browser to co-exist
# on the same hostname as a Docker Registry instance (see `docker-registry.md`).
docker_registry_browser_hostname: registry.example.com
docker_registry_browser_path_prefix: /browser

# If the browser will be able to delete images and live on the same private container network
# as the registry itself (like we do below), it's recommended to protect it with HTTP Basic Auth.
#
# If you're running a read-only browser, you may leave it publicly accessible.
docker_registry_browser_basic_auth_enabled: true
docker_registry_browser_basic_auth_username: admin
# You can put any string here, but generating a strong one is preferred (e.g. `pwgen -s 64 1`).
docker_registry_browser_basic_auth_password: ''

# To integrate with a locally running (in a container) Docker Registry (see `docker-registry.md`),
# point to its local container address and configure the browser to run in the registry's network.
docker_registry_browser_docker_registry_url: "http://{{ docker_registry_identifier }}:5000"
docker_registry_browser_container_network: "{{ docker_registry_container_network }}"

# Alternatively, to use a registry running elsewhere, delete both lines above
# (docker_registry_browser_docker_registry_url and docker_registry_browser_container_network),
# and use something this instead:
# docker_registry_browser_docker_registry_url: "https://registry.example.com"

# Image deletion is disabled by default, so you need to explicitly enable it if you need it.
docker_registry_browser_enabled_delete_images: true

########################################################################
#                                                                      #
# /docker-registry-browser                                             #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://registry.example.com/browser`.

If you make the registry browser live on the same container network as the [Docker Registry](docker-registry.md) itself (like we've done by overriding `docker_registry_browser_container_network` above), the browser will be able to talk to the registry over the private container network and IP restrictions (such as those defined in `docker_registry_private_services_whitelisted_ip_ranges`) will not be able to stop it.


## Usage

After installation, you should be able to go to the URL as configured via `docker_registry_browser_hostname` and `docker_registry_browser_path_prefix`.

You should be able to browse the images and possibly delete them (if enabled via `docker_registry_browser_enabled_delete_images`).


## Recommended other services

- [Docker Registry](docker-registry.md) — a container image distribution registry developed by [Docker Inc](https://www.docker.com/)
````

## File: docs/services/docker-registry-proxy.md
````markdown
# Docker Registry Proxy

[Docker Registry Proxy](https://gitlab.com/etke.cc/docker-registry-proxy/) is a pass-through docker registry (distribution) proxy with metadata caching, docker-compatible errors, prometheus metrics, etc.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# docker-registry-proxy                                                #
#                                                                      #
########################################################################

docker_registry_proxy_enabled: true

docker_registry_proxy_hostname: registry.example.com

# List of the IPs allowed to access the registry (GET, HEAD, OPTIONS requests only)
docker_registry_proxy_allowed_ips: []

# List of the User Agent names(!) allowed to access the registry (GET, HEAD, OPTIONS requests only)
docker_registry_proxy_allowed_uas:
- docker

# List of the IPs trusted to access the registry (PATCH, POST, PUT, DELETE requests only)
docker_registry_proxy_trusted_ips: []

########################################################################
#                                                                      #
# /docker-registry-proxy                                             #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://registry.example.com`.

## Usage

After installation, you should be able to go to the URL as configured via `docker_registry_proxy_hostname`.

## Recommended other services

- [Docker Registry](docker-registry.md) — a container image distribution registry developed by [Docker Inc](https://www.docker.com/), wired automatically to the proxy, just disable registry's traefik labels
- [Grafana](grafana.md) — a multi-platform open source analytics and interactive visualization web application, Docker Registry Proxy comes with [pre-configured grafana dashboard](https://gitlab.com/etke.cc/docker-registry-proxy/-/blob/main/contrib/grafana-dashboard.json)
````

## File: docs/services/docker-registry-purger.md
````markdown
# Docker Registry Purger

[Docker Registry Purger](https://github.com/devture/docker-registry-purger) is a small tool used for purging a private Docker registry's old tags.


## Dependencies

This service requires to be pointed to a container registry. It may be a registry powered by [Docker Registry](docker-registry.md) or by some other software.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# docker-registry-purger                                               #
#                                                                      #
########################################################################

docker_registry_purger_enabled: true

# To integrate with a locally running (in a container) Docker Registry (see `docker-registry.md`),
# point to its local container address and configure the purger to run in the registry's network.
docker_registry_purger_registry_url: "http://{{ docker_registry_identifier }}:5000"
docker_registry_purger_container_network: "{{ docker_registry_container_network }}"

# Alternatively, to use a registry running elsewhere, delete both lines above
# (docker_registry_purger_registry_url and docker_registry_purger_container_network),
# and use something this instead:
# docker_registry_purger_registry_url: "https://registry.example.com"

########################################################################
#                                                                      #
# /docker-registry-purger                                              #
#                                                                      #
########################################################################
```

You may wish to tweak some [default configuration]() variables, which ultimately control [environment variables](https://github.com/devture/docker-registry-purger#environment-variables) of the purger tool.


## Usage

After installation, you should be able to go to the URL as configured via `docker_registry_browser_hostname` and `docker_registry_browser_path_prefix`.

You should be able to browse the images and possibly delete them (if enabled via `docker_registry_browser_enabled_delete_images`).


## Recommended other services

- [Docker Registry](docker-registry.md) — a container image distribution registry developed by [Docker Inc](https://www.docker.com/)
````

## File: docs/services/docker-registry.md
````markdown
# Docker Registry

[Docker Registry](https://docs.docker.com/registry/) is a container image distribution registry developed by [Docker Inc](https://www.docker.com/).

This playbook supports installing a container image registry which is:

- completely public, when it comes to pulling images
- IP-restricted, when it comes to pushing images

Authentication is not supported.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# docker-registry                                                      #
#                                                                      #
########################################################################

docker_registry_enabled: true

docker_registry_hostname: registry.example.com

# Uncomment the line below if you'd like to allow for image deletion.
# docker_registry_storage_delete_enabled: true

# Only whitelisted IPs will be able to perform DELETE, PATCH, POST, PUT requests against the registry.
# All other IP addresses get read-only (GET, HEAD) access.
docker_registry_private_services_whitelisted_ip_ranges:
  - 1.2.3.4/32
  - 4.3.2.1/32

########################################################################
#                                                                      #
# /docker-registry                                                     #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://registry.example.com`.


## Usage

After installation, you should be able to:

- pull images from your registry from any IP address
- push images to your registry from the whitelisted IP addresses (`docker_registry_private_services_whitelisted_ip_ranges`)

With custom Traefik configuration (hint: see [`docker_registry_container_labels_traefik_rule_*` variables](https://github.com/mother-of-all-self-hosting/ansible-role-docker-registry/blob/main/defaults/main.yml) in the [docker-registry role](https://github.com/mother-of-all-self-hosting/ansible-role-docker-registry)), you may be able to add additional restrictions.

To **test pushing** images, try the following:

```sh
docker pull docker.io/alpine:3.17.2
docker tag docker.io/alpine:3.17.2 registry.example.com/alpine:3.17.2
docker push registry.example.com/alpine:3.17.2
```

To **test pulling** images, try the following:

```sh
# Clean up from before
docker rmi registry.example.com/alpine:3.17.2

docker pull registry.example.com/alpine:3.17.2
```

The base URL (e.g. `https://registry.example.com`) serves an empty (blank) page. To browse your registry's images via a web interface, you may need another piece of software, like [Docker Registry Browser](docker-registry-browser.md).


## Recommended other services

- [Docker Registry Browser](docker-registry-browser.md) — Web Interface for the Docker Registry HTTP API V2 written in Ruby on Rails
- [Docker Registry Purger](docker-registry-purger.md) — a small tool used for purging a private Docker Registry's old tags
````

## File: docs/services/docker.md
````markdown
# Docker

This playbook installs [Docker](https://www.docker.com/) by default, because all services require it.

To disable Docker installation (and install Docker yourself in another way), use: `mash_playbook_docker_installation_enabled: false`
````

## File: docs/services/dokuwiki.md
````markdown
# DokuWiki

[DokuWiki](https://dokuwiki.org/) is a lightweight, file-based wiki engine with intuitive syntax and no database requirements.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# dokuwiki                                                             #
#                                                                      #
########################################################################

dokuwiki_enabled: true

dokuwiki_hostname: dokuwiki.example.com

########################################################################
#                                                                      #
# /dokuwiki                                                            #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://dokuwiki.example.com/`.

## Usage

You will have to open the URL `https://dokuwiki.example.com/install.php` to complete the setup, as described in the official [setup instructions](https://www.dokuwiki.org/installer).
````

## File: docs/services/echoip.md
````markdown
# EchoIP

[EchoIP](https://github.com/mpolden/echoip) is simple service for looking up your IP address, powering [ifconfig.co](https://ifconfig.co)

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# echoip                                                               #
#                                                                      #
########################################################################

echoip_enabled: true

echoip_hostname: echoip.example.com

########################################################################
#                                                                      #
# /echoip                                                              #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://echoip.example.com`.


## Usage

```bash
curl https://echoip.example.com
```
````

## File: docs/services/endlessh.md
````markdown
# Endlessh

[Endlessh-go](https://github.com/shizunge/endlessh-go) is a Golang implementation of [endlessh](https://github.com/skeeto/endlessh), an [SSH tarpit](https://nullprogram.com/blog/2019/03/22). Installing it is powered by the [mother-of-all-self-hosting/ansible-role-endlessh](https://github.com/mother-of-all-self-hosting/ansible-role-endlessh) Ansible role.

## Dependencies

This service requires the following other services:

- (optionally) [Traefik](traefik.md) — a reverse-proxy server for exposing endlessh publicly
- (optionally) [Prometheus](./prometheus.md) — a database for storing metrics
- (optionally) [Grafana](./grafana.md) — a web UI that can query the prometheus datasource (connection) and display the logs

## Prerequisites

An SSH tarpit server needs a port to mimic the SSH server. Port 22 is therefore a good choice.
If you already have your SSH server on this port, you'll have to relocate it.
I recommend using a random port for the ssh server (eg: 14567) and port 22 for the tarpit.

## Installing

To configure and install endlessh on your own server(s), you should use a playbook like [Mother of all self-hosting](https://github.com/mother-of-all-self-hosting/mash-playbook) or write your own.

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# endlessh                                                             #
#                                                                      #
########################################################################

endlessh_enabled: true

########################################################################
#                                                                      #
# /endlessh                                                            #
#                                                                      #
########################################################################
```

By default, endlessh will try to bind to port 22 on all network interfaces.
You could change this behavior by setting `endlessh_container_host_bind_port`:

```yaml
endlessh_container_host_bind_port: 22
```

See the full list of options in the [default/main.yml](default/main.yml) file

## Integrating with Prometheus

Endlessh can natively expose metrics to [Prometheus](./prometheus.md).

### Prerequesites

The bare minimium is to ensure Prometheus can reach endlessh.

- If Endlessh is on a different host than Prometheus, refer to section [Expose metrics publicly](endlessh.md#)
- If Endlessh is on the same host than prometheus, refer to section [Ensure Prometheus is on the same container network as endlessh.](endlessh.md#)

### Ensure Prometheus is on the same container network as endlessh.

If endlessh and prometheus do not share a network (like traefik), you will have to

- Either connect Prometheus container network to Endlessh by editing `prometheus_container_additional_networks_auto`
- Either connect Endlessh container network to Prometheus by editing `endlessh_container_additional_networks_custom`

Exemple:

```yaml
prometheus_container_additional_networks:
  - "{{ endlessh_container_network }}"
```

### Set container extra flag:

The bare minimum is to set container extra flag `-enable_prometheus`

```yaml
endlessh_container_extra_arguments_custom:
  - "-enable_prometheus"
```

Default endlessh port for metrics is `2112`. It can be changed via container extra flag `-prometheus_port=8085`.

Default endlessh listening for metrics adress is `0.0.0.0.` (so endlessh will listing on all adresses). This parrameter can be changed via container extra flag `-prometheus_host=10.10.10.10`.

Default endlessh entrypoint for metrics is `/metrics`. It can be changed via container extra flag `-prometheus_entry=/endlessh`.

For more container extra flag, refer to the documentation of [endlessh-go](https://github.com/shizunge/endlessh-go).

### Exposing metrics publicly

Unless you're scraping the endlessh metrics from a local [Prometheus](prometheus.md) instance, as described in [Integrating with Prometheus](endlessh.md#), you will probably wish to expose the metrics publicly so that a remote Prometheus instance can fetch them. When exposing publicly, it's natural to set up [HTTP Basic Authentication](https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication) **or anyone would be able to read your metrics**.

```yaml
# To expose the metrics publicly, enable and configure the lines below:
endlessh_hostname: mash.example.com
endlessh_path_prefix: /metrics/mash-endlessh

# To protect the metrics with HTTP Basic Auth, enable and configure the lines below.
# See: https://doc.traefik.io/traefik/middlewares/http/basicauth/#users
endlessh_container_labels_metrics_middleware_basic_auth_enabled: true
endlessh_container_labels_metrics_middleware_basic_auth_users: ""
```

## Usage

After [installing](../installing.md), refer to the documentation of [endlessh-go](https://github.com/shizunge/endlessh-go).
````

## File: docs/services/etcd.md
````markdown
# etcd

[etcd](https://etcd.io/) is a strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines. It gracefully handles leader elections during network partitions and can tolerate machine failure, even in the leader node.

Our current setup and documentation are **aiming at running etcd for internal purposes** (as a dependency for other [services](../supported-services.md)).

If you need a production deployment, you will need to install multiple etcd instances (on multiple machines) and connect them in a cluster.
This is beyond the scope of our documentation here.

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# etcd                                                                 #
#                                                                      #
########################################################################

etcd_enabled: true

# By default, the playbook will set a root password by itself.
# If you'd like to set your own, uncomment and explicitly set this.
# etcd_environment_variable_etcd_root_password: ''

# Uncomment this if you'd like to run etcd without password-protection.
# etcd_environment_variable_allow_none_authentication: true

########################################################################
#                                                                      #
# /etcd                                                                #
#                                                                      #
########################################################################
```

If you'd like to do something more advanced, the [`ansible-role-etcd` Ansible role](https://github.com/mother-of-all-self-hosting/ansible-role-etcd) is very configurable and should not get in your way of exposing ports or configuring arbitrary settings.

Take a look at [its `default/main.yml` file](https://github.com/mother-of-all-self-hosting/ansible-role-etcd/blob/main/defaults/main.yml) for available Ansible variables you can use in your own `vars.yml` configuration file.


## Usage

As mentioned above, the purpose of the etcd component in this Ansible playbook is to serve as a dependency for other [services](../supported-services.md). For this use-case, you don't need to do anything special beyond enabling the component.
````

## File: docs/services/etherpad.md
````markdown
<!--
SPDX-FileCopyrightText: 2021 - 2024 Slavi Pantaleev
SPDX-FileCopyrightText: 2021 Béla Becker
SPDX-FileCopyrightText: 2021 pushytoxin
SPDX-FileCopyrightText: 2022 Jim Myhrberg
SPDX-FileCopyrightText: 2022 Nikita Chernyi
SPDX-FileCopyrightText: 2022 felixx9
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Etherpad

The playbook can install and configure [Etherpad](https://etherpad.org), an open source collaborative text editor, for you.

The [Ansible role for Etherpad](https://github.com/mother-of-all-self-hosting/ansible-role-etherpad) is developed and maintained by the MASH project. For details about configuring Etherpad, you can check them via:

- 🌐 [the role's documentation at the MASH project](https://github.com/mother-of-all-self-hosting/ansible-role-etherpad/blob/main/docs/configuring-etherpad.md) online
- 📁 `roles/galaxy/etherpad/docs/configuring-etherpad.md` locally, if you have [fetched the Ansible roles](../installing.md)

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server

## Adjusting the playbook configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# etherpad                                                             #
#                                                                      #
########################################################################

etherpad_enabled: true

etherpad_hostname: etherpad.example.com

########################################################################
#                                                                      #
# /etherpad                                                            #
#                                                                      #
########################################################################
```

As the most of the necessary settings for the role have been taken care of by the playbook, you can enable Etherpad on your server with this minimum configuration.

See the role's documentation for details about configuring Etherpad per your preference (such as [the name of the instance](https://github.com/mother-of-all-self-hosting/ansible-role-etherpad/blob/main/docs/configuring-etherpad.md#set-the-name-of-the-instance-optional) and [the default pad text](https://github.com/mother-of-all-self-hosting/ansible-role-etherpad/blob/main/docs/configuring-etherpad.md#set-the-default-text-optional)).

### Create admin user (optional)

You probably might want to enable authentication to disallow anonymous access to your Etherpad.

It is possible to enable HTTP basic authentication by **creating an admin user** with `etherpad_admin_username` and `etherpad_admin_password` variables. The admin user account is also used by plugins for authentication and authorization.

See [this section](https://github.com/mother-of-all-self-hosting/ansible-role-etherpad/blob/main/docs/configuring-etherpad.md#create-admin-user-optional) on the role's documentation for details about how to create the admin user.

### Control Etherpad's availability on Jitsi conferences (optional)

If a Jitsi video-conferencing platform (see [our docs on Jitsi](jitsi.md)) is enabled on your server, you can configure it so to make Etherpad available on conferences.

See [this section](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md#control-etherpads-availability-on-jitsi-conferences) on the Jitsi role's documentation for details about how to set it up.

## Usage

By default, the Etherpad UI should be available at `https://etherpad.example.com`, while the admin UI (if enabled) should then be available at `https://etherpad.example.com/admin`.

💡 For more information about usage, take a look at [this section](https://github.com/mother-of-all-self-hosting/ansible-role-etherpad/blob/main/docs/configuring-etherpad.md#usage) on the role's documentation.

## Troubleshooting

See [this section](https://github.com/mother-of-all-self-hosting/ansible-role-etherpad/blob/main/docs/configuring-etherpad.md#troubleshooting) on the role's documentation for details.
````

## File: docs/services/exim-relay.md
````markdown
<!--
SPDX-FileCopyrightText: 2018 - 2024 Slavi Pantaleev
SPDX-FileCopyrightText: 2019 Eduardo Beltrame
SPDX-FileCopyrightText: 2020 - 2025 MDAD project contributors
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Exim-relay

Various services need to send out email.

The default playbook configuration (`examples/vars.yml`) recommends that you enable the Exim relay SMTP mailer service (powered by [exim-relay](https://github.com/devture/exim-relay) and the [ansible-role-exim-relay](https://github.com/mother-of-all-self-hosting/ansible-role-exim-relay) Ansible role). Enabling this service **automatically wires various other services to send email through it**. Exim-relay then gives you a centralized place for configuring email-sending.

The [Ansible role for exim-relay](https://github.com/mother-of-all-self-hosting/ansible-role-exim-relay) is developed and maintained by the MASH project. For details about configuring exim-relay, you can check them via:
- 🌐 [the role's documentation](https://github.com/mother-of-all-self-hosting/ansible-role-exim-relay/blob/main/docs/configuring-exim-relay.md) online
- 📁 `roles/galaxy/exim_relay/docs/configuring-exim-relay.md` locally, if you have [fetched the Ansible roles](../installing.md)

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# exim_relay                                                           #
#                                                                      #
########################################################################

exim_relay_enabled: true

exim_relay_hostname: mash.example.com

exim_relay_sender_address: "someone@{{ exim_relay_hostname }}"

########################################################################
#                                                                      #
# /exim_relay                                                          #
#                                                                      #
########################################################################
```

### Enable DKIM authentication to improve deliverability (optional)

By default, exim-relay attempts to deliver emails directly. This may or may not work, depending on your domain configuration.

To improve email deliverability, you can configure authentication methods such as DKIM (DomainKeys Identified Mail), SPF, and DMARC for your domain. Without setting any of these authentication methods, your outgoing email is most likely to be quarantined as spam at recipient's mail servers.

For details about configuring DKIM, refer [this section](https://github.com/mother-of-all-self-hosting/ansible-role-exim-relay/blob/main/docs/configuring-exim-relay.md#enable-dkim-support-optional) on the role's documentation.

💡 If you cannot enable DKIM, SPF, or DMARC on your domain for some reason, we recommend relaying email through another SMTP server.

### Relaying email through another SMTP server (optional)

**On some cloud providers such as Google Cloud, [port 25 is always blocked](https://cloud.google.com/compute/docs/tutorials/sending-mail/), so sending email directly from your server is not possible.** In this case, you will need to relay email through another SMTP server.

For details about configuration, refer [this section](https://github.com/mother-of-all-self-hosting/ansible-role-exim-relay/blob/main/docs/configuring-exim-relay.md#relaying-email-through-another-smtp-server) on the role's document.

### Using a per-service sender address (optional)

By default, all roles that this playbook wires to `exim-relay` will all be configured to send emails using a `From` address as configured in `exim_relay_sender_address`.

To configure a given service to use another sender address, override the specific variables for the given service.

For example, to make [Vaultwarden](vaultwarden.md) (automatically wired to send via `exim-relay` if you have it enabled) send emails from a custom address (instead of the default, `exim_relay_sender_address`), add the following configuration to your `vars.yml` file:

```yml
vaultwarden_config_smtp_from: vaultwarden@example.com
```

## Troubleshooting

See [this section](https://github.com/mother-of-all-self-hosting/ansible-role-exim-relay/blob/main/docs/configuring-exim-relay.md#troubleshooting) on the role's documentation for details.
````

## File: docs/services/firezone.md
````markdown
# Firezone

[Firezone](https://www.firezone.dev/) is a self-hosted VPN server (based on [WireGuard](https://www.wireguard.com/)) with Web UI that this playbook can install, powered by the [mother-of-all-self-hosting/ansible-role-firezone](https://github.com/mother-of-all-self-hosting/ansible-role-firezone) Ansible role.

A more-lightweigth alternative for a self-hosted WireGuard VPN server which is more compatible with various ARM devices is [WireGuard Easy](wg-easy.md).


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# firezone                                                             #
#                                                                      #
########################################################################

firezone_enabled: true

firezone_hostname: vpn.example.org

firezone_default_admin_email: "user@invalid.org"
firezone_default_admin_password: "<securepassword>"

# Generate this with `openssl rand -base64 32`
firezone_database_encryption_key: "<secret>"

########################################################################
#                                                                      #
# /firezone                                                            #
#                                                                      #
########################################################################
```

After installation, you can use `just run-tags firezone-create-or-reset-admin` any time to:
- create the configured admin account
- or, reset the password to the current password configured in `vars.yml`

### Networking

By default, the following ports will be exposed by the container on **all network interfaces**:

- `51820` over **UDP**, controlled by `firezone_wireguard_bind_port` — used for [Wireguard](https://www.wireguard.com/) connections

Docker automatically opens these ports in the server's firewall, so you **likely don't need to do anything**. If you use another firewall in front of the server, you may need to adjust it.

### Usage

After [installing](../installing.md), you can login at the URL specified in `firezone_hostname`, with the credentials set in `firezone_default_admin_email` and `firezone_default_admin_password`.

Refer to the [official documentation](https://www.firezone.dev/docs/user-guides/add-devices/) to figure out how to add devices, etc.
````

## File: docs/services/focalboard.md
````markdown
# Focalboard

[Focalboard](https://www.focalboard.com/) is an open source, self-hosted alternative to [Trello](https://trello.com/), [Notion](https://www.notion.so/), and [Asana](https://asana.com/)


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# focalboard                                                           #
#                                                                      #
########################################################################

focalboard_enabled: true

focalboard_hostname: mash.example.com
focalboard_path_prefix: /focalboard

########################################################################
#                                                                      #
# /focalboard                                                          #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/focalboard`.

You can remove the `focalboard_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


## Usage

After [installation](../installing.md), you can go to the Focalboard URL you've configured above.

You can use the signup page to register the first (administrator) user. The first signup is always allowed. Users after the first one need an invitation link to sign up.
````

## File: docs/services/forgejo-runner.md
````markdown
# Forgejo Runner

[Forgejo Runner](https://code.forgejo.org/forgejo/runner) is a runner to use with [Forgejo Actions](https://forgejo.org/docs/latest/admin/actions/). It provides a way to perform CI using Forgejo. You might also be interested in [Woodpecker CI](https://woodpecker-ci.org/) (that this playbook also [supports](woodpecker-ci.md)).

> [!WARNING]
> Upstream considers this software as being an **alpha release**, and says it should **not** be considered secure enough to deploy in production. Use at your own risk.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# forgejo-runner                                                       #
#                                                                      #
########################################################################

forgejo_runner_enabled: true

forgejo_runner_runner_name: "Your Runner Name Here"

# The instance URL.
forgejo_runner_instance_url: "https://example.com"

# The registration token.
#
# Should be obtained via the web interface, by going to:
#
#   Site Administration -> Actions -> Runners -> Create new runner
forgejo_runner_registration_token: "TOKEN_HERE"

# The capacity of the runner, i.e., how many concurrent tasks it can run.
forgejo_runner_capacity: 1

# The labels associated with this runner.
forgejo_runner_labels:
  - ubuntu-22.04:docker://node:20-bullseye

########################################################################
#                                                                      #
# /forgejo-runner                                                      #
#                                                                      #
########################################################################
```

As mentioned in the example above, the registration token should be obtained via Forgejo's web interface, by going to `Site Administration -> Actions -> Runners -> Create new runner`.

Labels are an important aspect of the runner, and as such should be carefully chosen. Read [the official documentation](https://forgejo.org/docs/latest/admin/actions/#labels-and-runs-on) for more information.


## Usage

After the installation, the runner will register with the Forgejo instance (provided via the `forgejo_runner_instance_url` variable) and generate a `.runner` file inside its configuration path. This file should not be modified manually. If for some reason you wish to force the registration to run again, you can delete the `.runner` file and restart the service.

If you wish to change the labels associated with the runner, you can simply modify the `forgejo_runner_labels` variable and run the playbook again. There is no need to delete the `.runner` file and run the registration again.
````

## File: docs/services/forgejo.md
````markdown
# Forgejo

[Forgejo](https://forgejo.org/) is a self-hosted lightweight software forge (Git hosting service, etc.), an alternative to [Gitea](https://gitea.io/) (that this playbook also [supports](gitea.md)).


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# forgejo                                                              #
#                                                                      #
########################################################################

forgejo_enabled: true

# Forgejo uses port 22 by default.
# We recommend that you move your regular SSH server to another port,
# and stick to this default.
#
# If you wish to use another port, uncomment the variable below
# and adjust the port as you see fit.
# forgejo_ssh_port: 222

forgejo_hostname: mash.example.com
forgejo_path_prefix: /forgejo

########################################################################
#                                                                      #
# /forgejo                                                             #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/forgejo`.

You can remove the `forgejo_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


## Usage

After installation, you should be able to access your new Forgejo instance at the configured URL (see above).

Going there, you'll be taken to the initial setup wizard, which will let you assign some paswords and other configuration.


## Recommended other services

You may also wish to look into [Woodpecker CI](woodpecker-ci.md) and [Forgejo Runner](forgejo-runner.md), which can integrate nicely with Forgejo.


## Integration with Woodpecker CI

If you want to integrate Forgejo with [Woodpecker CI](woodpecker-ci.md), and if you plan to serve Woodpecker CI under a subpath on the same host as Forgejo (e.g., Forgejo lives at `https://mash.example.com` and Woodpecker CI lives at `https://mash.example.com/ci`), then you need to configure Forgejo to use the host's external IP when invoking webhooks from Woodpecker CI. You can do it by setting the following variables:

```yaml
forgejo_container_add_host_domain_name: "{{ woodpecker_ci_server_hostname }}"
forgejo_container_add_host_domain_ip_address: "{{ ansible_host }}"

# If ansible_host points to an internal IP address, you may need to allow Forgejo to make requests to it.
# By default, requests are only allowed to external IP addresses for security reasons.
# See: https://forgejo.org/docs/latest/admin/config-cheat-sheet/#webhook-webhook
forgejo_container_additional_environment_variables: |
  FORGEJO__webhook__ALLOWED_HOST_LIST=external,{{ ansible_host }}
```

## Migrating from Gitea

Forgejo is a fork of [Gitea](./gitea.md). Migrating Gitea (versions up to and including v1.22.0) to Forgejo was relatively easy, but [Gitea versions after v1.22.0 do not allow such transparent upgrades anymore](https://forgejo.org/2024-12-gitea-compatibility/).

Nevertheless, upgrades may be possible with some manual work.

Below is a rough guide to help you migrate from Gitea (tested with version v1.23.1) to Forgejo (v10.0.0).

> [!WARNING]
> 2FA does not seem to be working in Forgejo. If this is something you require to get working, do know that it's still an unsolved problem. Details for resetting/disabling 2FA are below.

1. Adjust `vars.yml` to enable the Forgejo service (prepare DNS records if using a dedicated hostname instead of your old Gitea hostname). We do not disable Gitea just yet.

2. Stop Gitea by running this on the server: `systemctl disable --now mash-gitea.service`

3. Dump the Gitea database and adapt for Forgejo

	Run these commands on the server:

	```sh
	mkdir /mash/gitea-to-forgejo-migration
	chown $(id -u mash):$(id -g mash) /mash/gitea-to-forgejo-migration

	docker run \
	--rm \
	--user=$(id -u mash):$(id -g mash) \
	--cap-drop=ALL \
	--env-file=/mash/postgres/env-postgres-psql \
	--mount type=bind,src=/mash/gitea-to-forgejo-migration,dst=/out \
	--network=mash-postgres \
	--entrypoint=/bin/sh \
	docker.io/postgres:17.2-alpine \
	-c 'set -o pipefail && pg_dump -h mash-postgres -d gitea > /out/forgejo.sql'

	sed --in-place 's/OWNER TO gitea/OWNER TO forgejo/g' /mash/gitea-to-forgejo-migration/forgejo.sql
	```

4. Prepare the `forgejo` Postgres database by running the playbook: `just run-tags install-postgres`

5. Import the database dump into the `forgejo` Postgres database

	Run this command on the server:

	```sh
	docker run \
	--rm \
	--user=$(id -u mash):$(id -g mash) \
	--cap-drop=ALL \
	--env-file=/mash/postgres/env-postgres-psql \
	--mount type=bind,src=/mash/gitea-to-forgejo-migration,dst=/out \
	--network=mash-postgres \
	--entrypoint=/bin/sh \
	docker.io/postgres:17.2-alpine \
	-c 'set -o pipefail && psql -h mash-postgres -d forgejo < /out/forgejo.sql'
	```

6. Install Forgejo using the playbook, but do not start it yet: `just run-tags install-forgejo`

7. Sync some files from Gitea by running these commands on the server

	```sh
	rsync -avr /mash/gitea/data/. /mash/forgejo/data/.

	# These files seem to live in a different place now, so we move them around.
	mv /mash/forgejo/data/data/gitea/repo-avatars /mash/forgejo/data/repo-avatars
	rmdir /mash/forgejo/data/data/gitea
	```

8. **For Gitea versions older than v1.22, skip this step**. For newer versions, revert the `forgejo` database to a schema migration version that Forgejo supports

	In this guide, we're using Gitea v1.23.1, the database schema version for which (`312`) is newer than what Forgejo supports (`305`).
	We need to [revert all migrations that are newer than `305`](https://github.com/go-gitea/gitea/tree/v1.23.1/models/migrations/v1_23).

	This may not always be possible or easy. For Gitea v1.23.1, the reverts can be done by running `/mash/postgres/bin/cli`, switching to the `forgejo` database (`\c forgejo`), and running the following queries:

	```sql
	-- Revert 311
	ALTER TABLE issue DROP COLUMN time_estimate;

	-- Revert 310
	ALTER TABLE protected_branch DROP column priority;

	-- Revert 309
	DROP INDEX IF EXISTS "IDX_notification_u_s_uu";
	DROP INDEX IF EXISTS "IDX_notification_user_id";
	DROP INDEX IF EXISTS "IDX_notification_repo_id";
	DROP INDEX IF EXISTS "IDX_notification_status";
	DROP INDEX IF EXISTS "IDX_notification_source";
	DROP INDEX IF EXISTS "IDX_notification_issue_id";
	DROP INDEX IF EXISTS "IDX_notification_commit_id";
	DROP INDEX IF EXISTS "IDX_notification_updated_by";

	-- Revert 308
	DROP INDEX IF EXISTS "IDX_action_r_u_d";
	DROP INDEX IF EXISTS "IDX_action_au_r_c_u_d";
	DROP INDEX IF EXISTS "IDX_action_c_u_d";
	DROP INDEX IF EXISTS "IDX_action_c_u";

	-- Revert 307
	-- Nothing to do

	-- Revert 306
	ALTER TABLE protected_branch DROP COLUMN block_admin_merge_override;

	-- Mark as reverted
	UPDATE version SET version = 305;
	```

9. Start Forgejo by running this on the server: `systemctl start mash-forgejo.service`

10. Complete the Forgejo installation on the web

	Forgejo may show an Installation page at the base URL with various configuration options, most of which prefilled.

	Our experience was that:

	- the "Run as user" field was empty, but required and "read only". Using the browser's inspector was necessary to remove the `readonly` attribute from the field, so we could enter a `git` value in it

	- the "Disable self-registration" checkbox (which is rightfully checked) had to be disabled to prevent an error saying: "You cannot disable user self-registration without creating an administrator account."

	After completing the installation, you should be able to access your new Forgejo instance.

11. Reset 2FA authentication settings for all users

	Forgejo seems to suffer from some issues when handling 2FA login and will return a "500 internal server error" while checking your 2FA code.
	This may be because some secret keys reset during the Gitea -> Forgejo migration, or most likely due to some bug.

	If you're experiencing issues with 2FA, run `/mash/postgres/bin/cli`, switch to the `forgejo` database (`\c forgejo`) and execute the following query:

	```psql
	DELETE FROM two_factor;
	```

	After this, you should be able to log in without being prompted for 2FA.
	Adding 2FA to your account later may be possible, but we also found issues with that:

	> SettingsTwoFactor: Failed to save two factor, pq: invalid byte sequence for encoding "UTF8": ...

	This is probably a bug.

12. If everything is working and you're happy with the migration, consider cleaning up

	You can do this by:

	- removing the Gitea configuration from `vars.yml` and [re-running the playbook](../installing.md) (e.g. `just run-tags setup-gitea`). This will delete the `/mash/gitea` directory
	- dropping the `gitea` database from the Postgres server (execute `/mash/postgres/bin/cli` and run `DROP DATABASE gitea;`)
	- deleting the `/mash/gitea-to-forgejo-migration` temporary directory: `rm -rf /mash/gitea-to-forgejo-migration`
````

## File: docs/services/freescout.md
````markdown
# Freescout

[Freescout](https://freescout.net/) is a free open-source helpdesk and shared inbox solution.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# freescout                                                            #
#                                                                      #
########################################################################

freescout_enabled: true

freescout_hostname: freescout.example.com

freescout_admin_email: your-email-here
freescout_admin_password: a-strong-password-here

########################################################################
#                                                                      #
# /freescout                                                           #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://freescout.example.com`.


## Usage

After installation, you can log in with your administrator login (`freescout_admin_email`) and password (`freescout_admin_password`).
````

## File: docs/services/freshrss.md
````markdown
# FreshRSS

[FreshRSS](https://freshrss.org) is a self-hosted RSS and Atom feed aggregator. It is lightweight, easy to work with, powerful, and customizable.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server
- an optional [Postgres](postgres.md) database, but FreshRSS will default to [SQLite](https://www.sqlite.org/) if you don't have Postgres enabled.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# freshrss                                                             #
#                                                                      #
########################################################################

freshrss_enabled: true

freshrss_hostname: freshrss.example.com

# Put a strong password below, generated with `pwgen -s 64 1` or in another way.
# You will need to use this password in the setup wizard after installation.
freshrss_database_password: ''

########################################################################
#                                                                      #
# /freshrss                                                            #
#                                                                      #
########################################################################
```

**NOTE**: while FreshRSS can potentially be installed under a subpath (using the `freshrss_path_prefix` variable), this [doesn't currently work](https://github.com/mother-of-all-self-hosting/mash-playbook/issues/116) and will be fixed in the future. For now, you'd need to install it on its own dedicated hostname.


## Usage

After installation, visit the configured path and complete the setup through the wizard. To do this you will need the database password from your `vars.yml` file (in the `freshrss_database_password` variable).

Feel free to follow FreshRSS [official documentation](http://freshrss.github.io/FreshRSS/en/).
````

## File: docs/services/funkwhale.md
````markdown
# Funkwhale

[Funkwhale](https://funkwhale.audio/) is a community-driven project that lets you listen and share music and audio within a decentralized, open network.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Valkey](valkey.md) data-store, installation details [below](#valkey)
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# funkwhale                                                               #
#                                                                      #
########################################################################

funkwhale_enabled: true

funkwhale_hostname: mash.example.com

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
funkwhale_django_secret_key: ''

# Valkey configuration, as described below

########################################################################
#                                                                      #
# /funkwhale                                                              #
#                                                                      #
########################################################################
```

### Valkey

As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require KeyDB on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to funkwhale](#creating-a-valkey-instance-dedicated-to-funkwhale).

If you're only running funkwhale on this server and don't need to use KeyDB for anything else, you can [use a single Valkey instance](#using-the-shared-valkey-instance-for-funkwhale).

#### Using the shared Valkey instance for funkwhale

To install a single (non-dedicated) Valkey instance (`mash-valkey`) and hook funkwhale to it, add the following **additional** configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# funkwhale                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point funkwhale to the shared Valkey instance
funkwhale_config_redis_hostname: "{{ valkey_identifier }}"

# Make sure the funkwhale API service (mash-funkwhale-api.service) starts after the shared KeyDB service
funkwhale_api_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the funkwhale API service (mash-funkwhale-api.service) is connected to the container network of the shared KeyDB service
funkwhale_api_container_additional_networks_custom:
  - "{{ valkey_container_network }}"

########################################################################
#                                                                      #
# /funkwhale                                                           #
#                                                                      #
########################################################################
```

This will create a `mash-valkey` Valkey instance on this host.

This is only recommended if you won't be installing other services which require KeyDB. Alternatively, go for [Creating a Valkey instance dedicated to funkwhale](#creating-a-valkey-instance-dedicated-to-funkwhale).


#### Creating a Valkey instance dedicated to funkwhale

The following instructions are based on the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `funkwhale.example.com` is your main one, create `funkwhale.example.com-deps`).

Then, create a new `vars.yml` file for the

`inventory/host_vars/funkwhale.example.com-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-funkwhale-'
mash_playbook_service_base_directory_name_prefix: 'funkwhale-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-funkwhale-valkey` instance on this host with its data in `/mash/funkwhale-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/funkwhale.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# funkwhale                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point funkwhale to its dedicated Valkey instance
funkwhale_config_redis_hostname: mash-funkwhale-valkey

# Make sure the funkwhale API service (mash-funkwhale-api.service) starts after its dedicated KeyDB service
funkwhale_api_systemd_required_services_list_custom:
  - "mash-funkwhale-valkey.service"

# Make sure the funkwhale API service (mash-funkwhale-api.service) is connected to the container network of its dedicated KeyDB service
funkwhale_api_container_additional_networks_custom:
  - "mash-funkwhale-valkey"

########################################################################
#                                                                      #
# /funkwhale                                                           #
#                                                                      #
########################################################################
```


## Installation

If you've decided to install a dedicated Valkey instance for funkwhale, make sure to first do [installation](../installing.md) for the supplementary inventory host (e.g. `funkwhale.example.com-deps`), before running installation for the main one (e.g. `funkwhale.example.com`).


## Usage

After installation, you can go to the funkwhale URL, as defined in `funkwhale_hostname`. To login and get started you first have to create a user. you can do this with
```bash
just run-tags funkwhale-add-superuser --extra-vars=username=USERNAME --extra-vars=password=PASSWORD --extra-vars=email=EMAIL
```

All other users can be created in the Web GUI.
````

## File: docs/services/gitea.md
````markdown
# Gitea

[Gitea](https://gitea.io/) is a painless self-hosted Git service. You may also wish to look into [Forgejo](https://forgejo.org/) — a fork of Gitea that this playbook also [supports](forgejo.md).

> [!WARNING]
> [Gitea is Open Core](https://codeberg.org/forgejo/discussions/issues/102) and your interests may be better served by using and supporting [Forgejo](forgejo.md) instead. See the [Comparison with Gitea](https://forgejo.org/compare-to-gitea/) page for more information. You may also wish to see our [Migrating from Gitea](./forgejo.md#migrating-from-gitea) guide.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# gitea                                                                #
#                                                                      #
########################################################################

gitea_enabled: true

# Gitea uses port 22 by default.
# We recommend that you move your regular SSH server to another port,
# and stick to this default.
#
# If you wish to use another port, uncomment the variable below
# and adjust the port as you see fit.
# gitea_ssh_port: 222

gitea_hostname: mash.example.com
gitea_path_prefix: /gitea

########################################################################
#                                                                      #
# /gitea                                                               #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/gitea`.

You can remove the `gitea_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


## Usage

After installation, you should be able to access your new Gitea instance at the configured URL (see above).

Going there, you'll be taken to the initial setup wizard, which will let you assign some paswords and other configuration.


## Recommended other services

You may also wish to look into [Woodpecker CI](woodpecker-ci.md), which can integrate nicely with Gitea.
````

## File: docs/services/gotosocial.md
````markdown
# GoToSocial

[GoToSocial](https://gotosocial.org/) is a self-hosted [ActivityPub](https://activitypub.rocks/) social network server, that this playbook can install, powered by the [mother-of-all-self-hosting/ansible-role-gotosocial](https://github.com/mother-of-all-self-hosting/ansible-role-gotosocial) Ansible role.

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server
- (optional) the [exim-relay](exim-relay.md) mailer


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# gotosocial                                                           #
#                                                                      #
########################################################################

gotosocial_enabled: true

# Hostname that this server will be reachable at.
# DO NOT change this after your server has already run once, or you will break things!
# Examples: ["gts.example.org","some.server.com"]
gotosocial_hostname: 'social.example.org'

########################################################################
#                                                                      #
# /gotosocial                                                          #
#                                                                      #
########################################################################
```

## Advanced account domain configuration

The account domain is the second part of a user handle in the Fediverse. If your handle is @username@example.org, `example.org` is your account domain. By default GoToSocial will use `gotosocial_hostname` that you provide as account domain e.g. `social.example.org`. You might want to change this by setting `gotosocial_account_domain` if you want the domain on accounts to be `example.org` because it looks better or is just shorter/easier to remember.

> [!WARNING]
> DO NOT change this change this after your server has already run once, or you will break things!

If you decide to use this read [the appropriate section of the documentation](https://docs.gotosocial.org/en/latest/advanced/host-account-domain/) as you will have to do some additional work on the base domain.

```yaml
gotosocial_account_domain: "example.org"
```

## E-Mail configuration

You can use the following variables in your `vars.yml` to enable e-mail notifications.

```yml
# Check out https://docs.gotosocial.org/en/latest/configuration/smtp/ for a configuration reference
gotosocial_smtp_host: 'smtp.example.org'
gotosocial_smtp_username: gotosocial@example.org
gotosocial_smtp_password: yourpassword
gotosocial_smtp_from: gotosocial@example.org
```

## Usage

After [installing](../installing.md), you can:

- create an **administrator** user account with a command like this: `just run-tags gotosocial-add-admin --extra-vars=username=USERNAME_HERE --extra-vars=password=PASSWORD_HERE --extra-vars=email=EMAIL_HERE`

- create a **regular** (non-administrator) user account with a command like this: `just run-tags gotosocial-add-user --extra-vars=username=USERNAME_HERE --extra-vars=password=PASSWORD_HERE --extra-vars=email=EMAIL_HERE`

Then, you should be able to visit the URL specified in `gotosocial_hostname` and see your instance.

To customize your instance, go to the `/admin` page.

Use the [GtS CLI Tool](https://docs.gotosocial.org/en/latest/admin/cli/) to do admin & maintenance tasks. E.g. use
```bash
docker exec -it mash-gotosocial /gotosocial/gotosocial admin account demote --username USERNAME_HERE
```
to demote a user from admin to normal user.

Refer to the [great official documentation](https://docs.gotosocial.org/en/latest/) for more information on GoToSocial.



## Migrate an existing instance

The following assumes you want to migrate from `serverA` to `serverB` (managed by mash) but you just cave to adjust the copy commands if you are on the same server.

Stop the initial instance on `serverA`

```bash
serverA$ systemctl stop gotosocial
```

Dump the database (depending on your existing setup you might have to adjust this)
```
serverA$ pg_dump gotosocial > latest.sql
```

Copy the files to the new server

```bash
serverA$ rsync -av -e "ssh" latest.sql root@serverB:/mash/gotosocial/
serverA$ rsync -av -e "ssh" data/* root@serverB:/mash/gotosocial/data/
```

Install (but don't start) the service and database on the server.

```bash
yourPC$ just run-tags install-all
yourPC$ just run-tags import-postgres --extra-vars=server_path_postgres_dump=/mash/gotosocial/latest.sql --extra-vars=postgres_default_import_database=mash-gotosocial
```

Start the services on the new server

```bash
yourPC$ just run-tags start
```

Done 🥳
````

## File: docs/services/grafana-loki.md
````markdown
# Grafana Loki

[Grafana Loki](https://grafana.com/docs/loki/latest/) is a set of components that can be composed into a fully featured logging stack. Installing it is powered by the [mother-of-all-self-hosting/ansible-role-loki](https://github.com/mother-of-all-self-hosting/ansible-role-loki) Ansible role.

Loki is just a log storage system. In order to make use of it, you'd need at least 2 other components

- some agent (like [Promtail](./promtail.md)) to send logs to Loki
- some system (like [Grafana](./grafana.md)) to read the logs out of Loki and display them nicely


## Dependencies

This service requires the following other services:

- (optionally) [Traefik](traefik.md) — a reverse-proxy server for exposing Loki publicly
- (optionally) [Promtail](./promtail.md) — an agent that can send logs to Loki
- (optionally) [Grafana](./grafana.md) — a web UI that can query the Loki datasource (connection) and display the logs


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# loki                                                                 #
#                                                                      #
########################################################################

loki_enabled: true

########################################################################
#                                                                      #
# /loki                                                                #
#                                                                      #
########################################################################
```

### Exposing the web interface

By setting a hostname and optionally a path prefix, you can expose Loki publicly. You may wish to do this, if you'd like to be able to:

- push logs from remote agents (e.g. Promtail installed on remote machines, etc.)
- query logs from remote systems (e.g. Grafana installed elsewhere)

When exposing publicly, it's natural to set up [HTTP Basic Authentication](https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication) **or anyone would be able to read your logs or push new ones**.

```yaml
loki_hostname: mash.example.com
loki_path_prefix: /loki

# If you are sure you wish to run without Basic Auth enabled,
# explicitly set the variable below to false.
loki_container_labels_middleware_basic_auth_enabled: true
# Use `htpasswd -nb USERNAME PASSSWORD` to generate the users below.
loki_container_labels_middleware_basic_auth_users: ''
```


## Usage

After [installing](../installing.md), refer to the [official documentation](https://grafana.com/docs/loki/latest/reference/api/#post-lokiapiv1push) to send logs to loki's API without an agent or set up one or more instances of the [Promtail](./promtail.md) agent.


## Recommended other services

- [Grafana](grafana.md) — a web-based tool for visualizing your Promtail logs (stored in [Grafana Loki](grafana-loki.md) or elsewhere)
- [Promtail](promtail.md) — an agent which ships the contents of local logs to a private [Grafana Loki](grafana-loki.md) instance
````

## File: docs/services/grafana.md
````markdown
# Grafana

[Grafana](https://grafana.com/) is an open and composable observability and data visualization platform, often used with [Prometheus](prometheus.md).


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# grafana                                                              #
#                                                                      #
########################################################################

grafana_enabled: true

grafana_hostname: mash.example.com
grafana_path_prefix: /grafana

grafana_default_admin_user: admin
# Generating a strong password (e.g. `pwgen -s 64 1`) is recommended
grafana_default_admin_password: ''

########################################################################
#                                                                      #
# /grafana                                                             #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/grafana`.

You can remove the `grafana_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

### Configuring data sources

Grafana is merely a visualization tool. It needs to pull data from a metrics (time-series) database, like [Prometheus](prometheus.md).

You can add multiple data sources to Grafana.

Below, we show a few examples of connecting Grafana to local datasources (running in containers on the same machine).
**If you're enabling multiple, you need to "merge" the configurations**. That is, don't define `grafana_provisioning_datasources` or `grafana_container_additional_networks_custom` twice, but combine them.

#### Integrating with a local Prometheus instance

If you're installing [Prometheus](prometheus.md) on the same server, you can hook Grafana to it over the container network with the following **additional** configuration:

```yaml
grafana_provisioning_datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: "http://{{ prometheus_identifier }}:9090"
    jsonData:
      timeInterval: "{{ prometheus_config_global_scrape_interval }}"
    # Enable below if connecting to a remote instance that uses Basic Auth.
    # basicAuth: true
    # basicAuthUser: loki
    # secureJsonData:
    #   basicAuthPassword: ""

# Prometheus runs in another container network, so we need to connect to it.
grafana_container_additional_networks_custom:
  - "{{ prometheus_container_network }}"
```

For connecting to a **remote** Prometheus instance, you may need to adjust this configuration.

#### Integrating with a local Loki instance

If you're installing [Grafana Loki](grafana-loki.md) on the same server, you can hook Grafana to it over the container network with the following **additional** configuration:

```yaml
grafana_provisioning_datasources:
  - name: Loki (your-tenant-id)
    type: loki
    access: proxy
    url: "http://{{ loki_identifier }}:{{ loki_server_http_listen_port }}"
    # Enable below and also (basicAuthPassword) if connecting to a remote instance that uses Basic Auth.
    # basicAuth: true
    # basicAuthUser: loki
    jsonData:
      httpHeaderName1: X-Scope-OrgID
    secureJsonData:
      httpHeaderValue1: "your-tenant-id"
      # basicAuthPassword: ""

# Loki runs in another container network, so we need to connect to it.
grafana_container_additional_networks_custom:
  - "{{ loki_container_network }}"
```

For connecting to a **remote** Loki instance, you may need to adjust this configuration.

If you're installing [Promtail](./promtail.md) on the same server as Loki, by default it's configured to send `mash` as the tenant ID.

### Integrating with Prometheus Node Exporter

If you've installed [Prometheus Node Exporter](prometheus-node-exporter.md) on any host (target) scraped by Prometheus, you may wish to install a dashboard for Prometheus Node Exporter.

The Prometheus Node Exporter role exposes a list of URLs containing dashboards (JSON files) in its `prometheus_node_exporter_dashboard_urls` variable.

You can add this **additional** configuration to make the Grafana service pull these dashboards:

```yaml
grafana_dashboard_download_urls: |
  {{
    prometheus_node_exporter_dashboard_urls
  }}
```

### Single-Sign-On

Grafana supports Single-Sign-On (SSO) via OAUTH. To make use of this you'll need a Identity Provider like [authentik](./authentik.md), [Keycloak](./keycloak.md) or [Authelia](./authelia.md).

Below, you can find some examples for Grafana configuration.


#### Single-Sign-On / Authentik

* Create a new OAUTH provider in authentik called `grafana`
* Create an application also named `grafana` in authentik using this provider
* Add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process (make sure to adjust `authentik.example.com`)

```yaml
# To make Grafana honor the expiration time of JWT tokens, enable this experimental feature below.
# grafana_feature_toggles_enable: accessTokenExpirationCheck

grafana_environment_variables_additional_variables: |
  GF_AUTH_GENERIC_OAUTH_ENABLED=true
  GF_AUTH_GENERIC_OAUTH_NAME=authentik
  GF_AUTH_GENERIC_OAUTH_CLIENT_ID=COPIED-CLIENTID
  GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=COPIED-CLIENTSECRET
  GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email
  GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://authentik.example.com/application/o/authorize/
  GF_AUTH_GENERIC_OAUTH_TOKEN_URL=https://authentik.example.com/application/o/token/
  GF_AUTH_GENERIC_OAUTH_API_URL=https://authentik.example.com/application/o/userinfo/
  GF_AUTH_SIGNOUT_REDIRECT_URL=https://authentik.example.com/application/o/grafana/end-session/
  # Optionally enable auto-login (bypasses Grafana login screen)
  #GF_AUTH_OAUTH_AUTO_LOGIN="true"
  GF_AUTH_GENERIC_OAUTH_ALLOW_ASSIGN_GRAFANA_ADMIN=true
  # Optionally map user groups to Grafana roles
  GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'Grafana Editors') && 'Editor' || 'Viewer'
```

Make sure the user you want to login as has an email address in authentik, otherwise there will be an error.


#### Single-Sign-On / Authelia

The configuration flow below assumes [Authelia](authelia.md) configured via the playbook, but you can run Authelia in another way too.

- Come up with a client ID you'd like to use. Example: `grafana`
- Generate a shared secret for the OpenID Connect application: `pwgen -s 64 1`. This is to be used in `GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET` below
- Hash the shared secret for use in Authelia's configuration (`authelia_config_identity_providers_oidc_clients`): `php -r 'echo password_hash("PASSWORD_HERE",  PASSWORD_ARGON2ID);'`. Feel free to use another language (or tool) for creating a hash as well. A few different hash algorithms are supported besides Argon2id.
- Define this `grafana` client in Authelia via `authelia_config_identity_providers_oidc_clients`. See [example configuration](authelia.md#protecting-a-service-with-openid-connect) on the Authelia documentation page.

```yaml
# To make Grafana honor the expiration time of JWT tokens, enable this experimental feature below.
# grafana_feature_toggles_enable: accessTokenExpirationCheck

grafana_environment_variables_additional_variables: |
  GF_AUTH_GENERIC_OAUTH_ENABLED=true
  GF_AUTH_GENERIC_OAUTH_NAME=Authelia
  GF_AUTH_GENERIC_OAUTH_CLIENT_ID=grafana
  GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=PLAIN_TEXT_SHARED_SECRET
  GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email groups
  GF_AUTH_GENERIC_OAUTH_EMPTY_SCOPES=false
  GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://authelia.example.com/api/oidc/authorization
  GF_AUTH_GENERIC_OAUTH_TOKEN_URL=https://authelia.example.com/api/oidc/token
  GF_AUTH_GENERIC_OAUTH_API_URL=https://authelia.example.com/api/oidc/userinfo
  GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH=preferred_username
  GF_AUTH_GENERIC_OAUTH_GROUPS_ATTRIBUTE_PATH=groups
  GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH=name
  GF_AUTH_GENERIC_OAUTH_USE_PKCE=true
```

## Usage

After installation, you should be able to access your new Grafana instance at the configured URL (see above).

Going there, you'll be taken to the initial setup wizard, which will let you assign some paswords and other configuration.


## Recommended other services

Grafana is just a visualization tool which requires pulling data from a metrics (time-series) database like.

You may be interested in combining it with [Prometheus](prometheus.md).
````

## File: docs/services/headscale.md
````markdown
# Headscale

[Headscale](https://headscale.net/) is an open source, self-hosted implementation of the [Tailscale](https://tailscale.com/) control server.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# headscale                                                            #
#                                                                      #
########################################################################

headscale_enabled: true

headscale_hostname: headscale.example.com

########################################################################
#                                                                      #
# /headscale                                                           #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://headscale.example.com`.

The `headscale_path_prefix` variable can be adjusted to host Headscale under a subpath (e.g. `headscale_path_prefix: /headscale`) on the given hostname.


## Usage

After installation, you would normally:

- first, [create some users](#creating-users)
- then, [connect some devices](#connecting-devices) by the official Tailscale applications, configured to talk to your own Headscale server

### Convenience script to call the binary

We provide a `/mash/headscale/bin/headscale` script on the server, which forwards commands to the `headscale` binary inside the container (`mash-headscale`).

Example usage: `/mash/headscale/bin/headscale version`

> [!WARNING]
> Command arguments which contain spaces may not be forwarded correctly.

### Creating users

To [create a user](https://headscale.net/stable/usage/getting-started/#create-a-user), run a command like this:

```sh
/usr/bin/env docker exec -it mash-headscale \
headscale users create \
john.doe \
--display-name "John Doe" \
--email "john.doe@example.com"
```

> [!WARNING]
> We use `docker exec` here because the [convenience script](#convenience-script-to-call-the-binary) does not handle forwarding arguments with spaces (like `--display-name`) correctly.

💡 You can [list the existing users](https://headscale.net/stable/usage/getting-started/#list-existing-users) with a command like this: `/mash/headscale/bin/headscale users list`

### Connecting devices

Here are some quick guides for the various platforms:

- [Android devices](https://headscale.net/stable/usage/connect/android/)
- [Apple devices](https://headscale.net/stable/usage/connect/apple/)
- [Windows devices](https://headscale.net/stable/usage/connect/windows/)
- Linux: install the `tailscale` CLI. See the official [Setting up Tailscale on Linux](https://tailscale.com/kb/1031/install-linux) documentation, or the [Archlinux Tailscale Wiki page](https://wiki.archlinux.org/title/Tailscale) (and specifically its [Third-party clients](https://wiki.archlinux.org/title/Tailscale#Third-party_clients) section for GUI clients).

All of these platforms would require confirmation after initial login, so consult the [Connecting Linux devices with manual confirmation](#connecting-linux-devices-with-manual-confirmation) section below for details on how to do it.

#### Connecting Linux devices with manual confirmation

To connect a Linux device, you can use a `tailscale up` command like this:

```sh
tailscale up --login-server=https://headscale.example.com
```

💡 You may wish to add additional arguments to this command, such as `--hostname`, `--advertise-exit-node`, `--advertise-routes`, etc. These settings can also be configured later using `tailscale set` (e.g. `tailscale set --hostname=custom-hostname-for-my-device`).

Running the `tailscale up` command will print a URL you need to open in your browser to complete the setup.

The URL would contain a `headscale` command you need to run. It looks something like this:

```sh
headscale nodes register --user USERNAME --key mkey:....
```

Take this command and:

- replace the `headscale` prefix with `/mash/headscale/bin/headscale`
- replace `USERNAME` with the username of a valid [user you created](#creating-users) earlier
- run it on the Headscale server

#### Connecting Linux devices with a preshared key

Instead of following the manual back-and-forth flow as specified in [Connecting Linux devices with manual confirmation](#connecting-linux-devices-with-manual-confirmation), you can also use a preshared key to connect your device.

First, generate a preshared key:

```sh
/mash/headscale/bin/headscale preauthkeys create --user=john.doe
```

You can then connect your device with the preshared key:

```sh
tailscale up --login-server=https://headscale.example.com --auth-key=...
```

The device will be automatically connected to the Headscale server, without any additional approval steps.
````

## File: docs/services/healthchecks.md
````markdown
# Healthchecks

[Healthchecks](https://healthchecks.io/) is simple and Effective **Cron Job Monitoring** solution.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server
- (optional) the [exim-relay](exim-relay.md) mailer


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# healthchecks                                                         #
#                                                                      #
########################################################################

healthchecks_enabled: true

healthchecks_hostname: mash.example.com

healthchecks_path_prefix: /healthchecks

########################################################################
#                                                                      #
# /healthchecks                                                        #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/healthchecks`.

You can remove the `healthchecks_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

### Authentication

The first superuser account is created after installation. See [Usage](#usage).
You can create as many accounts as you wish.

### Email integration

If you've enabled the [exim-relay](exim-relay.md) mailer service, Healtchecks will automatically be configured to send through it.

If you need to configure Healthchecks to send email directly, the [ansible.role.healthchecks](https://github.com/mother-of-all-self-hosting/ansible-role-healthchecks) Ansible role provides various variables for tweaking the email-sending configuration in its `default/main.yml` file (`healthchecks_environment_variable_default_from_email` and various `healthchecks_environment_variable_email_*` variables).

### Integrating with other services

Refer to the [upstream `.env.example` file](https://github.com/healthchecks/healthchecks/blob/master/docker/.env.example) for discovering additional environment variables.

You can pass these to the Healthchecks container using the `healthchecks_environment_variables_additional_variables` variable. Example:

```yml
healthchecks_environment_variables_additional_variables: |
  DISCORD_CLIENT_ID=123
  DISCORD_CLIENT_SECRET=456
```


## Usage

After installation, you need to **create a superuser account**.
This is an interactive process which can be initiated by **SSH-ing into into the server** and **running a command** like this:

```sh
docker exec -it mash-healthchecks /opt/healthchecks/manage.py createsuperuser
```

After creating the superuser account, you can go to the [Healthchecks URL](#url) to log in and start setting up healthchecks.


## Recommended other services

- [Prometheus](prometheus.md) — a metrics collection and alerting monitoring solution
````

## File: docs/services/hubsite.md
````markdown
# Hubsite

[Hubsite](https://github.com/moan0s/hubsite) is will provide you with a simple, static site that shows an overview of the available services.

You can use the following variables to enable & control your hubsite:

```yaml
########################################################################
#                                                                      #
# hubsite                                                              #
#                                                                      #
########################################################################

hubsite_enabled: true

hubsite_hostname: mash.example.com

hubsite_title: "My services"
hubsite_subtitle: "Just click on a service to use it"

# Use the `hubsite_service_list_additional` variable to add services that are not provided by this playbook
# hubsite_service_list_additional: |
#   {{
#     ([{'name': 'My blog', 'url': 'https://example.com', 'logo_location': '', 'description': 'A link to a blog not hosted by this playbook', 'priority': 1000 }])
#   }}

# If you want to explicitly control which services you want to show on this page you can overwrite
# hubsite_service_list_auto: |
#   {{
#     ([{'name': 'Miniflux', 'url': hubsite_service_miniflux_url, 'logo_location': '{{ role_path }}/assets/miniflux.png', 'description': 'An opinionated feed reader', 'priority': hubsite_service_miniflux_priority}] if hubsite_service_miniflux_enabled else [])
#     +
#     ([{'name': 'Uptime Kuma', 'url': hubsite_service_uptime_kuma_url, 'logo_location': '{{ role_path }}/assets/uptime-kuma.png', 'description': 'Check the status of the services', 'priority': hubsite_service_uptime_kuma_priority}] if hubsite_service_uptime_kuma_enabled else [])
#   }}
########################################################################
#                                                                      #
# /hubsite                                                             #
#                                                                      #
########################################################################
```

You can SSO-protect this website with the help of [Authelia](authelia.md) or [OAuth2-Proxy](oauth2-proxy.md) (connected to any OIDC provider).
````

## File: docs/services/ilmo.md
````markdown
# ILMO

[Ilmo](https://github.com/moan0s/ILMO2) is an open source library management tool.

Read [the documentation](https://ilmo2.readthedocs.io/) to learn what you can do with it.

> [!WARNING]
> This service is a custom solution for a small library. Feel free to use it but don't expect a solution that works for every use case

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# ilmo                                                                 #
#                                                                      #
########################################################################

ilmo_enabled: true
ilmo_hostname: ilmo.example.com
ilmo_instance_name: "My library"

########################################################################
#                                                                      #
# /ilmo                                                                #
#                                                                      #
########################################################################
```

## Setting up the first user

You need to create a first user (unless you import an existing database).
You can do this conveniently by running

```bash
just run-tags ilmo-add-superuser --extra-vars=username=USERNAME --extra-vars=password=PASSWORD --extra-vars=email=EMAIL
```

## Usage

After installation, you can go to the ILMO URL, as defined in `ilmo_hostname`. Log in with the user credentials from above.

Follow the [ILMO documentation](https://ilmo2.readthedocs.io/en/latest/index.html) to learn how to use ILMO.

## Migrate an existing instance

The following assumes you want to migrate from `serverA` to `serverB` (managed by mash) but you just cave to adjust the copy commands if you are on the same server.

Stop the initial instance on `serverA`

```bash
serverA$ systemctl stop ilmo
```

Dump the database (depending on your existing setup you might have to adjust this)
```
serverA$ pg_dump ilmo > latest.sql
```

Copy the files to the new server

```bash
serverA$ rsync -av -e "ssh" latest.sql root@serverB:/mash/ilmo/
serverA$ rsync -av -e "ssh" data/* root@serverB:/mash/ilmo/data/
```

Install (but don't start) the service and database on the server and import the database.

```bash
yourPC$ just run-tags install-postgres
yourPC$ just run-tags install-ilmo
yourPC$ just run-tags import-postgres --extra-vars=server_path_postgres_dump=/mash/ilmo/latest.sql --extra-vars=postgres_default_import_database=mash-ilmo
```

Start the services on the new server

```bash
yourPC$ just run-tags start
```

Done 🥳

### Troubleshooting

If you by accident started the service before importing the database you should

* stop the service
* use `/mash/postgres/bin/cli` to get a database interface
* Delete the existing database (THIS WILL DELETE ALL DATA!) `DROP DATABASE ilmo WITH (FORCE);`
* Continue from "Install (but don't start) the service and database on the server and import the database."
````

## File: docs/services/infisical.md
````markdown
# Infisical

[Infisical](https://infisical.com/) is an open-source end-to-end encrypted platform for securely managing secrets and configs across your team, devices, and infrastructure.


## Dependencies

This service requires the following other services:

- a [MongoDB](mongodb.md) document-oriented database server
- a [Traefik](traefik.md) reverse-proxy server
- a [Valkey](valkey.md) data-store, installation details [below](#valkey)


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# infisical                                                            #
#                                                                      #
########################################################################

infisical_enabled: true

infisical_hostname: infisical.example.com

# Generate this with: `openssl rand -hex 16`
infisical_backend_environment_variable_encryption_key: ''

# WARNING: uncomment this after creating your first user account,
# unless you'd like to run a server with public registration enabled.
# infisical_backend_environment_variable_invite_only_signup: true

########################################################################
#                                                                      #
# /infisical                                                           #
#                                                                      #
########################################################################
```


### URL

In the example configuration above, we configure the service to be hosted at `https://infisical.example.com`.

Hosting Infisical under a subpath (by configuring the `infisical_path_prefix` variable) does not seem to be possible right now, due to Infisical limitations.


### Authentication

Public registration can be enabled/disabled using the `infisical_backend_environment_variable_invite_only_signup` variable.

We recommend installing with public registration enabled at first (which is the default value for this variable), creating your first user account, and then disabling public registration by explicitly setting `infisical_backend_environment_variable_invite_only_signup` to `true`. Enabling invite-only signup requires that you configure [Email configuration](#email-configuration)


### Valkey

As described on the Valkey documentation page, if you're hosting additional services which require Valkey on the same server, you'd better go for installing a separate Valkey instance for each service. See Creating a Valkey instance dedicated to Infisical.

If you're only running Infisical on this server and don't need to use Valkey for anything else, you can use a single Valkey instance.
Using the shared Valkey instance for Infisical

To install a single (non-dedicated) Valkey instance (mash-valkey) and hook Infisical to it, add the following additional configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# infisical                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point Infisical to the shared Valkey instance
infisical_environment_variable_redis_host: "{{ valkey_identifier }}"
infisical_environment_variable_redis_cache_host: "{{ valkey_identifier }}"

# Make sure the Infisical service (mash-infisical.service) starts after the shared Valkey service (mash-valkey.service)
infisical_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the Infisical container is connected to the container network of the shared Valkey service (mash-valkey)
infisical_container_additional_networks_custom:
  - "{{ valkey_identifier }}"

########################################################################
#                                                                      #
# /infisical                                                           #
#                                                                      #
########################################################################
```

This will create a mash-valkey Valkey instance on this host.

This is only recommended if you won't be installing other services which require Valkey. Alternatively, go for Creating a Valkey instance dedicated to Infisical.
Creating a Valkey instance dedicated to Infisical

The following instructions are based on the Running multiple instances of the same service on the same host documentation.

Adjust your inventory/hosts file as described in Re-do your inventory to add supplementary hosts, adding a new supplementary host (e.g. if infisical.example.com is your main one, create infisical.example.com-deps).

Then, create a new vars.yml file for the

inventory/host_vars/infisical.example.com-deps/vars.yml:

```yaml

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-infisical-'
mash_playbook_service_base_directory_name_prefix: 'infisical-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################

This will create a mash-infisical-valkey instance on this host with its data in /mash/infisical-valkey.

Then, adjust your main inventory host's variables file (inventory/host_vars/infisical.example.com/vars.yml) like this:

########################################################################
#                                                                      #
# infisical                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above


# Point Infisical to its dedicated Valkey instance
infisical_environment_variable_redis_host: mash-infisical-valkey
infisical_environment_variable_redis_cache_host: mash-infisical-valkey

# Make sure the Infisical service (mash-infisical.service) starts after its dedicated Valkey service (mash-infisical-valkey.service)
infisical_systemd_required_services_list_custom:
  - "mash-infisical-valkey.service"

# Make sure the Infisical container is connected to the container network of its dedicated Valkey service (mash-infisical-valkey)
infisical_container_additional_networks_custom:
  - "mash-infisical-valkey"

########################################################################
#                                                                      #
# /infisical                                                           #
#                                                                      #
########################################################################
```

### Email configuration

As described in the Infisical documentation about [Email](https://infisical.com/docs/self-hosting/configuration/email), some important functionality requires email-sending to be configured.

Here are some additional variables you can add to your `vars.yml` file:

```yaml
infisical_backend_environment_variable_smtp_host: smtp.example.com
infisical_backend_environment_variable_smtp_port: 587
infisical_backend_environment_variable_smtp_secure: false

infisical_backend_environment_variable_smtp_username: infisical@example.com
infisical_backend_environment_variable_smtp_password: ''

infisical_backend_environment_variable_smtp_address: infisical@example.com
infisical_backend_environment_variable_smtp_name: Infisical
```

For additional SMTP-related variables, consult the [`defaults/main.yml` file](https://github.com/mother-of-all-self-hosting/ansible-role-infisical/blob/main/defaults/main.yml) in the [ansible-role-infisical](https://github.com/mother-of-all-self-hosting/ansible-role-infisical) Ansible role.


## Usage

After installation, you can go to the Infisical URL, as defined in `infisical_hostname`.

As mentioned in [Authentication](#authentication) above, you can create the first user from the web interface.

If you'd like to prevent other users from registering, consider disabling public registration as described in the [Authentication](#authentication) section and re-running the playbook (`just install-service infisical`).
````

## File: docs/services/influxdb.md
````markdown
# InfluxDB

[InfluxDB](https://www.influxdata.com/) is a self-hosted time-series database, that this playbook can install, powered by the [mother-of-all-self-hosting/ansible-role-influxdb](https://github.com/mother-of-all-self-hosting/ansible-role-influxdb) Ansible role.

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# influxdb                                                             #
#                                                                      #
########################################################################

influxdb_enabled: true
influxdb_hostname: 'example.org'

# Advanced configuration
# Configure the inital user, organization and bucket
#
# This setting will only be used once upon initial installation of influxdb. Changing this values
# after the first start of influxdb will have no effect.
# Not setting this will allow you to manually set these by visiting the domain you set in influxdb_hostname
# after installation.
#influxdb_init: true
#influxdb_init_username: "USERNAME"
#influxdb_init_password: "SUPERSECRETPASSWORD"
#influxdb_init_org: "EXAMPLE_ORG"
#influxdb_init_bucket: "SOMEBUCKET"
#
# In order to let external services (like Proxmox or Grafana) access the http API of influxdb,
# you need to specifically expose the corresponding port:
#
# Takes an "<ip>:<port>" (e.g. "127.0.0.1:8086") or "<port>" value (e.g. "8086"), or empty string to not expose.
#influxdb_container_http_host_bind_port: ""

########################################################################
#                                                                      #
# /influxdb                                                            #
#                                                                      #
########################################################################
```

After installation, visit the domain you set in `influxdb_hostname` to get started.

## Usage

After [installing](../installing.md), you can visit at the URL specified in `influxdb_hostname` and configure your first user (or login if you set `influxdb_init`)
````

## File: docs/services/jackett.md
````markdown
# Jackett

[Jackett](https://github.com/Jackett/Jackett) is an API for your favorite torrent trackers.

> Jackett works as a proxy server: it translates queries from apps ([Sonarr](https://github.com/Sonarr/Sonarr), [Radarr](https://github.com/Radarr/Radarr), [SickRage](https://sickrage.github.io/), [CouchPotato](https://couchpota.to/), [Mylar3](https://github.com/mylar3/mylar3), [Lidarr](https://github.com/lidarr/lidarr), [DuckieTV](https://github.com/SchizoDuckie/DuckieTV), [qBittorrent](https://www.qbittorrent.org/), [Nefarious](https://github.com/lardbit/nefarious), [NZBHydra2](https://github.com/theotherp/nzbhydra2) etc.) into tracker-site-specific http queries, parses the html or json response, and then sends results back to the requesting software. This allows for getting recent uploads (like RSS) and performing searches. Jackett is a single repository of maintained indexer scraping & translation logic — removing the burden from other apps.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# jackett                                                              #
#                                                                      #
########################################################################

jackett_enabled: true

jackett_hostname: jackett.example.com

# To mount additional data directories, use `jackett_container_additional_volumes`
#
# Example:
# jackett_container_additional_volumes:
#   - type: bind
#     src: /path/to/blackhole
#     dst: /downloads

########################################################################
#                                                                      #
# /jackett                                                             #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://jackett.example.com`.

A `jackett_path_prefix` variable can be adjusted to host under a subpath (e.g. `jackett_path_prefix: /jackett`), but this hasn't been tested yet.

## Usage

After [installation](../installing.md), you should access your new Jackett instance at the URL you've chosen and configure a Admin password.

For additional configuration options, refer to [ansible-role-jackett](https://github.com/spatterIight/ansible-role-jackett)'s `defaults/main.yml` file.

## Command-line arguments

Additional command line arguments can be passed to Jackett by use of the `RUN_OPTS` environment variable. To specify this, add the following to your configuration:

```yaml
# To add additional environment variables, use `jackett_container_additional_environment_variables`
#
# Example:
# jackett_container_additional_environment_variables: |
#   RUN_OPTS="--IgnoreSslErrors true --ProxyConnection 192.168.10.3:9999"
```

The full list of available arguments is as follows:

```sh
Jackett v0.22.1377
  -i, --Install            Install Jackett windows service (Must be admin)

  -r, --ReserveUrls        (Re)Register windows port reservations (Required for
                           listening on all interfaces).

  -u, --Uninstall          Uninstall Jackett windows service (Must be admin).

  -l, --Logging            Log all requests/responses to Jackett

  -t, --Tracing            Enable tracing

  -c, --UseClient          Override web client selection.
                           [automatic(Default)/httpclient/httpclient2]

  -s, --Start              Start the Jacket Windows service (Must be admin)

  -k, --Stop               Stop the Jacket Windows service (Must be admin)

  -x, --ListenPublic       Listen publicly

  -z, --ListenPrivate      Only allow local access

  -p, --Port               Web server port

  -n, --IgnoreSslErrors    [true/false] Ignores invalid SSL certificates

  -d, --DataFolder         Specify the location of the data folder (Must be
                           admin on Windows) eg. --DataFolder="D:\Your
                           Data\Jackett\". Don't use this on Unix (mono)
                           systems. On Unix just adjust the HOME directory of
                           the user to the datadir or set the XDG_CONFIG_HOME
                           environment variable.

  --NoRestart              Don't restart after update

  --PIDFile                Specify the location of PID file

  --NoUpdates              Disable automatic updates

  --help                   Display this help screen.

  --version                Display version information.
```

## Adding an Indexer

Once you've installed Jackett and setup an admin password you can start configuring it. One of the first things you're likely to want to do is configure some indexers. An indexer is basically a tracker, which can be either public, semi-private, or private.

To add an indexer, click the `+ Add indexer` button and select your tracker from the list.

![Jackett Add Indexer](../assets/jackett/add-indexer.png)

If its a semi-private or private tracker you will have to add some specific configuration, like a username and password. If its public you can just add it as-is.

Once its added you can test it using the `Test ✓` button, if it returns successfully you're good to go!

## Intergration with Sonarr/Radarr

To add Jackett to your [Sonarr](sonarr.md) or [Radarr](radarr.md) instance navigate to the form at `Settings > Indexers > Add > Torznab > Custom`:

![Sonarr Add Indexer](../assets/sonarr/add-indexer.png)

Next copy Jackett's `API Key` from in the top right of the Jackett dashboard:

![Jackett API Key](../assets/jackett/api-key.png)

Paste this into the Sonarr/Radarr form, under `API Key`.

Next, click `Copy Torznab Feed` of the indexer (tracker) you added to Jackett. Paste this into the Sonarr/Radarr form too, under `URL`.

Fill in the rest of the form with your preferences, and you're done!

## Recommended other services

Consider these other supported services that are also in the [*Arr stack](https://wiki.servarr.com/) of media automation tools:

- [Radarr](radarr.md)
- [Sonarr](sonarr.md)
- [qBittorrent](qbittorrent.md)
- [Overseerr](overseerr.md)
````

## File: docs/services/jitsi.md
````markdown
<!--
SPDX-FileCopyrightText: 2020 - 2024 Slavi Pantaleev
SPDX-FileCopyrightText: 2020 - 2024 MDAD project contributors
SPDX-FileCopyrightText: 2020 Aaron Raimist
SPDX-FileCopyrightText: 2020 Mickaël Cornière
SPDX-FileCopyrightText: 2020 Chris van Dijk
SPDX-FileCopyrightText: 2020 Dominik Zajac
SPDX-FileCopyrightText: 2022 François Darveau
SPDX-FileCopyrightText: 2022 Warren Bailey
SPDX-FileCopyrightText: 2023 Antonis Christofides
SPDX-FileCopyrightText: 2023 Pierre 'McFly' Marty
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Jitsi

The playbook can install and configure the [Jitsi](https://jitsi.org/) video-conferencing platform for you.

The [Ansible role for Jitsi](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi) is developed and maintained by the MASH project. For details about configuring Jitsi, you can check them via:
- 🌐 [the role's documentation](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md) online
- 📁 `roles/galaxy/jitsi/docs/configuring-jitsi.md` locally, if you have [fetched the Ansible roles](../installing.md)

## Prerequisites

Before proceeding, make sure to check server's requirements recommended by [the official deployment guide](https://jitsi.github.io/handbook/docs/devops-guide/devops-guide-requirements).

You may need to open some ports to your server, if you use another firewall in front of the server. Refer [the role's documentation](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md#prerequisites) to check which ones to be configured.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

## Adjusting the playbook configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# jitsi                                                                #
#                                                                      #
########################################################################

jitsi_enabled: true

jitsi_hostname: mash.example.com
jitsi_path_prefix: /jitsi

########################################################################
#                                                                      #
# /jitsi                                                               #
#                                                                      #
########################################################################
```

**Since Jitsi's performance heavily depends on server resource (bandwidth, RAM, and CPU), it is recommended to review settings and optimize them as necessary before deployment.** You can check [here](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md#example-configurations) for an example set of configurations to set up a Jitsi instance, focusing on performance. If you will host a large conference, you probably might also want to consider to provision additional JVBs ([Jitsi VideoBridge](https://github.com/jitsi/jitsi-videobridge)). See [here](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md#set-up-additional-jvbs-for-more-video-conferences-optional) for details about setting them up with the playbook.

See the role's documentation for details about configuring Jitsi per your preference (such as setting [a custom hostname](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md#set-the-hostname) and [the environment variable for running Jitsi in a LAN](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md#configure-jvb_advertise_ips-for-running-behind-nat-or-on-a-lan-environment-optional)).

### Adjusting the Jitsi URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/jitsi`.

You can remove the `jitsi_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

**Note**: there are minor quirks when hosting under a subpath, such as:

- [When hosting under a subpath, pwa-worker.js is attempted to be loaded from the base domain without a subpath](https://github.com/jitsi/docker-jitsi-meet/issues/1515)
- [When hosting under a subpath, ending the meeting redirects to the base domain without subpath](https://github.com/jitsi/docker-jitsi-meet/issues/1514)

### Enable authentication and guests mode (optional)

By default the Jitsi Meet instance **does not require for anyone to log in, and is open to use without an account**.

If you would like to control who is allowed to start meetings on your instance, you'd need to enable Jitsi's authentication and optionally guests mode.

See [this section](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md#configure-jitsi-authentication-and-guests-mode-optional) on the role's documentation for details about how to configure the authentication and guests mode.

## Usage

After installation, open the specified URL such as `https://mash.example.com/jitsi`, and you can start a videoconference. Note that you'll need to log in to your Jitsi's account if you have configured authentication with `internal` auth.

Check [the official user guide](https://jitsi.github.io/handbook/docs/category/user-guide) for details about how to use Jitsi Meet.

## Troubleshooting

See [this section](https://github.com/mother-of-all-self-hosting/ansible-role-jitsi/blob/main/docs/configuring-jitsi.md#troubleshooting) on the role's documentation for details.
````

## File: docs/services/keycloak.md
````markdown
# Keycloak

[Keycloak](https://www.keycloak.org/) is an open source identity and access management solution.

> [!WARNING]
> This service is a new addition to the playbook. It may not fully work or be configured in a suboptimal manner.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# keycloak                                                             #
#                                                                      #
########################################################################

keycloak_enabled: true

keycloak_hostname: mash.example.com
keycloak_path_prefix: /keycloak

keycloak_environment_variable_keycloak_admin: your_username_here
# Generating a strong password (e.g. `pwgen -s 64 1`) is recommended
keycloak_environment_variable_keycloak_admin_password: ''

########################################################################
#                                                                      #
# /keycloak                                                            #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/keycloak`.

You can remove the `keycloak_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

### Authentication

On first start, the admin user account will be created as defined with the `keycloak_environment_variable_keycloak_admin` and `keycloak_environment_variable_keycloak_admin_password` variables.

On each start after that, Keycloak will attempt to create the user again and report a non-fatal error (Keycloak will continue running).

Subsequent changes to the password will not affect an existing user's password.


## Usage

After installation, you can go to the Keycloak URL, as defined in `keycloak_hostname` and `keycloak_path_prefix` and log in as described in [Authentication](#authentication).

Follow the [Keycloak documentation](https://www.keycloak.org/documentation) or other guides for learning how to use Keycloak.


## Related services

- [OAuth2-Proxy](oauth2-proxy.md) — A reverse proxy and static file server that provides authentication using OpenID Connect Providers (Google, GitHub, [Authentik](authentik.md), [Keycloak](keycloak.md), and others) to SSO-protect services which do not support SSO natively
````

## File: docs/services/keydb.md
````markdown
# KeyDB

[KeyDB](https://docs.keydb.dev/) is an open source, in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker.

⚠️ We used to advocate for using [Redis](redis.md), but since [Redis is now "source available"](https://redis.com/blog/redis-adopts-dual-source-available-licensing/) we started recommending that you use KeyDB instead. KeyDB is compatible with Redis, so switching should be straightforward. You can learn more about the switch from Redis to KeyDB in [this changelog entry](https://github.com/spantaleev/matrix-docker-ansible-deploy/blob/50813c600db1c47b1f3e76707b81fe05d6c46ef5/CHANGELOG.md#backward-compatibility-break-the-playbook-now-defaults-to-keydb-instead-of-redis) for [matrix-docker-ansible-deploy](https://github.com/spantaleev/matrix-docker-ansible-deploy). Since 2024-11-23, we recommend [Valkey](valkey.md) instead of [KeyDB](./keydb.md).

Some of the services installed by this playbook require a KeyDB data store.

> [!WARNING]
> Because KeyDB is not as flexible as [Postgres](postgres.md) when it comes to authentication and data separation, it's **recommended that you run separate KeyDB instances** (one for each service). KeyDB supports multiple database and a [SELECT](https://docs.keydb.dev/docs/commands/#select) command for switching between them. However, **reusing the same KeyDB instance is not good enough** because:

- if all services use the same KeyDB instance and database (id = 0), services may conflict with one another
- the number of databases is limited to [16 by default](https://github.com/Snapchat/KeyDB/blob/0731a0509a82af5114da1b5aa6cf8ba84c06e134/keydb.conf#L342-L345), which may or may not be enough. With configuration changes, this is solveable.
- some services do not support switching the KeyDB database and always insist on using the default one (id = 0)
- KeyDB [does not support different authentication credentials for its different databases](https://stackoverflow.com/a/37262596), so each service can potentially read and modify other services' data

If you're only hosting a single service (like [PeerTube](peertube.md) or [NetBox](netbox.md)) on your server, you can get away with running a single instance. If you're hosting multiple services, you should prepare separate instances for each service.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process to **host a single instance of the KeyDB service**:

```yaml
########################################################################
#                                                                      #
# keydb                                                                #
#                                                                      #
########################################################################

keydb_enabled: true

########################################################################
#                                                                      #
# /keydb                                                               #
#                                                                      #
########################################################################
```

To **host multiple instances of the KeyDB service**, follow the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation or the **KeyDB** section (if available) of the service you're installing.
````

## File: docs/services/labelstudio.md
````markdown
# LabelStudio

[LabelStudio](https://labelstud.io/) is an open source data labeling tool that supports multiple projects.

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# LabelStudio                                                          #
#                                                                      #
########################################################################

labelstudio_enabled: true
labelstudio_hostname: labelstudio.example.com

########################################################################
#                                                                      #
# /LabelStudio                                                         #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `labelstudio.example.com`.

## Usage

After installation, you should be able to access your new LabelStudio instance at the configured URL (see above).

Going there, you can register new accounts, log in with them and start working.

Keep in mind that every user will see every project.
It may be more secure to disable user registration and use an admin use (created during setup) to send out sign-up emails to additional users later on.

This admin user can be enabled by using the following settings:

```yml
labelstudio_environment_variables_disable_signup_without_link: true
labelstudio_environment_variables_username: "admin-username"
labelstudio_environment_variables_password: "admin-user-password"
```


## Recommended other services

It is possible to attach a pre-labeling backend to LabelStudio.
One such example project can be found in [this repository](https://github.com/seblful/label-studio-yolo-backend).
````

## File: docs/services/lago.md
````markdown
# Lago

[Lago](https://www.getlago.com/) is an open-source metering and usage-based billing solution.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Valkey](valkey.md) data-store, installation details [below](#valkey)
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# lago                                                                 #
#                                                                      #
########################################################################

lago_enabled: true

lago_hostname: lago.example.com

# Generate this using `openssl genrsa 2048 | base64 --wrap=0`
lago_api_environment_variable_lago_rsa_private_key: ''

# WARNING: remove this after you create your user account,
# unless you'd like to run a server with public registration enabled.
lago_front_environment_variable_lago_disable_signup: false

# Valkey configuration, as described below

########################################################################
#                                                                      #
# /lago                                                                #
#                                                                      #
########################################################################
```


### URL

In the example configuration above, we configure the service to be hosted at `https://lago.example.com`.

Hosting Lago under a subpath (by configuring the `lago_path_prefix` variable) does not seem to be possible right now, due to Lago limitations.

Our setup hosts the Lago frontend at the root path (`/`) and the Lago API at the `/api` prefix.
This seems to work well, except for [PDF invoices failing due to a Lago bug](https://github.com/getlago/lago/issues/221).


### Authentication

Public registration can be enabled/disabled using the `lago_front_environment_variable_lago_disable_signup` variable.

We recommend installing with public registration enabled at first, creating your first user account, and then disabling public registration (unless you need it).

It should be noted that disabling public signup with this variable merely disables the Sign-Up page in the web interface, but [does not actually disable signups due to a Lago bug](https://github.com/getlago/lago/issues/220).


### Valkey

As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require KeyDB on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to Lago](#creating-a-valkey-instance-dedicated-to-lago).

If you're only running Lago on this server and don't need to use KeyDB for anything else, you can [use a single Valkey instance](#using-the-shared-valkey-instance-for-lago).

#### Using the shared Valkey instance for Lago

To install a single (non-dedicated) Valkey instance (`mash-valkey`) and hook Lago to it, add the following **additional** configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# lago                                                                 #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point Lago to the shared Valkey instance
lago_redis_hostname: "{{ valkey_identifier }}"

# Make sure the Lago service (mash-lago.service) starts after the shared KeyDB service (mash-valkey.service)
lago_api_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the Lago container is connected to the container network of the shared KeyDB service (mash-valkey)
lago_api_container_additional_networks_custom:
  - "{{ valkey_identifier }}"

########################################################################
#                                                                      #
# /lago                                                                #
#                                                                      #
########################################################################
```

This will create a `mash-valkey` Valkey instance on this host.

This is only recommended if you won't be installing other services which require KeyDB. Alternatively, go for [Creating a Valkey instance dedicated to Lago](#creating-a-valkey-instance-dedicated-to-lago).

#### Creating a Valkey instance dedicated to Lago

The following instructions are based on the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `lago.example.com` is your main one, create `lago.example.com-deps`).

Then, create a new `vars.yml` file for the

`inventory/host_vars/lago.example.com-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-lago-'
mash_playbook_service_base_directory_name_prefix: 'lago-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-lago-valkey` instance on this host with its data in `/mash/lago-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/lago.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# lago                                                             #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point Lago to its dedicated Valkey instance
lago_redis_hostname: mash-lago-valkey

# Make sure the Lago service (mash-lago.service) starts after its dedicated KeyDB service (mash-lago-valkey.service)
lago_api_systemd_required_services_list_custom:
  - "mash-lago-valkey.service"

# Make sure the Lago container is connected to the container network of its dedicated KeyDB service (mash-lago-valkey)
lago_api_container_additional_networks_custom:
  - "mash-lago-valkey"

########################################################################
#                                                                      #
# /lago                                                            #
#                                                                      #
########################################################################
```


## Usage

After installation, you can go to the Lago URL, as defined in `lago_hostname`.

As mentioned in [Authentication](#authentication) above, you can create the first user from the web interface.

If you'd like to prevent other users from registering, consider disabling public registration by removing the `lago_front_environment_variable_lago_disable_signup` references from your configuration and re-running the playbook (`just install-service lago`).
````

## File: docs/services/languagetool.md
````markdown
# LanguageTool

[LanguageTool](https://languagetool.org/) is an open source online grammar, style and spell checker. Installing it is powered by the [etke.cc/roles/languagetool](https://gitlab.com/etke.cc/roles/languagetool) Ansible role and [Erikvl87/docker-languagetool](https://github.com/Erikvl87/docker-languagetool) docker image.

## Dependencies

This service requires the following other services:

- [Traefik](traefik.md) — a reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# languagetool                                                         #
#                                                                      #
########################################################################

languagetool_enabled: true

languagetool_hostname: mash.example.com
languagetool_path_prefix: /languagetool

########################################################################
#                                                                      #
# /languagetool                                                        #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/languagetool`.

You can remove the `languagetool_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

### Enabling n-gram data

LanguageTool can make use of large n-gram data sets to detect errors with words that are often confused, like *their* and *there*.
Learn more in [Finding errors using n-gram data](https://dev.languagetool.org/finding-errors-using-n-gram-data).

The **n-gram data set is huge and thus not part of the LanguageTool installation**. To make use of it with your own LanguageTool server, you may enable n-gram data and choose which languages to download n-gram data for.

For a list of languages for which the Ansible role supports downloading n-gram data, consult the `languagetool_ngrams_langs` variable in [the `default/main.yml` file](https://gitlab.com/etke.cc/roles/languagetool/-/blob/main/defaults/main.yml). Additional languages may be available. If the role doesn't have a download URL for them, consider redefining `languagetool_ngrams_langs` yourself with your own language-code to download URL mapping or submit a PR to the [languagetool Ansible role](https://gitlab.com/etke.cc/roles/languagetool).

```yaml
languagetool_ngrams_enabled: true

# See `languagetool_ngrams_langs` for a list of language-codes
# that the Ansible role supports.
languagetool_ngrams_langs_enabled: ['fr', 'en']
```

## Usage

After [installing](../installing.md), you can test your instance by making requests to [LanguageTool's HTTP API](https://dev.languagetool.org/public-http-api).

An example HTTP request can be made with [curl](https://curl.se/): `curl --data "language=en-US&text=a simple test" https://mash.example.com/languagetool/v2/check`

See the list of [software that supports LanguageTool as an add-on](https://dev.languagetool.org/software-that-supports-languagetool-as-a-plug-in-or-add-on) and set `https://mash.example.com/languagetool/v2` as the LanguageTool server (assuming you've installed at the `/languagetool` path prefix).
````

## File: docs/services/linkding.md
````markdown
# Linkding

[Linkding](https://github.com/sissbruecker/linkding) bookmark manager that is designed be to be minimal and fast.

## Dependencies

This service requires the following other services:

-  a [Postgres](postgres.md) database, but will fallback to [SQLite](https://www.sqlite.org/) if unavailable
-  a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# linkding                                                             #
#                                                                      #
########################################################################

linkding_enabled: true

linkding_hostname: mash.example.com
linkding_path_prefix: /linkding

linkding_superuser_username: ''
linkding_superuser_password: ''

########################################################################
#                                                                      #
# /linkding                                                            #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/linkding`.

You can remove the `linkding_path_prefix` variable definition, so that the service is served at `https://mash.example.com/`.

### Superuser

Please note the use of [`linkding_superuser_username`](https://github.com/sissbruecker/linkding/blob/master/docs/Options.md#ld_superuser_name) and [`linkding_superuser_password`](https://github.com/sissbruecker/linkding/blob/master/docs/Options.md#ld_superuser_password) variables. These are not mandatory and are meant to be set the first time you run this role.

## Usage

After installation, you can log in with your superuser login (`linkding_superuser_username`) and password (`linkding_superuser_password`).
````

## File: docs/services/mariadb.md
````markdown
# MariaDB

[MariaDB](https://mariadb.org/) is a powerful, open source object-relational database system.

Some of the services installed by this playbook require a MariaDB database.

Enabling the MariaDB database service will automatically wire all other services which require such a database to use it.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# mariadb                                                              #
#                                                                      #
########################################################################

mariadb_enabled: true

# Put a strong password below, generated with `pwgen -s 64 1` or in another way
mariadb_root_password: ''

########################################################################
#                                                                      #
# /mariadb                                                             #
#                                                                      #
########################################################################
```


## Getting a database terminal

You can use the `/mash/mariadb/bin/cli` tool to get interactive terminal access to the MariaDB server.

To see the available databases, run `SHOW DATABASES`.

To change to another existing database (for example `miniflux`), run `USE miniflux`.

You can then proceed to write queries. Example: `SELECT COUNT(*) FROM users;`

**Be careful**. Modifying the database directly (especially as services are running) is dangerous and may lead to irreversible database corruption.
When in doubt, consider [making a backup](#backing-up-mariadb).


## Upgrading MariaDB

The major MariaDB version you start with (e.g. `10.10` or `10.11`) will be kept until you manually upgrade it. The playbook will stick to this major version and only do minor version upgrades (e.g. `10.10.1` -> `10.10.3`).

This is because newer MariaDB versions cannot start with data generated by older MariaDB versions.

Upgrades must be performed manually.

This playbook can upgrade your existing MariaDB setup with the following command:

```sh
just run-tags upgrade-mariadb
```

**The old MariaDB data directory is backed up** automatically, by renaming it to `/mash/mariadb/data-auto-upgrade-backup`.
To rename to a different path, pass some extra flags to the command above, like this: `--extra-vars="mariadb_auto_upgrade_backup_data_path=/another/disk/mash-postgres-before-upgrade"`

The auto-upgrade-backup directory stays around forever, until you **manually decide to delete it**.

As part of the upgrade, the database is dumped to `/tmp`, an upgraded and empty MariaDB server is started, and then the dump is restored into the new server.
To use a different directory for the dump, pass some extra flags to the command above, like this: `--extra-vars="mariadb_dump_dir=/directory/to/dump/here"`

To save disk space in `/tmp`, the dump file is gzipped on the fly at the expense of CPU usage.
If you have plenty of space in `/tmp` and would rather avoid gzipping, you can explicitly pass a dump filename which doesn't end in `.gz`.
Example: `--extra-vars="mariadb_dump_name=mash-postgres-dump.sql"`

**All databases, users, etc. on the MariaDB server are migrated**.


## Backing up MariaDB

A `/mash/mariadb/bin/dump-all` script will be installed, which can dump the database to a path of your choosing.
````

## File: docs/services/matterbridge.md
````markdown
# Matterbridge

[Matterbridge](https://github.com/42wim/matterbridge)

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

The configuration itself is documented [here](https://github.com/42wim/matterbridge/wiki/How-to-create-your-config)

```yaml
########################################################################
#                                                                      #
# matterbridge                                                         #
#                                                                      #
########################################################################

matterbridge_enabled: true
matterbridge_configuration_toml:
  accounts:
    - protocol: matrix
      identifier: someidentifier
      configuration:
        Server: "https://matrix.example.com"
        Login: "{{ matterbridge_matrix_user }}"
        Password: "{{ matterbridge_matrix_password }}"
        RemoteNickFormat: "{NICK}: "
        NoHomeServerSuffix: "false"

  gateways:
    - name: "A Gateway"
      enable: "true"
      channels:

        - type: inout
          account: matrix.someidentifier
          channel: "!roomA:matrix.example.com"

        - type: inout
          account: matrix.freifunk
          channel: "!roomB:matrix.example.com"

########################################################################
#                                                                      #
# /matterbridge                                                        #
#                                                                      #
########################################################################
```
````

## File: docs/services/miniflux.md
````markdown
# Miniflux

[Miniflux](https://miniflux.app/) is a minimalist and opinionated feed reader.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# miniflux                                                             #
#                                                                      #
########################################################################

miniflux_enabled: true

miniflux_hostname: mash.example.com
miniflux_path_prefix: /miniflux

miniflux_admin_login: your-username-here
miniflux_admin_password: a-strong-password-here

########################################################################
#                                                                      #
# /miniflux                                                            #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/miniflux`.

You can remove the `miniflux_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


## Usage

After installation, you can log in with your administrator login (`miniflux_admin_login`) and password (`miniflux_admin_password`).

You can create additional users (admin-privileged or not) after logging in.
````

## File: docs/services/mobilizon.md
````markdown
# Mobilizon

[Mobilizon](https://joinmobilizon.org/en/) is a ActivityPub/Fediverse server to create and share events here powered by the [mother-of-all-self-hosting/ansible-role-mobilizon](https://github.com/mother-of-all-self-hosting/ansible-role-mobilizon) Ansible role.

## Depedencies


This service requires the following other services:

- a [Postgis](postgis.md) database (postgres based database that supports geospatial data)
- a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file. Also you need to enable postgis which will serve as database for mobilizon.
After that you can re-run the [installation](../installing.md) process.

```yaml
########################################################################
#                                                                      #
# mobilizon                                                            #
#                                                                      #
########################################################################

mobilizon_enabled: true


# Hostname that this server will be reachable at.
# DO NOT change this after your server has already run once, or you will break things!
mobilizon_hostname: 'events.example.org'

# to open registrations uncomment the following line
# mobilizon_registrations_open: true

########################################################################
#                                                                      #
# /mobilizon                                                           #
#                                                                      #
########################################################################
```

After installation, you can use `just run-tags mobilizon-add-admin --extra-vars=password=<password> --extra-vars=email=<email>`
to create your an admin account.

### Usage

After [installing](../installing.md), you can visit at the URL specified in `mobilizon_hostname` and should see your instance.

Refer to the [great official documentation](https://docs.joinmobilizon.org/use/) for more information on Mobilizon.
````

## File: docs/services/mongodb.md
````markdown
# MongoDB

[MongoDB](https://www.mongodb.com/) is a source-available cross-platform document-oriented (NoSQL) database program.

Some of the services installed by this playbook require a MongoDB database.

Enabling the MongoDB database service will automatically wire all other services which require such a database to use it.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# mongodb                                                              #
#                                                                      #
########################################################################

mongodb_enabled: true

# Put a strong password below, generated with `pwgen -s 64 1` or in another way
mongodb_root_password: ''

########################################################################
#                                                                      #
# /mongodb                                                             #
#                                                                      #
########################################################################
```

## Importing

### Importing an existing MongoDB database from another installation (optional)

Follow this section if you'd like to import your database from a previous installation.

### Prerequisites

The playbook supports importing **gzipped** MongoDB database dumps (created with `mongodump --gzip -o /directory`).

Before doing the actual import, **you need to upload your MongoDB dump file to the server** (any path is okay).


### Importing a dump

To import, run this command (make sure to replace `SERVER_PATH_TO_MONGODB_DUMP_DIRECTORY` with a file path on your server):

```sh
just run-tags import-mongodb \
--extra-vars=mongodb_server_path_dump=SERVER_PATH_TO_MONGODB_DUMP_DIRECTORY
```

**Note** that `SERVER_PATH_TO_MONGODB_DUMP_DIRECTORY` must be a path to a **gzipped** MongoDB dump directory on the server (not on your local machine!)


## Maintenance

This section shows you how to perform various maintenance tasks related to the MongoDB database server used by various components of this playbook.

Table of contents:

- [Getting a database terminal](#getting-a-database-terminal), for when you wish to execute queries

- [Backing up MongoDB](#backing-up-mongodb), for when you wish to make a backup

### Getting a database terminal

You can use the `/mash/mongodb/bin/cli` tool to get interactive terminal access using the MongoDB Shell [mongosh](https://www.mongodb.com/docs/mongodb-shell/).

By default, this tool puts you in the `admin` database, which contains nothing.

To see the available databases, run `show dbs`.

To change to another database (for example `infisical`), run `use infisical`.

To see the available tables in the current database, run `show tables`.

You can then proceed to write queries. Example: `db.users.find()`

**Be careful**. Modifying the database directly (especially as services are running) is dangerous and may lead to irreversible database corruption.
When in doubt, consider [making a backup](#backing-up-mongodb).


### Backing up MongoDB

To make a one-off back up of the current MongoDB database, make sure it's running and then execute a command like this on the server:

```bash
# Prepare the backup directory
mkdir /path-to-some-directory
chown mash:mash /path-to-some-directory

# Back up
/mash/mongodb/bin/dump-all /path-to-some-directory
```

Restoring a backup made this way can be done by [importing it](#importing).
````

## File: docs/services/mosquitto.md
````markdown
# Mosquitto

[Mosquitto](https://mosquitto.org/) is an open source [MQTT](https://en.wikipedia.org/wiki/MQTT) broker.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
mosquitto_enabled: true

# If you need to change the MQTT port you can uncomment and adjust
# mosquitto_container_mqtt_host_bind_port: "1884"
```

## Usage

After installation, you can use `just run-tags mosquitto-add-user --extra-vars=username=<username> --extra-vars=password=<password>` to create a user. For the setting to take effect, you must restart the container. To do that you can use `just start-group mosquitto`.

You can then start to send and subscribe to MQTT topics. Use port `1883` and the server's IP or any domain you configured to point to this server.

## Alternatives

* [rumqttd](rumqttd.md) is another MQTT broker
````

## File: docs/services/mrs.md
````markdown
# Matrix Rooms Search API

[Matrix Rooms Search](https://gitlab.com/etke.cc/mrs) is a fully-featured, standalone, [Matrix](https://matrix.org/) rooms search service.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# mrs                                                                  #
#                                                                      #
########################################################################

mrs_enabled: true
mrs_hostname: mrs.example.com

mrs_admin_login: admin
mrs_admin_password: changeme
mrs_admin_ips:
  - 123.123.123.123

mrs_servers:
  - matrix.org

########################################################################
#                                                                      #
# /mrs                                                                 #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mrs.example.com`.


## Usage

After installation, call the `https://mrs.example.com/-/full` endpoint using admin credentials (see the `mrs_admin_*` variables) to discover and parse content.

To see the list of supported public and private APIs, see the [API documentation](https://gitlab.com/etke.cc/mrs/api/-/blob/main/openapi.yml).
````

## File: docs/services/n8n.md
````markdown
# n8n

[n8n](https://n8n.io/) is a workflow automation tool for technical people.

## Dependencies

This service requires the following other services:

-   a [Postgres](postgres.md) database
-   a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# n8n                                                                  #
#                                                                      #
########################################################################

n8n_enabled: true

n8n_hostname: mash.example.com
n8n_path_prefix: /n8n

########################################################################
#                                                                      #
# /n8n                                                                 #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/n8n`.

You can remove the `n8n_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

## Usage

You can create additional users (admin-privileged or not) after logging in.
````

## File: docs/services/navidrome.md
````markdown
# Navidrome

[Navidrome](https://www.navidrome.org/) is a [Subsonic-API](http://www.subsonic.org/pages/api.jsp) compatible music server.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# navidrome                                                            #
#                                                                      #
########################################################################

navidrome_enabled: true

navidrome_hostname: mash.example.com
navidrome_path_prefix: /navidrome

# By default, Navidrome will look at the /music directory for music files,
# controlled by the `navidrome_environment_variable_nd_musicfolder` variable.
#
# You'd need to mount some music directory into the Navidrome container, like shown below.
# The "Syncthing integration" section below may be relevant.
# navidrome_container_additional_volumes:
#   - type: bind
#     src: /on-host/path/to/music
#     dst: /music
#     options: readonly

########################################################################
#                                                                      #
# /navidrome                                                           #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/navidrome`.

You can remove the `navidrome_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

### Authentication

On first use (see [Usage](#usage) below), you'll be asked to create the first administrator user.

You can create additional users from the web UI after that.

### Syncthing integration

If you've got a [Syncthing](syncthing.md) service running, you can use it to synchronize your music directory onto the server and then mount it as read-only into the Navidrome container.

We recommend that you make use of the [aux](auxiliary.md) role to create some shared directory like this:

```yaml
########################################################################
#                                                                      #
# aux                                                                  #
#                                                                      #
########################################################################

aux_directory_definitions:
  - dest: "{{ mash_playbook_base_path }}/storage"
  - dest: "{{ mash_playbook_base_path }}/storage/music"

########################################################################
#                                                                      #
# /aux                                                                 #
#                                                                      #
########################################################################
```

You can then mount this `{{ mash_playbook_base_path }}/storage/music` directory into the Syncthing container and synchronize it with some other computer:

```yaml
########################################################################
#                                                                      #
# syncthing                                                            #
#                                                                      #
########################################################################

# Other Syncthing configuration..

syncthing_container_additional_volumes:
  - type: bind
    src: "{{ mash_playbook_base_path }}/storage/music"
    dst: /music

########################################################################
#                                                                      #
# /syncthing                                                           #
#                                                                      #
########################################################################
```

Finally, mount the `{{ mash_playbook_base_path }}/storage/music` directory into the Navidrome container as read-only:

```yaml
########################################################################
#                                                                      #
# navidrome                                                            #
#                                                                      #
########################################################################

# Other Navidrome configuration..

navidrome_container_additional_volumes:
  - type: bind
    src: "{{ mash_playbook_base_path }}/storage/music"
    dst: /music
    options: readonly

########################################################################
#                                                                      #
# /navidrome                                                           #
#                                                                      #
########################################################################
```

## Usage

After installation, you can go to the Navidrome URL, as defined in `navidrome_hostname` and `navidrome_path_prefix`.

As mentioned in [Authentication](#authentication) above, you'll be asked to create the first administrator user the first time you open the web UI.

You can also connect various Subsonic-API-compatible [apps](https://www.navidrome.org/docs/overview/#apps) (desktop, web, mobile) to your Navidrome instance.


## Recommended other services

- [Syncthing](syncthing.md) — a continuous file synchronization program which synchronizes files between two or more computers in real time. See [Syncthing integration](#syncthing-integration)
````

## File: docs/services/neko.md
````markdown
# n.eko

[n.eko](https://neko.m1k1o.net/) is a self-hosted virtual browser, that this playbook can install, powered by the [mother-of-all-self-hosting/ansible-role-neko](https://github.com/mother-of-all-self-hosting/ansible-role-neko) Ansible role.

> [!WARNING]
> The neko service will run in a container with root privileges, no dropped capabilities and will be able to write inside the container. This seems to be a neccessary deviation from the usual security standards in this playbook.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# neko                                                                 #
#                                                                      #
########################################################################

neko_enabled: true
neko_hostname: 'neko.example.org'
neko_password: 'SECURE_PASSWORD'
neko_password_admin: 'SUPER_SECURE_PASSWORD'

########################################################################
#                                                                      #
# /neko                                                                #
#                                                                      #
########################################################################
```

## Advanced configuration

There are different flavours of neko and while `firefox` is the default, you can try others by setting

```yaml
neko_version: "kde"
```

All available tags can be found on [Dockerhub](https://hub.docker.com/r/m1k1o/neko/tags)
````

## File: docs/services/netbox.md
````markdown
# NetBox

[NetBox](https://docs.netbox.dev/en/stable/) is an open-source web application that provides [IP address management (IPAM)](https://en.wikipedia.org/wiki/IP_address_management) and [data center infrastructure management (DCIM)](https://en.wikipedia.org/wiki/Data_center_management#Data_center_infrastructure_management) functionality.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Valkey](valkey.md) data-store, installation details [below](#valkey)
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# netbox                                                               #
#                                                                      #
########################################################################

netbox_enabled: true

netbox_hostname: mash.example.com
netbox_path_prefix: /netbox

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
netbox_environment_variable_secret_key: ''

# The following superuser will be created upon launch.
netbox_environment_variable_superuser_name: your_username_here
netbox_environment_variable_superuser_email: your.email@example.com
# Put a strong secret below, generated with `pwgen -s 64 1` or in another way.
# Changing the password subsequently will not affect the user's password.
netbox_environment_variable_superuser_password: ''

# Valkey configuration, as described below

########################################################################
#                                                                      #
# /netbox                                                              #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/netbox`.

You can remove the `netbox_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


### Authentication

If `netbox_environment_variable_superuser_*` variables are specified, NetBox will try to create the user (if missing).

[Single-Sign-On](#single-sign-on-sso-integration) is also supported.

### Valkey

As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require KeyDB on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to NetBox](#creating-a-valkey-instance-dedicated-to-netbox).

If you're only running NetBox on this server and don't need to use KeyDB for anything else, you can [use a single Valkey instance](#using-the-shared-valkey-instance-for-netbox).

#### Using the shared Valkey instance for NetBox

To install a single (non-dedicated) Valkey instance (`mash-valkey`) and hook NetBox to it, add the following **additional** configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# netbox                                                               #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point NetBox to the shared Valkey instance
netbox_environment_variable_redis_host: "{{ valkey_identifier }}"
netbox_environment_variable_redis_cache_host: "{{ valkey_identifier }}"

# Make sure the NetBox service (mash-netbox.service) starts after the shared KeyDB service (mash-valkey.service)
netbox_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the NetBox container is connected to the container network of the shared KeyDB service (mash-valkey)
netbox_container_additional_networks_custom:
  - "{{ valkey_identifier }}"

########################################################################
#                                                                      #
# /netbox                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-valkey` Valkey instance on this host.

This is only recommended if you won't be installing other services which require KeyDB. Alternatively, go for [Creating a Valkey instance dedicated to NetBox](#creating-a-valkey-instance-dedicated-to-netbox).


#### Creating a Valkey instance dedicated to NetBox

The following instructions are based on the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `netbox.example.com` is your main one, create `netbox.example.com-deps`).

Then, create a new `vars.yml` file for the

`inventory/host_vars/netbox.example.com-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-netbox-'
mash_playbook_service_base_directory_name_prefix: 'netbox-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-netbox-valkey` instance on this host with its data in `/mash/netbox-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/netbox.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# netbox                                                               #
#                                                                      #
########################################################################

# Base configuration as shown above


# Point NetBox to its dedicated Valkey instance
netbox_environment_variable_redis_host: mash-netbox-valkey
netbox_environment_variable_redis_cache_host: mash-netbox-valkey

# Make sure the NetBox service (mash-netbox.service) starts after its dedicated KeyDB service (mash-netbox-valkey.service)
netbox_systemd_required_services_list_custom:
  - "mash-netbox-valkey.service"

# Make sure the NetBox container is connected to the container network of its dedicated KeyDB service (mash-netbox-valkey)
netbox_container_additional_networks_custom:
  - "mash-netbox-valkey"

########################################################################
#                                                                      #
# /netbox                                                              #
#                                                                      #
########################################################################
```

### Single-Sign-On (SSO) integration

NetBox supports different [Remote Authentication](https://docs.netbox.dev/en/stable/configuration/remote-authentication/) backends, including those provided by the [Python Social Auth](https://python-social-auth.readthedocs.io/) library. This library is included in the NetBox container image by default, so you can invoke any [backend](https://github.com/python-social-auth/social-core/tree/master/social_core/backends) provided by it.

Each module's Python file contains detailed information about how to configure it. It should be noted that module-specific configuration is passed as Python configuration (via `netbox_configuration_extra_python`), and **not as environment variables**.

We have detailed information about integrating with [Keycloak](keycloak.md) below.
You can use the configuration in the [Keycloak section](#keycloak) as a template for configuring other backends.

#### Keycloak

To integrate with [Keycloak](keycloak.md) use the following **additional** configuration:

```yaml
netbox_environment_variables_additional_variables: |
  REMOTE_AUTH_ENABLED=True
  REMOTE_AUTH_BACKEND=social_core.backends.keycloak.KeycloakOAuth2

  # Space-separated names of groups that new users will be assigned to.
  # These groups must be created manually (from the Admin panel's Groups section) before use.
  REMOTE_AUTH_DEFAULT_GROUPS=

netbox_configuration_extra_python: |
  # These need to match your Client app information in Keycloak. See below
  SOCIAL_AUTH_KEYCLOAK_KEY = ''
  SOCIAL_AUTH_KEYCLOAK_SECRET = ''

  # The value for this is retrieved from Keycloak -> Realm Settings -> Keys tab -> Public key button for RS256
  SOCIAL_AUTH_KEYCLOAK_PUBLIC_KEY = ''

  # The value for these are retrieved from Keycloak -> Realm Settings -> General tab -> OpenID Endpoint Configuration button
  SOCIAL_AUTH_KEYCLOAK_AUTHORIZATION_URL = 'https://KEYCLOAK_URL/realms/REALM_IDENTIFIER/protocol/openid-connect/auth'
  SOCIAL_AUTH_KEYCLOAK_ACCESS_TOKEN_URL = 'https://KEYCLOAK_URL/realms/REALM_IDENTIFIER/protocol/openid-connect/token'

# If Keycloak is running on the same server, uncomment the lines below
# and replace HOSTNAME with the hostname of the Keycloak server (e.g. mash.example.com or keycloak.example.com).
# netbox_container_extra_arguments:
#  - --add-host=HOSTNAME:{{ ansible_host }}
```

The Client app needs to be created and configured in a special way on the Keycloak side by:

- activating **Client authentication**
- **Valid redirect URIs**: `https://NETBOX_URL/oauth/complete/keycloak/`
- **Web origins**: `https://NETBOX_URL/`
- in **Advanced**, changing the following settings:
  - **Request object signature algorithm** = `RS256`
  - **User info signed response algorithm** = `RS256`
- in **Client scopes** (for this Client app via the **Client scopes** tab, not for all apps via the left-most menu), configure the `*-dedicated` scope (e.g. `netbox-dedicated` if you named your Client app `netbox`) and in the **Mappers** tab, click **Configure a new mapper** add a new **Audience** mapper with the following settings:
  - **Name** = anything you like (e.g. `netbox-audience`)
  - **Included Client Audience** = the key of this Client app (e.g. `netbox`)
  - **Add to access token** = On

For additional environment variables controlling groups and permissions for new users (like `REMOTE_AUTH_DEFAULT_GROUPS`), see the NetBox documentation for [Remote Authentication](https://docs.netbox.dev/en/stable/configuration/remote-authentication/).

## Installation

If you've decided to install a dedicated Valkey instance for NetBox, make sure to first do [installation](../installing.md) for the supplementary inventory host (e.g. `netbox.example.com-deps`), before running installation for the main one (e.g. `netbox.example.com`).


## Usage

After installation, you can go to the NetBox URL, as defined in `netbox_hostname` and `netbox_path_prefix`.

You can log in with the **username** (**not** email) and password specified in the `netbox_environment_variable_superuser*` variables.
````

## File: docs/services/nextcloud.md
````markdown
# Nextcloud

[Nextcloud](https://nextcloud.com/) is the most popular self-hosted collaboration solution for tens of millions of users at thousands of organizations across the globe.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server
- (optional) a [Valkey](valkey.md) data-store, installation details [below](#valkey)
- (optional) the [exim-relay](exim-relay.md) mailer


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# nextcloud                                                            #
#                                                                      #
########################################################################

nextcloud_enabled: true

nextcloud_hostname: mash.example.com
nextcloud_path_prefix: /nextcloud

# Valkey configuration, as described below

########################################################################
#                                                                      #
# /nextcloud                                                           #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/nextcloud`.

You can remove the `nextcloud_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

### Valkey

Valkey can **optionally** be enabled to improve Nextcloud performance.
It's dubious whether using using Valkey helps much, so we recommend that you **start without** it, for a simpler deployment.

To learn more, read the [Memory caching](https://docs.nextcloud.com/server/latest/admin_manual/configuration_server/caching_configuration.html) section of the Nextcloud documentation.

As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require Valkey on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to Nextcloud](#creating-a-valkey-instance-dedicated-to-nextcloud).

If you're only running Nextcloud on this server and don't need to use Valkey for anything else, you can [use a single Valkey instance](#using-the-shared-valkey-instance-for-nextcloud).

**Regardless** of the method of installing Valkey, you may need to adjust your Nextcloud configuration file (e.g. `/mash/nextcloud/data/config/config.php`) to **add** this:

```php
  'memcache.distributed' => '\OC\Memcache\Redis',
  'memcache.locking' => '\OC\Memcache\Redis',
  'redis' => [
     'host' => 'VALKEY_HOSTNAME_HERE',
     'port' => 6379,
  ],
```

Where `VALKEY_HOSTNAME_HERE` is to be replaced with:

- `mash-nextcloud-valkey`, when [Creating a Valkey instance dedicated to Nextcloud](#creating-a-valkey-instance-dedicated-to-nextcloud)
- `mash-valkey`, when [using a single Valkey instance](#using-the-shared-valkey-instance-for-nextcloud).


#### Using the shared Valkey instance for Nextcloud

To install a single (non-dedicated) Valkey instance (`mash-valkey`) and hook Nextcloud to it, add the following **additional** configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# nextcloud                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point Nextcloud to the shared Valkey instance
nextcloud_redis_hostname: "{{ valkey_identifier }}"

# Make sure the Nextcloud service (mash-nextcloud.service) starts after the shared KeyDB service (mash-valkey.service)
nextcloud_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the Nextcloud container is connected to the container network of the shared KeyDB service (mash-valkey)
nextcloud_container_additional_networks_custom:
  - "{{ valkey_identifier }}"

########################################################################
#                                                                      #
# /nextcloud                                                           #
#                                                                      #
########################################################################
```
This will create a `mash-valkey` Valkey instance on this host.

This is only recommended if you won't be installing other services which require KeyDB. Alternatively, go for [Creating a Valkey instance dedicated to Nextcloud](#creating-a-valkey-instance-dedicated-to-nextcloud).

#### Creating a Valkey instance dedicated to Nextcloud

The following instructions are based on the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `nextcloud.example.com` is your main one, create `nectcloud.example.com-deps`).

Then, create a new `vars.yml` file for the

`inventory/host_vars/nextcloud.example.com-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-nextcloud-'
mash_playbook_service_base_directory_name_prefix: 'nextcloud-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-nextcloud-valkey` instance on this host with its data in `/mash/nextcloud-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/nextcloud.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# nextcloud                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point Nextcloud to its dedicated Valkey instance
nextcloud_redis_hostname: mash-nextcloud-valkey

# Make sure the Nextcloud service (mash-nextcloud.service) starts after its dedicated KeyDB service (mash-nextcloud-valkey.service)
nextcloud_systemd_required_services_list_custom:
  - "mash-nextcloud-valkey.service"

# Make sure the Nextcloud container is connected to the container network of its dedicated KeyDB service (mash-nextcloud-valkey)
nextcloud_container_additional_networks_custom:
  - "mash-nextcloud-valkey"

########################################################################
#                                                                      #
# /nextcloud                                                           #
#                                                                      #
########################################################################
```

###  Single-Sign-On / Authentik

Nextcloud supports Single-Sign-On (SSO) via LDAP, SAML, and OIDC. To make use of this you'll need a Identity Provider like [authentik](./authentik.md) or [Keycloak](./keycloak.md). The following assumes you use authentik.


**The official documentation of authentik to connect nextcloud via SAML seems broken**

MASH can connect Nextcloud with authentik via OIDC. The setup is quite straightforward, refer to [this blogpost by Jack](https://blog.cubieserver.de/2022/complete-guide-to-nextcloud-oidc-authentication-with-authentik/) for a full explanation.

In short you should:

* Create a new provider in authentik and trim the client secret to <64 characters
* Create an application in authentik using this provider
* Install the app `user_oidc` in Nextcloud
* Fill in the details from authentik in the app settings

**Troubleshooting**

If you encounter problems during login check (error message containes `SHA1 mismatch`) that
* Nextcloud users and authentik users do not have the same name -> if they do check `Use unique user ID` in the OIDC App settings

### Samba

To enable [Samba](https://www.samba.org/) external Windows fileshares using [smbclient](https://www.samba.org/samba/docs/current/man-html/smbclient.1.html), add the following additional configuration to your `vars.yml` file:

```yml
nextcloud_container_image_customizations_samba_enabled: true
```

## Installation

If you've decided to install a dedicated Valkey instance for Nextcloud, make sure to first do [installation](../installing.md) for the supplementary inventory host (e.g. `nextcloud.example.com-deps`), before running installation for the main one (e.g. `nextcloud.example.com`).

## Usage

After [installation](../installing.md), you should follow Nextcloud's setup wizard at the URL you've chosen.

You can choose any username/password for your account.

In **Storage & database**, you should choose PostgreSQL (changing the default **SQLite** choice), with the credentials you see after running `just run-tags print-nextcloud-db-credentials`

Once you've fully installed Nextcloud, you'd better adjust its default configuration (URL paths, trusted reverse-proxies, etc.) by running: `just run-tags adjust-nextcloud-config`


## Recommended other services

### Collabora Online

To integrate the [Collabora Online](collabora-online.md) office suite, first install it by following its dedicated documentation page.

Then add the following **additional** Nextcloud configuration:

```yaml
nextcloud_collabora_app_wopi_url: "{{ collabora_online_url }}"

# By default, various private IPv4 networks are whitelited to connect to the WOPI API (document serving API).
# If your Collabora Online installation does not live on the same server as Nextcloud,
# you may need to adjust the list of networks.
# If necessary, redefined the `nextcloud_collabora_app_wopi_allowlist` environment variable here.
```

There's **no need** to [re-run the playbook](../installing.md) after adjusting your `vars.yml` file.
You should, however run: `just run-tags install-nextcloud-app-collabora`

This will install and configure the [Office](https://apps.nextcloud.com/apps/richdocuments) app for Nextcloud.

You should then be able to click any document (`.doc`, `.odt`, `.pdf`, etc.) in Nextcloud Files and it should automatically open a Collabora Online editor.

You can also create new documents via the "plus" button.

### Preview Generator

It is possible to setup preview generation, using this playbook.

First modify your `vars.yml` file by adding at least the following line (other options are also present, check the corresponding `defaults/main.yml` file):

```yaml
nextcloud_preview_enabled: true
```

then install Nextcloud (or rerun the playbook if already installed).

Next, from the Settings/Application menu in your Nextcloud instance install the preview generator app (https://apps.nextcloud.com/apps/previewgenerator).

After the application is installed run `just run-tags adjust-nextcloud-config` that will start the original preview-generation and when finished, enables the periodic generation of new images.

The original generation may take a long time, but a continuous prompt is presented by ansible as some visual feedback (it is being run as an async task), however it will timeout after approximately 27 hours.

On 60GBs, most of the data being images, it took about 10 minutes to finish.

If it takes more time to run than a day, you may want to start it from the host by calling

```sh
/usr/bin/env docker exec mash-nextcloud-server php /var/www/html/occ preview:generate-all
```

Also, please note: every time Nextcloud version is updated, you should rerun: `just run-tags adjust-nextcloud-config`.

Other supported variables:

`nextcloud_preview_preview_max_x` and `nextcloud_preview_preview_max_y` sets the maximum size of the preview in pixels..
Their default value according to the [documentation](https://docs.nextcloud.com/server/latest/admin_manual/configuration_files/previews_configuration.html) is `null` which is set by the playbook.
Using a numeric value will set the corresponding nextcloud variable and sets the size of the preview images.

`nextcloud_preview_app_jpeg_quality` JPEG quality setting for preview images.
The default follows upstream default with 80.
````

## File: docs/services/notfellchen.md
````markdown
# Notfellchen

[Notfellchen](https://codeberg.org/moanos/notfellchen) is a self-hosted tool to list animals available for adoption to increase their chance of finding a forever-home.


> [!WARNING]
> This service is a custom solution. Feel free to use it but don't expect a solution that works for every use case. Issues with this should be filed in the [project itself](https://codeberg.org/moanos/notfellchen).

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# notfellchen                                                          #
#                                                                      #
########################################################################

notfellchen_enabled: true
notfellchen_hostname: notfellchen.example.com

########################################################################
#                                                                      #
# /notfellchen                                                         #
#                                                                      #
########################################################################
```

### Valkey

As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require KeyDB on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to notfellchen](#creating-a-valkey-instance-dedicated-to-notfellchen).

If you're only running notfellchen on this server and don't need to use KeyDB for anything else, you can [use a single Valkey instance](#using-the-shared-valkey-instance-for-notfellchen).

#### Using the shared Valkey instance for notfellchen

To install a single (non-dedicated) Valkey instance (`mash-valkey`) and hook notfellchen to it, add the following **additional** configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# notfellchen                                                          #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point notfellchen to the shared Valkey instance
notfellchen_config_redis_hostname: "{{ valkey_identifier }}"

# Make sure the notfellchen API service (mash-notfellchen.service) starts after the shared KeyDB service
notfellchen_api_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the notfellchen API service (mash-notfellchen.service) is connected to the container network of the shared KeyDB service
notfellchen_container_additional_networks_custom:
  - "{{ valkey_container_network }}"

########################################################################
#                                                                      #
# /notfellchen                                                           #
#                                                                      #
########################################################################
```

This will create a `mash-valkey` Valkey instance on this host.

This is only recommended if you won't be installing other services which require KeyDB. Alternatively, go for [Creating a Valkey instance dedicated to notfellchen](#creating-a-valkey-instance-dedicated-to-notfellchen).


#### Creating a Valkey instance dedicated to notfellchen

The following instructions are based on the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `notfellchen.example.com` is your main one, create `notfellchen.example.com-deps`).

Then, create a new `vars.yml` file for the

`inventory/host_vars/notfellchen.example.com-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-notfellchen-'
mash_playbook_service_base_directory_name_prefix: 'notfellchen-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-notfellchen-valkey` instance on this host with its data in `/mash/notfellchen-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/notfellchen.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# notfellchen                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point notfellchen to its dedicated Valkey instance
notfellchen_config_redis_hostname: mash-notfellchen-valkey

# Make sure the notfellchen ervice (mash-notfellchen.service) starts after its dedicated KeyDB service
notfellchen_systemd_required_services_list_custom:
  - "mash-notfellchen-valkey.service"

# Make sure the notfellchen service (mash-notfellchen.service) is connected to the container network of its dedicated KeyDB service
notfellchen_api_container_additional_networks_custom:
  - "mash-notfellchen-valkey"

########################################################################
#                                                                      #
# /notfellchen                                                         #
#                                                                      #
########################################################################
```



## Setting up the first user

You need to create a first user (unless you import an existing database).
You can do this conveniently by running

```bash
just run-tags notfellchen-add-superuser --extra-vars=username=USERNAME --extra-vars=password=PASSWORD --extra-vars=email=EMAIL
```

## Usage

After installation, you can go to the URL, as defined in `notfellchen_hostname`. Log in with the user credentials from above.
````

## File: docs/services/ntfy.md
````markdown
<!--
SPDX-FileCopyrightText: 2020 - 2024 MDAD project contributors
SPDX-FileCopyrightText: 2020 - 2024 Slavi Pantaleev
SPDX-FileCopyrightText: 2020 Aaron Raimist
SPDX-FileCopyrightText: 2020 Chris van Dijk
SPDX-FileCopyrightText: 2020 Dominik Zajac
SPDX-FileCopyrightText: 2020 Mickaël Cornière
SPDX-FileCopyrightText: 2022 François Darveau
SPDX-FileCopyrightText: 2022 Julian Foad
SPDX-FileCopyrightText: 2022 Warren Bailey
SPDX-FileCopyrightText: 2023 Antonis Christofides
SPDX-FileCopyrightText: 2023 Felix Stupp
SPDX-FileCopyrightText: 2023 Pierre 'McFly' Marty
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# ntfy

The playbook can install and configure the [ntfy](https://ntfy.sh/) (pronounced "notify") push notifications server for you.

ntfy lets you send push notifications to your phone or desktop via scripts from any computer, using simple HTTP PUT or POST requests. It enables you to send/receive notifications, without relying on servers owned and controlled by third parties.

See the project's [documentation](https://docs.ntfy.sh/) to learn what it does and why it might be useful to you.

The [Ansible role for ntfy](https://github.com/mother-of-all-self-hosting/ansible-role-ntfy) is developed and maintained by the MASH project. For details about configuring ntfy, you can check them via:
- 🌐 [the role's documentation](https://github.com/mother-of-all-self-hosting/ansible-role-ntfy/blob/main/docs/configuring-ntfy.md) online
- 📁 `roles/galaxy/ntfy/docs/configuring-ntfy.md` locally, if you have [fetched the Ansible roles](../installing.md)

**Note**: you need to install [the ntfy Android/iOS app](https://docs.ntfy.sh/subscribe/phone/) on your device in order to receive push notifications from the ntfy server. Notifications can also be sent/received on the ntfy's web app if it is enabled (disabled by default). Refer [this section](#usage) for details about how to use the apps.

### How ntfy works with UnifiedPush

⚠️ [UnifiedPush does not work on iOS.](https://unifiedpush.org/users/faq/#will-unifiedpush-ever-work-on-ios)

ntfy implements [UnifiedPush](https://unifiedpush.org), the standard which makes it possible to send and receive push notifications without using Google's Firebase Cloud Messaging (FCM) service.

Working as a **Push Server**, a ntfy server can forward messages to a **Distributor** running on Android and other devices (see [here](https://unifiedpush.org/users/distributors/#definitions) for the definition of the Push Server and the Distributor).

This role installs and manages a self-hosted ntfy server as the Push Server, which the Distributor (such as the ntfy Android app) on your device listens to.

Your UnifiedPush-compatible applications (such as [Tusky](https://tusky.app/) and [DAVx⁵](https://www.davx5.com/)) listen to the Distributor, and push notitications are "distributed" from it. This means that the UnifiedPush-compatible applications cannot receive push notifications from the Push Server without the Distributor.

As the ntfy Android app functions as the Distributor too, you do not have to install something else on your device.

💡 **Notes**:
- Refer [this official documentation of UnifiedPush](https://unifiedpush.org/users/troubleshooting/#understand-unifiedpush) for a simple explanation about relationship among UnifiedPush-compatible application, Distributor, Push Server, and the application's server.
- [Here](https://unifiedpush.org/users/apps/) is a non-exhaustive list of the end-user applications that use UnifiedPush.
- Unlike push notifications using Google's FCM or Apple's APNs, each end-user can choose the Push Server which one prefer. This means that deploying a ntfy server cannot enforce a UnifiedPush-compatible application (and its users) to use the exact server.

### iOS instant notification

Because iOS heavily restricts background processing, it is impossible to implement instant push notifications without a central server.

To implement instant notification through the self-hosted ntfy server, see [this official documentation](https://docs.ntfy.sh/config/#ios-instant-notifications) for instructions.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server
- (optional) the [exim-relay](exim-relay.md) mailer

## Adjusting the playbook configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# ntfy                                                                 #
#                                                                      #
########################################################################

ntfy_enabled: true

ntfy_hostname: ntfy.example.com

########################################################################
#                                                                      #
# /ntfy                                                                #
#                                                                      #
########################################################################
```

As the most of the necessary settings for the role have been taken care of by the playbook, you can enable ntfy on your server with this minimum configuration.

See the role's documentation for details about configuring ntfy per your preference (such as [setting access control with authentication](https://github.com/mother-of-all-self-hosting/ansible-role-ntfy/blob/main/docs/configuring-ntfy.md#enable-access-control-with-authentication-optional), [allowing attachments](https://github.com/mother-of-all-self-hosting/ansible-role-ntfy/blob/main/docs/configuring-ntfy.md#allow-attachments-optional), [enabling the web app](https://github.com/mother-of-all-self-hosting/ansible-role-ntfy/blob/main/docs/configuring-ntfy.md#enable-web-app-optional) and [e-mail notification](https://github.com/mother-of-all-self-hosting/ansible-role-ntfy/blob/main/docs/configuring-ntfy.md#enable-e-mail-notification-optional), etc.)

## Usage

To receive push notifications from the ntfy server, you need to **install [the ntfy Android/iOS app](https://docs.ntfy.sh/subscribe/phone/)**, **log in to the account on the ntfy app** if you have enabled the access control, and then **subscribe to a topic** where messages will be published. You can also send/receive notifications on the ntfy's web app at `example.com`.

See [this section](https://github.com/mother-of-all-self-hosting/ansible-role-ntfy/blob/main/docs/configuring-ntfy.md#usage) on the role's documentation for details.

If you enable [Uptime Kuma](uptime-kuma.md), the self-hosted monitoring tool (hint: this playbook enables it by default on its example `vars.yml` files), it is possible to set it up to have it send notifications to a topic when the monitored web service is down. You can subscribe to the topic both from the ntfy Android/iOS app and the web app.

### UnifiedPush-compatible application

To receive push notifications on a UnifiedPush-compatible application, it must be able to communicate with the ntfy Android app which works as the Distributor on the same device.

Consult to documentation of applications for instruction about how to enable UnifiedPush support. Note that some applications quietly detect and use the Distributor, so you do not always have to configure the applications.

If you are configuring UnifiedPush on a [Matrix](https://matrix.org) client, you can refer [this section](https://github.com/spantaleev/matrix-docker-ansible-deploy/blob/master/docs/configuring-playbook-ntfy.md#setting-up-a-unifiedpush-compatible-matrix-client) on matrix-docker-ansible-deploy (MDAD) playbook's documentation.

## Troubleshooting

See [this section](https://github.com/mother-of-all-self-hosting/ansible-role-ntfy/blob/main/docs/configuring-ntfy.md#troubleshooting) on the role's documentation for details.
````

## File: docs/services/oauth2-proxy.md
````markdown
# OAuth2-Proxy

[OAuth2-Proxy](https://oauth2-proxy.github.io/oauth2-proxy/) is a reverse proxy and static file server that provides authentication using OpenID Connect Providers (Google, GitHub, [Authentik](authentik.md), [Keycloak](keycloak.md), and others) to SSO-protect services which do not support SSO natively.


## Modes of operation

OAuth2-Proxy can be used in 2 different modes:

1. Capturing incoming traffic for the app (e.g. https://app.example.com/), and then proxying it to the application container if the user is authenticated

2. Letting the application itself capture incoming traffic for itself (on https://app.example.com/) and use Traefik's [ForwardAuth](https://doc.traefik.io/traefik/middlewares/http/forwardauth/) middleware to authenticate the request via OAuth2-Proxy. In this case, OAuth2-Proxy will only handle the `/oauth2/` prefix on the application domain (e.g. https://app.example.com/oauth/).

The 1st one is a bit invasive, as it requires moving all custom reverse-proxying configuration for the handled domain to the OAuth2-Proxy side.

The 2nd one lets you keep the existing application configuration. However, it needs all URLs to go to one service (the application) with the exception of `/oauth2/` (which should go to OAuth2-Proxy). As such, it it requires that both services (the application and OAuth2-Proxy) run on the same machine.

Our [Sample configuration](#sample-configuration) below uses [ForwardAuth](https://doc.traefik.io/traefik/middlewares/http/forwardauth/).

The [OAuth2-Proxy Ansible role](https://github.com/mother-of-all-self-hosting/ansible-role-oauth2-proxy) should be flexible enough to let you reconfigure it for both modes of operation. However, if feasible, we recommend using the 2nd (ForwardAuth) method.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server
- an OIDC provider running anywhere. See [Choosing a provider](#choosing-a-provider).


## Choosing a provider

To use OAuth2-Proxy, you need to choose an [OIDC provider](https://oauth2-proxy.github.io/oauth2-proxy/configuration/providers/).

This can be any of the supported providers. If hosting your own (via this playbook or via other means), the OIDC provider may be hosted anywhere (not necessarily on the same server as OAuth2-Proxy or the service you're SSO-protecting).


## Sample configuration

The configuration is [provider](https://oauth2-proxy.github.io/oauth2-proxy/configuration/providers/)-specific and also depends on the the service you're SSO-protecting, on which server it runs (in relation to OAuth-Proxy), etc.

Below is a **sample** configuration for protecting a static website (in this case powered by the [Hubsite](hubsite.md)) service via [Keycloak](keycloak.md).

For this to work as described here, both OAuth2-Proxy and the protected service (e.g. [Hubsite](hubsite.md)) need to run on the same machine.

Keycloak may run anywhere.

You also need to have prepared Keycloak and a "Client app" for it, according to the [Keycloak OIDC](https://oauth2-proxy.github.io/oauth2-proxy/configuration/providers/keycloak_oidc) documentation of OAuth2-Proxy.


#### OAuth2-Proxy configuration

```yaml
########################################################################
#                                                                      #
# oauth2_proxy                                                         #
#                                                                      #
########################################################################

oauth2_proxy_enabled: true

oauth2_proxy_environment_variable_provider: keycloak-oidc
oauth2_proxy_environment_variable_provider_display_name: SSO

oauth2_proxy_environment_variable_client_id: hubsite
oauth2_proxy_environment_variable_client_secret: ''
oauth2_proxy_environment_variable_oidc_issuer_url: https://keycloak.example.com/realms/my-realm
oauth2_proxy_environment_variable_redirect_url: "https://{{ hubsite_hostname }}/oauth2/callback"

oauth2_proxy_environment_variable_code_challenge_method: S256

# Generate this with: `python -c 'import os,base64; print(base64.urlsafe_b64encode(os.urandom(32)).decode())'`
oauth2_proxy_environment_variable_cookie_secret: ''

oauth2_proxy_container_labels_additional_labels: |
  traefik.http.routers.{{ oauth2_proxy_identifier }}-hubsite.rule=Host(`{{ hubsite_hostname }}`) && PathPrefix(`/oauth2/`)
  traefik.http.routers.{{ oauth2_proxy_identifier }}-hubsite.service={{ oauth2_proxy_identifier }}
  traefik.http.routers.{{ oauth2_proxy_identifier }}-hubsite.entrypoints={{ oauth2_proxy_container_labels_traefik_entrypoints }}
  traefik.http.routers.{{ oauth2_proxy_identifier }}-hubsite.tls={{ oauth2_proxy_container_labels_traefik_tls }}
  traefik.http.routers.{{ oauth2_proxy_identifier }}-hubsite.tls.certResolver={{ oauth2_proxy_container_labels_traefik_tls_certResolver }}

########################################################################
#                                                                      #
# /oauth2_proxy                                                        #
#                                                                      #
########################################################################
```

After adding this to your `vars.yml` file, [re-run the playbook](../installing.md): `just install-service oauth-2proxy`.

This merely configures OAuth2-Proxy to handle the `/oauth2/` paths for Hubsite's domain.

[Hubsite configuration adjustments](#hubsite-configuration-adjustments) are also necessary, so proceed to do those as well.


### Hubsite configuration adjustments

Now that OAuth2-Proxy is ready and handling the `/oauth2/` paths on the domain Hubsite is running, we need to set up Traefik's [ForwardAuth](https://doc.traefik.io/traefik/middlewares/http/forwardauth/) middlware, so that all Hubsite requests would consult OAuth2-Proxy.

The configuration described below is based on the official [Configuring for use with the Traefik (v2) ForwardAuth middleware](https://oauth2-proxy.github.io/oauth2-proxy/configuration/overview#configuring-for-use-with-the-traefik-v2-forwardauth-middleware) documentation of OAuth2-Proxy.

```yml
########################################################################
#                                                                      #
# hubsite                                                              #
#                                                                      #
########################################################################

# Your other Hubsite configuration goes here.
# See the documentation in hubsite.md.

hubsite_container_labels_additional_labels: |
  # Create a middleware which catches "unauthenticated" errors and serves the OAuth-Proxy sign in page.
  traefik.http.middlewares.{{ hubsite_identifier }}-oauth-errors.errors.status=401-403
  traefik.http.middlewares.{{ hubsite_identifier }}-oauth-errors.errors.service={{ oauth2_proxy_identifier }}
  traefik.http.middlewares.{{ hubsite_identifier }}-oauth-errors.errors.query=/oauth2/sign_in?rd={url}

  # Create a middlware which passes each incoming request to OAuth2-Proxy,
  # so it can decide whether it should be let through (to Hubsite) or should blocked (serving the OAuth2-Proxy sign in page).
  traefik.http.middlewares.{{ hubsite_identifier }}-oauth-auth.forwardAuth.address=http://{{ oauth2_proxy_identifier }}:{{ oauth2_proxy_container_process_http_port }}/oauth2/auth

  traefik.http.middlewares.{{ hubsite_identifier }}-oauth-auth.forwardAuth.trustForwardHeader=true

  # Let a few HTTP headers set by OAuth2-Proxy get passed to Hubsite.
  # Hubsite is a static website, so it cannot make use of them.
  # Nevertheless, this is here as an example of how you can whitelist headers,
  # so that applications which can make use of these headers can benefit from it.
  # See more information about this in the comments for `oauth2_proxy_environment_variable_set_xauthrequest`.
  traefik.http.middlewares.{{ hubsite_identifier }}-oauth-auth.forwardAuth.authResponseHeaders=X-Auth-Request-Preferred-Username, X-Auth-Request-Groups

  # Inject the 2 middlewares defined above into the router of the Hubsite service
  traefik.http.routers.{{ hubsite_identifier }}.middlewares={{ hubsite_identifier }}-oauth-errors,{{ hubsite_identifier }}-oauth-auth

########################################################################
#                                                                      #
# /hubsite                                                             #
#                                                                      #
########################################################################
```

After adding this to your `vars.yml` file, [re-run the playbook](../installing.md): `just install-service hubsite`.

Some [services](../supported-services.md) already define their own `middlewares` in their Traefik `labels` file, so you may not be able to inject new ones the same way as done for Hubsite above.

Specific services (e.g. [Nextcloud](./nextcloud.md)) provide Ansible variables (`nextcloud_container_labels_traefik_http_middlewares_custom`) for injecting new middlewares at a specific position (priority) in the list. Others services (Ansible roles) do not support this yet, which would prevent you from using them this way. Consider submitting an issue or better yet opening a PR to improve these services.


## Further reading

If you'd like to do something more advanced, the [`ansible-role-oauth2-proxy` Ansible role](https://github.com/mother-of-all-self-hosting/ansible-role-oauth2-proxy) is very configurable and should let you do what you need.

Take a look at [its `default/main.yml` file](https://github.com/mother-of-all-self-hosting/ansible-role-oauth2-proxy/blob/main/defaults/main.yml) for available Ansible variables you can use in your own `vars.yml` configuration file.


## Related services

- [authentik](authentik.md) — An open-source Identity Provider focused on flexibility and versatility.
- [Keycloak](keycloak.md) — An open source identity and access management solution
- [Authelia](authelia.md) — An open-source authentication and authorization server that can work as a companion to [common reverse proxies](https://www.authelia.com/overview/prologue/supported-proxies/) (like [Traefik](traefik.md) frequently used by this playbook)
````

## File: docs/services/outline.md
````markdown
# Outline

[Outline](https://www.getoutline.com/) is an open-source knowledge base for growing teams.


## Dependencies

This service requires the following other services:

- [Postgres](postgres.md)
- [Valkey](valkey.md)
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, besides enabling the [required Dependencies](#dependencies), add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# outline                                                              #
#                                                                      #
########################################################################

outline_enabled: true

outline_hostname: outline.example.com

# This must be generated with: `openssl rand -hex 32`
outline_environment_variable_secret_key: ''

# The configuration below connects Outline to the Valkey instance, for session storage purposes.
# You may wish to run a separate Valkey instance for Outline, because Valkey is not multi-tenant.
# Read more in docs/services/valkey.md.
outline_redis_hostname: "{{ valkey_identifier if valkey_enabled else '' }}"

outline_container_additional_networks_custom: |
  {{
    [valkey_container_network]
  }}

# By default, files are stored locally.
# To use another file storage provider, see the "File Storage" section below.

# At least one authentication method MUST be enabled for Outline to work.
# See the "Authentication" section below.

########################################################################
#                                                                      #
# /outline                                                             #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://outline.example.com`.

While the Outline Ansible role provides an `outline_path_prefix` variable, Outline does not support being hosted at a subpath right now.

### File Storage

Outline supports multiple [file storage](https://docs.getoutline.com/s/hosting/doc/file-storage-N4M0T6Ypu7) mechanisms.

The default configuration stores files locally in a `data` directory, but you can also stores files on AWS S3 (or [compatible S3 altenative](https://docs.getoutline.com/s/hosting/doc/file-storage-N4M0T6Ypu7#h-s3-compatible-services)).

To enable S3 storage, add the following to your `vars.yml` configuration:

```yml
outline_environment_variable_file_storage: s3

outline_environment_variable_aws_access_key_id: ''
outline_environment_variable_aws_secret_access_key: ''
outline_environment_variable_aws_region: eu-central-1 # example
outline_environment_variable_aws_s3_upload_bucket_url: https://OUTLINE_ASSETS_BUCKET_NAME.s3.eu-central-1.amazonaws.com
outline_environment_variable_aws_s3_upload_bucket_name: OUTLINE_ASSETS_BUCKET_NAME
outline_environment_variable_aws_s3_force_path_style: false
```

### Authentication

For Outline to work, at least one [authentication method](https://docs.getoutline.com/s/hosting/doc/authentication-7ViKRmRY5o) must be enabled.

The Outline Ansible role provides dedicated Ansible variables for configuring these authentication methods via environment variables (see the `outline_environment_variable_*` variables in [`defaults/main.yml` of ansible-role-outline](https://github.com/mother-of-all-self-hosting/ansible-role-outline/blob/main/defaults/main.yml)).

If you need to pass additional environment variables to Outline, for which dedicated Ansible variables are not available, you can use `outline_environment_variables_additional_variables`.

If you define SMTP settings (see the `outline_environment_variable_smtp_*` variables in `defaults/main.yml`), the [Email magic link](https://docs.getoutline.com/s/hosting/doc/email-magic-link-N2CPh5tmTS) authentication method will be enabled:

Unfortunately, even with SMTP settings being defined, we haven't been able to get Outline to succesfully send emails just yet, hitting issues similar to [this one](https://github.com/outline/outline/discussions/2605).
````

## File: docs/services/overseerr.md
````markdown
# Overseerr

[Overseerr](https://www.overseerr.org/) is a request management and media discovery tool for the Plex ecosystem.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# overseerr                                                            #
#                                                                      #
########################################################################

overseerr_enabled: true

overseerr_hostname: overseerr.example.com

########################################################################
#                                                                      #
# /overseerr                                                           #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://overseerr.example.com`.

A `overseerr_path_prefix` variable can be adjusted to host under a subpath (e.g. `overseerr_path_prefix: /overseerr`), but this hasn't been tested yet.

## Usage

After [installation](../installing.md), you should access your new Overseerr instance at the URL you've chosen. Follow the prompts to finish setup. The below guide may be useful:

1. Sign in to Overseerr via Plex. You should get a browser pop-up window.

![Overseerr Sign In](../assets/overseerr/setup-1.png)

2. Configure Plex. There are a few ways to do this, either by manually filling in the form with your Plex URL or clicking the little 'load' icon the right to populate the drop-down and selecting one of those. Whichever you do, its best to select an option that uses HTTPS. Click the `Save` button.

![Overseerr Configure Plex](../assets/overseerr/setup-2.png)

3. Configure Plex some more. Scroll down and click the `Sync Libraries` button and select the libraries you want Overseerr to know about. Next, click `Start Scan` and then `Continue`.

![Overseerr Configure Plex](../assets/overseerr/setup-3.png)

4. Configure Radarr & Sonarr. The only tricky bit here is getting your API key for the service, which can be obtained at your Radarr/Sonarr `Settings -> General` page.

![Overseerr Configure Radarr](../assets/overseerr/setup-4.png)

For additional configuration options, refer to [ansible-role-overseerr](https://github.com/spatterIight/ansible-role-overseerr)'s `defaults/main.yml` file.

## Recommended other services

Consider these other supported services that are also in the [*Arr stack](https://wiki.servarr.com/) of media automation tools:

- [Radarr](radarr.md)
- [Sonarr](sonarr.md)
- [Jackett](jackett.md)
- [qBittorrent](qbittorrent.md)
````

## File: docs/services/owncast.md
````markdown
# Owncast

[Owncast](https://owncast.online/) is a free and open source live video and web chat server for use with existing popular broadcasting software. This playbook can install owncast, powered by the [mother-of-all-self-hosting/ansible-role-owncast](https://github.com/mother-of-all-self-hosting/ansible-role-owncast) Ansible role.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# owncast                                                            #
#                                                                      #
########################################################################

owncast_enabled: true

owncast_hostname: live.example.com

########################################################################
#                                                                      #
# /owncast                                                           #
#                                                                      #
########################################################################
```


### Networking

By default, the following ports will be exposed by the container on **all network interfaces**:

- `1935` over **TCP**, controlled by `owncast_container_rtmp_bind_port` — used for TCP based [RTMP](https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol)

Docker automatically opens this port in the server's firewall, so you **likely don't need to do anything**. If you use another firewall in front of the server, you may need to adjust it.

## Usage

After installation, you can go to the owncast URL, as defined in `owncast_hostname`.

To customize your installation visit `live.example.com/admin`. **You should immediatly change the stream key which is set to `abc123` by default**.
````

## File: docs/services/oxitraffic.md
````markdown
# OxiTraffic

[OxiTraffic](https://codeberg.org/mo8it/oxitraffic) is a self-hosted, simple and privacy respecting website traffic tracker, that this playbook can install, powered by the [mother-of-all-self-hosting/ansible-role-oxitraffic](https://github.com/mother-of-all-self-hosting/ansible-role-oxitraffic) Ansible role.

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# oxitraffic                                                           #
#                                                                      #
########################################################################

oxitraffic_enabled: true
oxitraffic_hostname: traffic.example.org
oxitraffic_tracked_origin: https://example.org

########################################################################
#                                                                      #
# /oxitraffic                                                          #
#                                                                      #
########################################################################
```

You must include the counting script on the `oxitraffic_tracked_origin` by adding the following to you website
```html
<script type="module" src="https://YOUR-OXITRAFFIC_HOSTNAME/count.js"></script>
```

# Notes on Troubleshooting

Internal OxiTraffic errors will not be logged to `stdout` and will therefore not be part of `journalctl -fu mash-oxitraffic`. You should check the log file that is created by OxiTraffic with `tail -f logs/oxitraffic`.

# Data Protection

*This is not legal advice, talk to a lawyer!*

OxiTraffic does not collet IP Adresses, Browser Information etc.. Each visitor is assigned a anonymous ID upon visiting the site. This will only be used to store information on how long the visitor spends on this site. No cookies are set.
````

## File: docs/services/paperless-ngx.md
````markdown
# Paperless-ngx

[Paperless-ngx](https://paperless-ngx.com) s a community-supported open-source document management system that transforms your physical documents into a searchable online archive so you can keep, well, less paper. MASH can install paperless-ngx with the [`mother-of-all-self-hosting/ansible-role-paperless`](https://github.com/mother-of-all-self-hosting/ansible-role-paperless) ansible role.

> [!WARNING]
> Paperless-ngx currently [does not support](https://github.com/paperless-ngx/paperless-ngx/issues/6352) running the container rootless, therefore the role has not the usual security features of other services provided by this playbook. This put your system more at higher risk as vulnerabilities can have a higher impact.

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Valkey](valkey.md) data-store, installation details [below](#valkey)
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# paperless                                                            #
#                                                                      #
########################################################################

paperless_enabled: true

paperless_hostname: paperless.example.org

# Set the following variables to create an initial admin user
# It will not re-create an admin user, it will not change a password if the user is already created
# paperless_admin_user: USERNAME
# paperless_admin_password: SECURE_PASSWORD

# Valkey configuration, as described below

########################################################################
#                                                                      #
# /paperless                                                           #
#                                                                      #
########################################################################
```

### Valkey

As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require KeyDB on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to paperless-ngx](#creating-a-valkey-instance-dedicated-to-paperless-ngx).

If you're only running paperless-ngx on this server and don't need to use KeyDB for anything else, you can [use a single Valkey instance](#using-the-shared-valkey-instance-for-paperless).

#### Using the shared Valkey instance for paperless-ngx

To install a single (non-dedicated) Valkey instance (`mash-valkey`) and hook paperless to it, add the following **additional** configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# paperless                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point paperless to the shared Valkey instance
paperless_redis_hostname: "{{ valkey_identifier }}"

# Make sure the paperless service (mash-paperless.service) starts after the shared KeyDB service (mash-valkey.service)
paperless_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the paperless container is connected to the container network of the shared KeyDB service (mash-valkey)
paperless_container_additional_networks_custom:
  - "{{ valkey_identifier }}"

########################################################################
#                                                                      #
# /paperless                                                           #
#                                                                      #
########################################################################
```

This will create a `mash-valkey` Valkey instance on this host.

This is only recommended if you won't be installing other services which require KeyDB. Alternatively, go for [Creating a Valkey instance dedicated to paperless-ngx](#creating-a-valkey-instance-dedicated-to-paperless-ngx).


#### Creating a Valkey instance dedicated to paperless

The following instructions are based on the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `paperless.example.org` is your main one, create `paperless.example.org-deps`).

Then, create a new `vars.yml` file for the

`inventory/host_vars/paperless.example.org-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-paperless-'
mash_playbook_service_base_directory_name_prefix: 'paperless-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-paperless-valkey` instance on this host with its data in `/mash/paperless-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/paperless.example.org/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# paperless                                                            #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point paperless to its dedicated Valkey instance
paperless_redis_hostname: mash-paperless-valkey

# Make sure the paperless service (mash-paperless.service) starts after its dedicated KeyDB service (mash-paperless-valkey.service)
paperless_systemd_required_services_list_custom:
  - "mash-paperless-valkey.service"

# Make sure the paperless container is connected to the container network of its dedicated KeyDB service (mash-paperless-valkey)
paperless_container_additional_networks_custom:
  - "mash-paperless-valkey"

########################################################################
#                                                                      #
# /paperless                                                           #
#                                                                      #
########################################################################
```


## Installation

If you've decided to install a dedicated Valkey instance for paperless, make sure to first do [installation](../installing.md) for the supplementary inventory host (e.g. `paperless.example.org-deps`), before running installation for the main one (e.g. `paperless.example.org`).


## Usage

Access your instance in your browser at `https://paperless.example.org`

Refer to the [official documentation](https://docs.paperless-ngx.com/) to learn how to use paperless.
````

## File: docs/services/peertube.md
````markdown
# PeerTube

[PeerTube](https://joinpeertube.org/) is a tool for sharing online videos developed by [Framasoft](https://framasoft.org/), a french non-profit.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Valkey](valkey.md) data-store, installation details [below](#valkey)
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# peertube                                                             #
#                                                                      #
########################################################################

peertube_enabled: true

peertube_hostname: peertube.example.com

# PeerTube does not support being hosted at a subpath right now,
# so using the peertube_path_prefix variable is not possible.

# A PeerTube secret.
# You can put any string here, but generating a strong one is preferred (e.g. `pwgen -s 64 1`).
peertube_config_secret: ''

# An email address to be associated with the `root` PeerTube administrator account.
peertube_config_admin_email: ''

# The initial password that the `root` PeerTube administrator account will be created with.
# You can put any string here, but generating a strong one is preferred (e.g. `pwgen -s 64 1`).
peertube_config_root_user_initial_password: ''

# Uncomment and adjust this after completing the initial installation.
# Find the `traefik` network's IP address range by running the following command on the server:
# `docker network inspect traefik -f "{{ (index .IPAM.Config 0).Subnet }}"`
# Then, replace the example IP range below, and re-run the playbook.
# peertube_trusted_proxies_values_custom: ["172.21.0.0/16"]

# Valkey configuration, as described below

########################################################################
#                                                                      #
# /peertube                                                            #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://peertube.example.com`.

Hosting PeerTube under a subpath (by configuring the `peertube_path_prefix` variable) does not seem to be possible right now, due to PeerTube limitations.

### Valkey

As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require KeyDB on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to PeerTube](#creating-a-valkey-instance-dedicated-to-peertube).

If you're only running PeerTube on this server and don't need to use KeyDB for anything else, you can [use a single Valkey instance](#using-the-shared-valkey-instance-for-peertube).

#### Using the shared Valkey instance for PeerTube

To install a single (non-dedicated) Valkey instance (`mash-valkey`) and hook PeerTube to it, add the following **additional** configuration:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# peertube                                                             #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point PeerTube to the shared Valkey instance
peertube_config_redis_hostname: "{{ valkey_identifier }}"

# Make sure the PeerTube service (mash-peertube.service) starts after the shared KeyDB service (mash-valkey.service)
peertube_systemd_required_services_list_custom:
  - "{{ valkey_identifier }}.service"

# Make sure the PeerTube container is connected to the container network of the shared KeyDB service (mash-valkey)
peertube_container_additional_networks_custom:
  - "{{ valkey_identifier }}"

########################################################################
#                                                                      #
# /peertube                                                            #
#                                                                      #
########################################################################
```

This will create a `mash-valkey` Valkey instance on this host.

This is only recommended if you won't be installing other services which require KeyDB. Alternatively, go for [Creating a Valkey instance dedicated to PeerTube](#creating-a-valkey-instance-dedicated-to-peertube).


#### Creating a Valkey instance dedicated to PeerTube

The following instructions are based on the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `peertube.example.com` is your main one, create `peertube.example.com-deps`).

Then, create a new `vars.yml` file for the

`inventory/host_vars/peertube.example.com-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-peertube-'
mash_playbook_service_base_directory_name_prefix: 'peertube-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-peertube-valkey` instance on this host with its data in `/mash/peertube-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/peertube.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# peertube                                                             #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point PeerTube to its dedicated Valkey instance
peertube_config_redis_hostname: mash-peertube-valkey

# Make sure the PeerTube service (mash-peertube.service) starts after its dedicated KeyDB service (mash-peertube-valkey.service)
peertube_systemd_required_services_list_custom:
  - "mash-peertube-valkey.service"

# Make sure the PeerTube container is connected to the container network of its dedicated KeyDB service (mash-peertube-valkey)
peertube_container_additional_networks_custom:
  - "mash-peertube-valkey"

########################################################################
#                                                                      #
# /peertube                                                            #
#                                                                      #
########################################################################
```


## Installation

If you've decided to install a dedicated Valkey instance for PeerTube, make sure to first do [installation](../installing.md) for the supplementary inventory host (e.g. `peertube.example.com-deps`), before running installation for the main one (e.g. `peertube.example.com`).


## Usage

After [installation](../installing.md), you should be able to access your new PeerTube instance at the URL you've chosen (depending on `peertube_hostname` and `peertube_path_prefix` values set in `vars.yml`).

You should then be able to log in with:

- username: `root`
- password: the password you've set in `peertube_config_root_user_initial_password` in `vars.yml`
````

## File: docs/services/plausible.md
````markdown
# Plausible Analytics

[Plausible Analytics](https://plausible.io/) is intuitive, lightweight and open source web analytics. No cookies and fully compliant with GDPR, CCPA and PECR.

With this playbook, you can install the [Community Edition](https://plausible.io/blog/community-edition) of Plausible Analytics.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [ClickHouse](clickhouse.md) database
- a [Traefik](traefik.md) reverse-proxy server
- (optional) the [exim-relay](exim-relay.md) mailer


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# plausible                                                            #
#                                                                      #
########################################################################

plausible_enabled: true

plausible_hostname: plausible.example.com

# Generate this with: `openssl rand -base64 48`
plausible_environment_variable_secret_key_base: ''

# Generate this with: `openssl rand -base64 32`
plausible_environment_variable_totp_vault_key: ''

# Controls which user ids will be system admins
# By default, only the first user (`1`) to be registered will be made an admin.
# plausible_environment_variable_admin_user_ids: '1,2,3'

########################################################################
#                                                                      #
# /plausible                                                           #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://plausible.example.com`.

The Ansible role for Plausible Analytics contains a `plausible_path_prefix` variable for hosting at a subdirectory, but this is not implemented yet. See the comments about `plausible_path_prefix` in [ansible-role-plausible](https://github.com/mother-of-all-self-hosting/ansible-role-plausible)'s `defaults/main.yml` file.


## Usage

After [installation](../installing.md), you should be able to access your new Plausible Analytics instance at the URL you've chosen.

You should then be able to create your first user account, which will be created as an admin (see the details about `plausible_environment_variable_admin_user_ids` above).

After logging in with your user account you can create properties (websites) and invite other users by email.
By default, the system is configured to allow registrations that are coming from an explicit invitation, while public registrations are disabled. This can be controlled via the `plausible_environment_variable_disable_registration` variable.

For additional configuration options, refer to [ansible-role-plausible](https://github.com/mother-of-all-self-hosting/ansible-role-plausible)'s `defaults/main.yml` file.
````

## File: docs/services/postgis.md
````markdown
# Postgis

[Postgis](https://postgis.net/) is a spatial database extender for PostgreSQL object-relational database. It adds support for geographic objects allowing location queries to be run in SQL. 

Services like [Mobilizon](./mobilizon.md) depend on the ability to store gespatial data. 
Enabling the PPostgisostgres database service will automatically wire these services to use it.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# postgis                                                              #
#                                                                      #
########################################################################

postgis_enabled: true

# Put a strong password below, generated with `pwgen -s 64 1` or in another way
postgis_connection_password: ''

########################################################################
#                                                                      #
# /postgis                                                             #
#                                                                      #
########################################################################
```
````

## File: docs/services/postgres-backup.md
````markdown
<!--
SPDX-FileCopyrightText: 2021 foxcris
SPDX-FileCopyrightText: 2021 - 2024 Slavi Pantaleev
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Postgres Backup

The playbook can install and configure [docker-postgres-backup-local](https://github.com/prodrigestivill/docker-postgres-backup-local) for you.

The [Ansible role for docker-postgres-backup-local](https://github.com/mother-of-all-self-hosting/ansible-role-postgres-backup) is developed and maintained by the MASH project. For details about configuring docker-postgres-backup-local, you can check them via:
- 🌐 [the role's documentation](https://github.com/mother-of-all-self-hosting/ansible-role-postgres-backup/blob/main/docs/configuring-postgres-backup.md) online
- 📁 `roles/galaxy/postgres_backup/docs/configuring-postgres-backup.md` locally, if you have [fetched the Ansible roles](../installing.md)

**Note**: for a more complete backup solution (one that includes not only Postgres, but also other configuration/data files), you may wish to look into [BorgBackup](backup-borg.md) instead.
````

## File: docs/services/postgres.md
````markdown
<!--
SPDX-FileCopyrightText: 2019 - 2024 Slavi Pantaleev
SPDX-FileCopyrightText: 2020 - 2021 Aaron Raimist
SPDX-FileCopyrightText: 2020 Hardy Erlinger
SPDX-FileCopyrightText: 2021 - 2024 MDAD project contributors
SPDX-FileCopyrightText: 2021 Marc Leuser
SPDX-FileCopyrightText: 2024 Kim Brose
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Postgres

[PostgreSQL](https://www.postgresql.org) is a powerful, open source object-relational database system with over 35 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.

Many of the services installed by this playbook require a Postgres database.

Enabling the Postgres database service will automatically wire all other services which require such a database to use it.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# postgres                                                             #
#                                                                      #
########################################################################

postgres_enabled: true

# Put a strong password below, generated with `pwgen -s 64 1` or in another way
postgres_connection_password: ''

########################################################################
#                                                                      #
# /postgres                                                            #
#                                                                      #
########################################################################
```

## Importing

### Importing an existing Postgres database from another installation (optional)

Follow this section if you'd like to import your database from a previous installation.

### Prerequisites

The playbook supports importing Postgres dump files in **text** (e.g. `pg_dump > dump.sql`) or **gzipped** formats (e.g. `pg_dump | gzip -c > dump.sql.gz`).

Importing multiple databases (as dumped by `pg_dumpall`) is also supported.

Before doing the actual import, **you need to upload your Postgres dump file to the server** (any path is okay).


### Importing a dump file

To import, run this command (make sure to replace `SERVER_PATH_TO_POSTGRES_DUMP_FILE` with a file path on your server):

```sh
just run-tags import-postgres \
--extra-vars=server_path_postgres_dump=SERVER_PATH_TO_POSTGRES_DUMP_FILE \
--extra-vars=postgres_default_import_database=main
```

**Notes**:

- `SERVER_PATH_TO_POSTGRES_DUMP_FILE` must be a file path to a Postgres dump file on the server (not on your local machine!)
- `postgres_default_import_database` defaults to `main`, which is useful for importing multiple databases (for dumps made with `pg_dumpall`). If you're importing a single database (e.g. `miniflux`), consider changing `postgres_default_import_database` to the name of the database (e.g. `miniflux`)
- after importing a large database, it's a good idea to run [an `ANALYZE` operation](https://www.postgresql.org/docs/current/sql-analyze.html) to make Postgres rebuild its database statistics and optimize its query planner. You can easily do this via the playbook by running `just run-tags run-postgres-vacuum -e postgres_vacuum_preset=analyze` (see [Vacuuming PostgreSQL](#vacuuming-postgresql) for more details).


## Maintenance

This section shows you how to perform various maintenance tasks related to the Postgres database server used by various components of this playbook.

Table of contents:
- [Getting a database terminal](#getting-a-database-terminal), for when you wish to execute SQL queries
- [Vacuuming PostgreSQL](#vacuuming-postgresql), for when you wish to run a Postgres [VACUUM](https://www.postgresql.org/docs/current/sql-vacuum.html) (optimizing disk space)
- [Backing up PostgreSQL](#backing-up-postgresql), for when you wish to make a backup
- [Upgrading PostgreSQL](#upgrading-postgresql), for upgrading to new major versions of PostgreSQL. Such **manual upgrades are sometimes required**.
- [Tuning PostgreSQL](#tuning-postgresql) to make it run faster

### Getting a database terminal

You can use the `/mash/postgres/bin/cli` tool to get interactive terminal access ([psql](https://www.postgresql.org/docs/current/app-psql.html)) to the PostgreSQL server.

By default, this tool puts you in the `main` database, which contains nothing.

To see the available databases, run `\list` (or just `\l`).

To change to another database (for example `miniflux`), run `\connect miniflux` (or just `\c miniflux`).

You can then proceed to write queries. Example: `SELECT COUNT(*) FROM users;`

> [!WARNING]
> **Modifying the database directly (especially as services are running) is dangerous and may lead to irreversible database corruption.** When in doubt, consider [making a backup](#backing-up-postgresql).

### Vacuuming PostgreSQL

Deleting lots data from Postgres does not make it release disk space, until you perform a [`VACUUM` operation](https://www.postgresql.org/docs/current/sql-vacuum.html).

You can run different `VACUUM` operations via the playbook, with the default preset being `vacuum-complete`:

- (default) `vacuum-complete`: stops all services temporarily and runs `VACUUM FULL VERBOSE ANALYZE`.
- `vacuum-full`: stops all services temporarily and runs `VACUUM FULL VERBOSE`
- `vacuum`: runs `VACUUM VERBOSE` without stopping any services
- `vacuum-analyze` runs `VACUUM VERBOSE ANALYZE` without stopping any services
- `analyze` runs `ANALYZE VERBOSE` without stopping any services (this is just [ANALYZE](https://www.postgresql.org/docs/current/sql-analyze.html) without doing a vacuum, so it's faster)

**Note**: for the `vacuum-complete` and `vacuum-full` presets, you'll need plenty of available disk space in your Postgres data directory (usually `/mash/postgres/data`). These presets also stop all services while the vacuum operation is running.

Example playbook invocations:

- `just run-tags run-postgres-vacuum`: runs the default `vacuum-complete` preset and restarts all services
- `just run-tags run-postgres-vacuum -e postgres_vacuum_preset=analyze`: runs the `analyze` preset with all services remaining operational at all times

### Backing up PostgreSQL

To automatically make Postgres database backups on a fixed schedule, consider enabling the [Postgres Backup](postgres-backup.md) service.

To make a one-off back up of the current PostgreSQL database, make sure it's running and then execute a command like this on the server:

```sh
/usr/bin/docker exec \
--env-file=/mash/postgres/env-postgres-psql \
mash-postgres \
/usr/local/bin/pg_dumpall -h mash-postgres \
| gzip -c \
> /mash/postgres.sql.gz
```

Restoring a backup made this way can be done by [importing it](#importing).

### Upgrading PostgreSQL

Once installed, the playbook attempts to preserve the Postgres version it starts with. This is because newer Postgres versions cannot start with data generated by older Postgres versions.

Upgrades must be performed manually.

The playbook can upgrade your existing Postgres setup with the following command:

```sh
just run-tags upgrade-postgres
```

**The old Postgres data directory is backed up** automatically, by renaming it to `/mash/postgres/data-auto-upgrade-backup`. To rename to a different path, pass some extra flags to the command above, like this: `--extra-vars="postgres_auto_upgrade_backup_data_path=/another/disk/mash-postgres-before-upgrade"`

The auto-upgrade-backup directory stays around forever, until you **manually decide to delete it**.

As part of the upgrade, the database is dumped to `/tmp`, an upgraded and empty Postgres server is started, and then the dump is restored into the new server. To use a different directory for the dump, pass some extra flags to the command above, like this: `--extra-vars="postgres_dump_dir=/directory/to/dump/here"`

To save disk space in `/tmp`, the dump file is gzipped on the fly at the expense of CPU usage. If you have plenty of space in `/tmp` and would rather avoid gzipping, you can explicitly pass a dump filename which doesn't end in `.gz`. Example: `--extra-vars="postgres_dump_name=mash-postgres-dump.sql"`

**All databases, roles, etc. on the Postgres server are migrated**.

## Tuning PostgreSQL

PostgreSQL can be [tuned](https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server) to make it run faster. This is done by passing extra arguments to the Postgres process.

The [Postgres Ansible role](https://github.com/mother-of-all-self-hosting/ansible-role-postgres) **already does some tuning by default**, which matches the [tuning logic](https://github.com/le0pard/pgtune/blob/master/src/features/configuration/configurationSlice.js) done by websites like https://pgtune.leopard.in.ua/. You can manually influence some of the tuning variables. These parameters (variables) are injected via the `postgres_postgres_process_extra_arguments_auto` variable.

Most users should be fine with the automatically-done tuning. However, you may wish to:

- **adjust the automatically-determined tuning parameters manually**: change the values for the tuning variables defined in the Postgres role's [default configuration file](https://github.com/mother-of-all-self-hosting/ansible-role-postgres/blob/main/defaults/main.yml) (see `postgres_max_connections`, `postgres_data_storage` etc). These variables are ultimately passed to Postgres via a `postgres_postgres_process_extra_arguments_auto` variable

- **turn automatically-performed tuning off**: override it like this: `postgres_postgres_process_extra_arguments_auto: []`

- **add additional tuning parameters**: define your additional Postgres configuration parameters in `postgres_postgres_process_extra_arguments_custom`. See `postgres_postgres_process_extra_arguments_auto` defined in the Postgres role's [default configuration file](https://github.com/mother-of-all-self-hosting/ansible-role-postgres/blob/main/defaults/main.yml) for inspiration

## Recommended other services

You may also wish to look into:

- [Postgres Backup](postgres-backup.md) for backing up your Postgres database

- [Prometheus](prometheus.md), [prometheus-postgres-exporter](prometheus-postgres-exporter.md) and [Grafana](grafana.md) for monitoring your Postgres database
````

## File: docs/services/prometheus-blackbox-exporter.md
````markdown
# Prometheus Blackbox Exporter

This playbook can configure [Prometheus Blackbox Exporter](https://github.com/prometheus/blackbox_exporter).

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# prometheus-blackbox-exporter                                         #
#                                                                      #
########################################################################

prometheus_blackbox_exporter_enabled: true

# To expose the metrics publicly, enable and configure the lines below:
# prometheus_blackbox_exporter_hostname: mash.example.com
# prometheus_blackbox_exporter_path_prefix: /metrics/mash-prometheus-blackbox-exporter

# To protect the metrics with HTTP Basic Auth, enable and configure the lines below.
# See: https://doc.traefik.io/traefik/middlewares/http/basicauth/#users
# prometheus_blackbox_exporter_container_labels_metrics_middleware_basic_auth_enabled: true
# prometheus_blackbox_exporter_container_labels_metrics_middleware_basic_auth_users: ''

########################################################################
#                                                                      #
# /prometheus-blackbox-exporter                                        #
#                                                                      #
########################################################################
```

## Usage

After you've installed the blackbox exporter, your blackbox prober will be available on `mash.example.com/metrics/mash-prometheus-blackbox-exporter` with the basic auth credentials you've configured if hostname and path prefix where provided
````

## File: docs/services/prometheus-node-exporter.md
````markdown
# Prometheus Node Exporter

This playbook can configure [Prometheus Node Exporter](https://github.com/prometheus/node_exporter).


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# prometheus-node-exporter                                             #
#                                                                      #
########################################################################

prometheus_node_exporter_enabled: true

# To expose the metrics publicly, enable and configure the lines below:
# prometheus_node_exporter_hostname: mash.example.com
# prometheus_node_exporter_path_prefix: /metrics/mash-prometheus-node-exporter

# To protect the metrics with HTTP Basic Auth, enable and configure the lines below.
# See: https://doc.traefik.io/traefik/middlewares/http/basicauth/#users
# prometheus_node_exporter_container_labels_metrics_middleware_basic_auth_enabled: true
# prometheus_node_exporter_container_labels_metrics_middleware_basic_auth_users: ''

########################################################################
#                                                                      #
# /prometheus-node-exporter                                            #
#                                                                      #
########################################################################
```

Unless you're scraping the Prometheus Node Exporter metrics from a local [Prometheus](prometheus.md) instance, as described in [Integrating with Prometheus Node Exporter](prometheus.md#integrating-with-prometheus-node-exporter), you will probably wish to expose the metrics publicly so that a remote Prometheus instance can fetch them.

## Usage

After you installed the node exporter, your node stats will be available on `mash.example.com/metrics/mash-prometheus-node-exporter` with the basic auth credentials you configured.

To integrate Prometheus Node Exporter with a [Prometheus](prometheus.md) instance, see the [Integrating with Prometheus Node Exporter](prometheus.md#integrating-with-prometheus-node-exporter) section of the documentation.


## Recommended other services

- [Promtail](promtail.md) — an agent which ships the contents of local logs to a private [Grafana Loki](grafana-loki.md) instance
````

## File: docs/services/prometheus-postgres-exporter.md
````markdown
# Postgres Exporter

This playbook can configure [Postgres Exporter](https://github.com/prometheus-community/postgres_exporter) by utilizing [mother-of-all-self-hosting/ansible-role-postgres-exporter](https://github.com/mother-of-all-self-hosting/ansible-role-prometheus-postgres-exporter.git).


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# prometheus_postgres_exporter                                         #
#                                                                      #
########################################################################

prometheus_postgres_exporter_enabled: true

# To expose the metrics publicly, enable and configure the lines below:
# prometheus_postgres_exporter_hostname: mash.example.com
# prometheus_postgres_exporter_path_prefix: /metrics/mash-prometheus-postgres-exporter

# To protect the metrics with HTTP Basic Auth, enable and configure the lines below.
# See: https://doc.traefik.io/traefik/middlewares/http/basicauth/#users
# prometheus_postgres_exporter_container_labels_metrics_middleware_basic_auth_enabled: true
# prometheus_postgres_exporter_container_labels_metrics_middleware_basic_auth_users: ''

########################################################################
#                                                                      #
# /prometheus_postgres_exporter                                        #
#                                                                      #
########################################################################
```

Unless you're scraping the Postgres Exporter metrics from a local [Prometheus](prometheus.md) instance, as described in [Integrating with Postgres Exporter](prometheus.md#integrating-with-postgres-exporter), you will probably wish to expose the metrics publicly so that a remote Prometheus instance can fetch them.

## Usage

After you installed the exporter, your stats will be available on `mash.example.com/metrics/mash-prometheus-postgres-exporter` with basic auth credentials you configured
````

## File: docs/services/prometheus-ssh-exporter.md
````markdown
# Prometheus SSH Exporter

This playbook can configure [Prometheus SSH Exporter](https://github.com/treydock/ssh_exporter).

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# prometheus-ssh-exporter                                              #
#                                                                      #
########################################################################

prometheus_ssh_exporter_enabled: true

# To expose the metrics publicly, enable and configure the lines below:
# prometheus_ssh_exporter_hostname: mash.example.com
# prometheus_ssh_exporter_path_prefix: /metrics/mash-prometheus-ssh-exporter

# To protect the metrics with HTTP Basic Auth, enable and configure the lines below.
# See: https://doc.traefik.io/traefik/middlewares/http/basicauth/#users
# prometheus_ssh_exporter_container_labels_metrics_middleware_basic_auth_enabled: true
# prometheus_ssh_exporter_container_labels_metrics_middleware_basic_auth_users: ''

########################################################################
#                                                                      #
# /prometheus-ssh-exporter                                             #
#                                                                      #
########################################################################
```

## Usage

After you've installed the ssh exporter, your ssh prober will be available on `mash.example.com/metrics/mash-prometheus-ssh-exporter` with the basic auth credentials you've configured if hostname and path prefix were provided.
````

## File: docs/services/prometheus.md
````markdown
# Prometheus

[Prometheus](https://prometheus.io/) is a metrics collection and alerting monitoring solution.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# prometheus                                                           #
#                                                                      #
########################################################################

prometheus_enabled: true

########################################################################
#                                                                      #
# /prometheus                                                          #
#                                                                      #
########################################################################
```

By default, Prometheus is configured to scrape (collect metrics from) its own process. If you wish to disable this behavior, use `prometheus_self_process_scraper_enabled: false`.

To make Prometheus useful, you'll need to make it scrape one or more hosts by adjusting the configuration.


### Integrating with Prometheus Node Exporter

If you've installed [Prometheus Node Exporter](prometheus-node-exporter.md) on the same host, you can make Prometheus scrape its metrics with the following **additional configuration**:

```yaml
prometheus_self_node_scraper_enabled: true
prometheus_self_node_scraper_static_configs_target: "{{ prometheus_node_exporter_identifier }}:9100"

# node-exporter runs in another container network, so we need to connect to it.
prometheus_container_additional_networks:
  - "{{ prometheus_node_exporter_container_network }}"
```

To scrape a **remote** Prometheus Node Exporter instance, do not use `prometheus_self_node_scraper_*`, but rather follow the [Scraping any other exporter service](#scraping-any-other-exporter-service) guide below.


### Scraping any other exporter service

To inject your own scrape configuration, use the `prometheus_config_scrape_configs_additional` variable that's part of the [ansible-role-prometheus](https://github.com/mother-of-all-self-hosting/ansible-role-prometheus) Ansible role.

Example **additional** configuration:

```yaml
prometheus_config_scrape_configs_additional:
  - job_name: some_job
    metrics_path: /metrics
    scrape_interval: 120s
    scrape_timeout: 120s
    static_configs:
      - targets:
          - some-host:8080

  - job_name: another_job
    metrics_path: /metrics
    scrape_interval: 120s
    scrape_timeout: 120s
    static_configs:
      - targets:
          - another-host:8080
```

If you're scraping others services running in containers over the container network, make sure the Prometheus container is connected to their own network by adjusting `prometheus_container_additional_networks` as demonstrated above for [Integrating with Prometheus Node Exporter](#integrating-with-prometheus-node-exporter).


### Exposing the web interface

By setting a hostname you will expose prometheus on this domain.
Usually you should also set up basic_auth in this case, otherwise everyone will be able to access your metrics

```yaml
prometheus_hostname: prometheus.example.com

# Uncommenting the following lines allows you to configure basic auth
# prometheus_container_labels_metrics_middleware_basic_auth_enabled: true
# Use `htpasswd -nb USERNAME PASSSWORD` to generate the users below.
# prometheus_container_labels_metrics_middleware_basic_auth_users: ''
```

## Recommended other services

- [Grafana](grafana.md) — a web-based tool for visualizing your Prometheus metrics (time-series)
- [Grafana Loki](grafana-loki.md) — a log aggregation system that helps collect, store, and analyze logs in a scalable and efficient manner (like Prometheus, but for logs)
- [prometheus-blackbox-exporter](prometheus-blackbox-exporter.md) — Blackbox probing of HTTP/HTTPS/DNS/TCP/ICMP and gRPC endpoints
- [prometheus-node-exporter](prometheus-node-exporter.md) — an exporter for machine metrics
- [prometheus-postgres-exporter](prometheus-postgres-exporter.md) — an exporter for monitoring a [Postgres](postgres.md) database server
- [Healthchecks](healthchecks.md) — a simple and Effective Cron Job Monitoring solution
````

## File: docs/services/promtail.md
````markdown
# Promtail

[Promtail](https://grafana.com/oss/promtail/) agent is a log aggregation system designed to store and query logs from all your applications and infrastructure. It integrates nicely with [Grafana Loki](./grafana-loki.md).


## Dependencies

This service requires the following other services:

- [Grafana Loki](grafana-loki.md) — a log-storage server where you'd be sending the logs
- (optional) [Traefik](traefik.md) — a reverse-proxy server, if you're exposing Promtail's metrics or API


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# promtail                                                             #
#                                                                      #
########################################################################

promtail_enabled: true

# See "Configuring scrapers" below.
# You need to enable at least one scraper to have Promtail do anything.

# If you haven't enabled Grafana Loki on the same server, you will need
# to define some clients to push logs to.
# See "Configuring clients" below.

########################################################################
#                                                                      #
# /promtail                                                            #
#                                                                      #
########################################################################
```

### Configuring scrapers

**No scrapers are enabled by default**. As such, Promtail does not do anything in its default configuration.

Below, we show you a few built-in scrapers you can easily enable, as well as how to create your own custom ones.

#### Scraping systemd-journald logs

To scrape the [systemd Journal](https://wiki.archlinux.org/title/Systemd/Journal), enable the already-prepared scraper for this with this additional `vars.yml` configuration:

```yml
# Some distros only store a non-persistent (in-memory) journal in a path like in `/run/log/journal`.
# Others may be using a path different than `/var/log/journal`.
# Adjust accordingly.
promtail_journald_scraper_enabled: true
promtail_journald_scraper_host_path: /var/log/journal
```

#### Scraping textual log files (/var/log, etc.)

A lot of distros dump textual log files in `/var/log`. To scrape them, enable the already-prepared scraper for this with this additional `vars.yml` configuration:

```yml
promtail_varlog_scraper_enabled: true
# Consider adjusting this if you'd like to scrape a different path
# promtail_varlog_scraper_host_path: /var/log
```

You can see the configuration for this scraper in the `promtail_varlog_scraper_config` variable in [the `defaults/main.yml` file](https://github.com/mother-of-all-self-hosting/ansible-role-promtail/blob/main/defaults/main.yml) of the ansible-role-promtail Ansible role.

When using this scraper, beware that **log-rotation may lead to double-ingestion** as described [here](https://grafana.com/docs/loki/latest/send-data/promtail/configuration/#example-static-config) in the official documentation:

> If you are rotating logs, be careful when using a wildcard pattern like *.log, and make sure it doesn’t match the rotated log file. For example, if you move your logs from server.log to server.01-01-1970.log in the same directory every night, a static config with a wildcard search pattern like *.log will pick up that new file and read it, effectively causing the entire days logs to be re-ingested.

To work around it, you may wish to adjust `promtail_varlog_scraper_config_labels_path_suffix` which defaults to `/**/*log`.

#### Scraping other directories

Besides the predefined scrapers described above, you can also define your own additional ones with the help of these variables:

- `promtail_container_additional_mounts_custom`, to mount additional paths into the Promtail container
- `promtail_config_scrape_configs_custom`, to inject additional jobs into Promtail's `scrape_configs` configuration. See `promtail_journald_scraper_config` and `promtail_varlog_scraper_config` for an example

Here's an example for scraping some hypothethical SSH logs stored somewhere:

```yml
promtail_container_additional_mounts_custom:
  - "type=bind,source=</path/to/ssh/logs>,target=/data/ssh,readonly"

promtail_config_scrape_configs_custom:
  - job_name: ssh
    static_configs:
    - localhost
      __path__: /data/ssh
      labels:
        job: ssh
```

##### Scraping syslog

The following example demonstrates the use of rsyslog and promtail to scrape syslog logs.

**Prerequisites**: Edit your rsyslog configuration in order to send logs to `promtail.*``
This could be done by creating a `/etc/rsyslog.d/00-promtail-relay.conf` file with the following content:

```
*.* action(type="omfwd" protocol="tcp" target="<promtail_host>" port="<promtail_port>" Template="RSYSLOG_SyslogProtocol23Format" TCP_Framing="octet-counted" KeepAlive="on")
```

The port is a port number that you come up with yourself (e.g. `1234`).

First, you need a custom scrape configuration which tells Promtail to listen on this port (replace `SOME_PORT_NUMBER_IN_CONTAINER` with your port number of choice):

```yaml
promtail_config_scrape_configs_custom:
  - job_name: syslog
    syslog:
      listen_address: 0.0.0.0:SOME_PORT_NUMBER_IN_CONTAINER
      labels:
        job: syslog
    relabel_configs:
      - source_labels: [__syslog_message_hostname]
        target_label: host
      - source_labels: [__syslog_message_hostname]
        target_label: hostname
      - source_labels: [__syslog_message_severity]
        target_label: level
      - source_labels: [__syslog_message_app_name]
        target_label: application
      - source_labels: [__syslog_message_facility]
        target_label: facility
      - source_labels: [__syslog_connection_hostname]
        target_label: connection_hostname
```

You'd then need to expose this TCP port outside of the container, so that the local host (or remote host) can reach it.

To expose it on the loopback interface (reachable only from the same machine), use a configuration like this:
```yaml
promtail_container_extra_arguments_custom:
  - "-p 127.0.0.1:1234:1234"
```


### Configuring clients

If you've also enabled [Grafana Loki](./grafana-loki.md) on the same server, Promtail will automatically be configured to push logs to it.

Otherwise, you will need to extend the Promtail configuration by specifying clients to push to. Add something like this to your `vars.yml` configuration:

```yml
promtail_config_clients_custom:
  # Note the double /loki/loki.
  # This assumes Loki is installed at a `/loki` path-prefix.
  - url: https://mash.example.com/loki/loki/api/v1/push
    tenant_id: some-tenant-id-here
```

For more information about configuring clients, see the [Promtail `clients` configuration reference](https://grafana.com/docs/loki/latest/send-data/promtail/configuration/#clients).


### Exposing the web interface

There are 2 reasons to expose Promtail to the public web:

1. So that you can scrape its Prometheus-compatible `/metrics` endpoint or observe its current `/targets` via API
2. So that you can use [loki_push_api](https://grafana.com/docs/loki/latest/send-data/promtail/configuration/#loki_push_api) and push logs to Promtail (so that it can forward them onto its [clients](#configuring-clients)). This feature likely needs to be enabled explicitly.

To expose Promtail to the web, you need to assign a hostname in `promtail_hostname` and optionally a path-prefix.

You can then decide whether you'd like to expose Promtail's whole API via `promtail_container_labels_api_enabled` or just its metrics endpoint via `promtail_container_labels_metrics_enabled`.

Consult the `defaults/main.yml` file for variables related to these.

When exposing metrics, and especially the whole API, it's important to protected them. The Promtail Ansible role has variables that let you easily set up [HTTP Basic Authentication](https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication) via `promtail_container_labels_api_traefik_middleware_basic_auth_*` and `promtail_container_labels_metrics_traefik_middleware_basic_auth_*` variables.



## Recommended other services

- [Grafana Loki](grafana-loki.md) — a storage server for your logs compatible with Promtail
- [Grafana](grafana.md) — a web-based tool for visualizing your Promtail logs (stored in [Grafana Loki](grafana-loki.md) or elsewhere)
````

## File: docs/services/qbittorrent.md
````markdown
# qBittorrent

[qBittorrent](https://www.qbittorrent.org/) is a bittorrent client programmed in C++ / Qt that uses libtorrent.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# qbittorrent                                                          #
#                                                                      #
########################################################################

qbittorrent_enabled: true

qbittorrent_hostname: qbittorrent.example.com

# The path where downloaded files will be stored on the host system
qbittorrent_download_path: "{{ qbittorrent_base_path }}/downloads"

# The path at which qbittorrent_download_path is mounted to inside the container
qbittorrent_download_bind_path: "/downloads"

# The port qBittorrent is listening for torrents on inside the container
qbittorrent_container_torrenting_port: 6881

# Controls whether the container exposes its torrenting port
# To become an "active node" you'll want to set this and configure port-forwarding in your router
qbittorrent_container_torrenting_bind_port: "{{ qbittorrent_container_torrenting_port }}"

########################################################################
#                                                                      #
# /qbittorrent                                                         #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://qbittorrent.example.com`.

A `qbittorrent_path_prefix` variable can be adjusted to host under a subpath (e.g. `qbittorrent_path_prefix: /qbittorrent`), but this hasn't been tested yet.

## Usage

After [installation](../installing.md), you should access your new qBittorrent instance at the URL you've chosen.

To login you'll need to obtain the **temporary** randomly generated password for your instance, to get this run `just run-tags print-qbittorrent-password`

Once you've got that, login as the `admin` user with the password and change it under `Tools -> Options -> WebUI` in the `Authentication` section. Make sure you change the password, since the default one is temporary and will change with each start-up.

For additional configuration options, refer to [ansible-role-qbittorrent](https://github.com/mother-of-all-self-hosting/ansible-role-qbittorrent)'s `defaults/main.yml` file.

## Intergration with Sonarr/Radarr

To add qBittorrent to your [Sonarr](sonarr.md) or [Radarr](radarr.md) instance navigate to the form at `Settings > Download Clients > Add > qBittorrent`:

Set the `host` field to your qBittorrent URL (without the protocol) and `port` as 443. Make sure to click `Use SSL`. Set the `username` and `password` fields as your qBittorrent credentials. 

Fill in the rest of the form with your preferences, and you're done!

![Sonarr Add Download Client](../assets/sonarr/add-download-client.png)

## Recommended other services

Consider these other supported services that are also in the [*Arr stack](https://wiki.servarr.com/) of media automation tools:

- [Radarr](radarr.md)
- [Sonarr](sonarr.md)
- [Jackett](jackett.md)
- [Overseerr](overseerr.md)
````

## File: docs/services/radarr.md
````markdown
# Radarr

[Radarr](https://radarr.video/) is a movie organizer/manager for usenet and torrent users.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# radarr                                                               #
#                                                                      #
########################################################################

radarr_enabled: true

radarr_hostname: radarr.example.com

# To mount additional data directories, use `radarr_container_additional_volumes`
#
# Example:
# radarr_container_additional_volumes:
#   - type: bind
#     src: /path/on/the/host
#     dst: /data
#   - type: bind
#     src: /another-path/on/the/host
#     dst: /read-only
#     options: readonly

########################################################################
#                                                                      #
# /radarr                                                              #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://radarr.example.com`.

A `radarr_path_prefix` variable can be adjusted to host under a subpath (e.g. `radarr_path_prefix: /radarr`), but this hasn't been tested yet.

## Usage

After [installation](../installing.md), you should access your new Radarr instance at the URL you've chosen and configure a username and password. The recommended authenticaton method is `Forms (Login Page)`.

For additional configuration options, refer to [ansible-role-radarr](https://github.com/spatterIight/ansible-role-radarr)'s `defaults/main.yml` file.

## Recommended other services

Consider these other supported services that are also in the [*Arr stack](https://wiki.servarr.com/) of media automation tools:

- [Sonarr](sonarr.md)
- [Jackett](jackett.md)
  - For Jackett integration instructions, see the [setup guide](jackett.md#intergration-with-sonarrradarr)
- [qBittorrent](qbittorrent.md)
  - For qBittorrent integration instructions, see the [setup guide](qbittorrent.md#intergration-with-sonarrradarr)
- [Overseerr](overseerr.md)
````

## File: docs/services/radicale.md
````markdown
# Radicale

[Radicale](https://radicale.org/) is a Free and Open-Source CalDAV and CardDAV Server (solution for hosting contacts and calendars).


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# radicale                                                             #
#                                                                      #
########################################################################

radicale_enabled: true

radicale_hostname: mash.example.com
radicale_path_prefix: /radicale

radicale_credentials:
  - username: someone
    password: secret-password
  - username: another
    password: more-secret-password

########################################################################
#                                                                      #
# /radicale                                                            #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/radicale`.

You can remove the `radicale_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


## Usage

After installation, you can log in with your credentials (see the `radicale_credentials` configuration variable).

Creating new users requires changing the `radicale_credentials` variable and [re-running the playbook](../installing.md). You can rebuild only this service quickly by running: `just install-service radicale`.
````

## File: docs/services/readeck.md
````markdown
# Readeck

[Readeck](https://readeck.org) is a simple web application that lets you save the precious readable content of web pages you like and want to keep forever.
See it as a bookmark manager and a read later tool.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# Readeck                                                              #
#                                                                      #
########################################################################

readeck_enabled: true

readeck_hostname: readeck.example.com

########################################################################
#                                                                      #
# /Readeck                                                             #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://readeck.example.com/`.

## Usage

After installation, you can go to the Readeck URL, as defined in `readeck_hostname`, and create a user. The User Documentation is embedded in Readeck so it's easy to access and always up-to-date.
````

## File: docs/services/redis.md
````markdown
# Redis

[Redis](https://redis.io/) is an open source, in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker.

⚠️ We used to used to advocate for using Redis, but since [Redis is now "source available"](https://redis.com/blog/redis-adopts-dual-source-available-licensing/) we recommend that you use [Valkey](valkey.md) instead. Valkey is compatible with Redis, so switching should be straightforward. You can learn more about the switch from Redis to KeyDB in [this changelog entry](https://github.com/spantaleev/matrix-docker-ansible-deploy/blob/50813c600db1c47b1f3e76707b81fe05d6c46ef5/CHANGELOG.md#backward-compatibility-break-the-playbook-now-defaults-to-valkey-instead-of-redis) for [matrix-docker-ansible-deploy](https://github.com/spantaleev/matrix-docker-ansible-deploy). Since 2024-11-23, we recommend [Valkey](valkey.md) instead of [KeyDB](./keydb.md).

Some of the services installed by this playbook require a Redis data store.

> [!WARNING]
> Because Redis is not as flexible as [Postgres](postgres.md) when it comes to authentication and data separation, it's **recommended that you run separate Redis instances** (one for each service). Redis supports multiple database and a [SELECT](https://redis.io/commands/select/) command for switching between them. However, **reusing the same Redis instance is not good enough** because:

- if all services use the same Redis instance and database (id = 0), services may conflict with one another
- the number of databases is limited to [16 by default](https://github.com/redis/redis/blob/aa2403ca98f6a39b6acd8373f8de1a7ba75162d5/redis.conf#L376-L379), which may or may not be enough. With configuration changes, this is solveable.
- some services do not support switching the Redis database and always insist on using the default one (id = 0)
- Redis [does not support different authentication credentials for its different databases](https://stackoverflow.com/a/37262596), so each service can potentially read and modify other services' data

If you're only hosting a single service (like [PeerTube](peertube.md) or [NetBox](netbox.md)) on your server, you can get away with running a single instance. If you're hosting multiple services, you should prepare separate instances for each service.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process to **host a single instance of the Redis service**:

```yaml
########################################################################
#                                                                      #
# redis                                                                #
#                                                                      #
########################################################################

redis_enabled: true

########################################################################
#                                                                      #
# /redis                                                               #
#                                                                      #
########################################################################
```

To **host multiple instances of the Redis service**, follow the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation or the **Redis** section (if available) of the service you're installing.
````

## File: docs/services/redmine.md
````markdown
# Redmine

[Redmine](https://redmine.org/) is a flexible project management web application. Written using the Ruby on Rails framework, it is cross-platform and cross-database.

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# redmine                                                              #
#                                                                      #
########################################################################

redmine_enabled: true

redmine_hostname: redmine.example.com

# If you'll be installing Redmine plugins which pull Ruby gems,
# which need to compile native code, consider installing build tools in the contianer image,
# by uncommenting the line below.
# redmine_container_image_customizations_build_tools_installation_enabled: true

########################################################################
#                                                                      #
# /redmine                                                             #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://redmine.example.com`.


## Usage

After installation, you can register your administrator account

You can create additional users (admin-privileged or not) after that.
````

## File: docs/services/roundcube.md
````markdown
# Roundcube

[Roundcube](https://roundcube.net/) is a browser-based multilingual IMAP client with an application-like user interface. It provides full functionality you expect from an email client, including MIME support, address book, folder manipulation, message searching and spell checking.

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# roundcube                                                            #
#                                                                      #
########################################################################

roundcube_enabled: true

roundcube_hostname: mash.example.com

roundcube_path_prefix: "/roundcube"

# The default IMAP server to connect to.
roundcube_default_imap_host: "imap.example.com"
# If not specified, the default port is 143.
roundcube_default_imap_port: "143"

# The default SMTP server to use.
roundcube_smtp_server: "smtp.example.com"
# If not specified, the default port is 587.
roundcube_smtp_port: "587"

########################################################################
#                                                                      #
# /roundcube                                                           #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/roundcube`.

You can remove the `roundcube_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


## Usage

After installation, you should be able to access your new Roundcube instance at the configured URL (see above).

The username/password you will use to login are the same ones used in your IMAP server.
````

## File: docs/services/rumqttd.md
````markdown
# rumqttd

[rumqttd](https://github.com/bytebeamio/rumqtt) is a high performance, embeddable [MQTT](https://en.wikipedia.org/wiki/MQTT) broker installed via [mother-of-all-self-hosting/ansible-role-rumqttd](https://github.com/mother-of-all-self-hosting/ansible-role-rumqttd).


# Configuring this role for your playbook

## Dependencies

This service does not require any dependecies.

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# rumqttd                                                              #
#                                                                      #
########################################################################


rumqttd_enabled: true


########################################################################
#                                                                      #
# /rumqttd                                                             #
#                                                                      #
########################################################################
```


## Usage

You can then start to send and subscribe to MQTT topics. Use port 1883 and the servers IP or any domain you configured to point at this server.

## Alternatives

* [Mosquitto](mosquitto.md) is another, more feature-complete MQTT broker
````

## File: docs/services/searxng.md
````markdown
# SearXNG

[SearXNG](https://github.com/searxng/searxng/) is a privacy-respecting, hackable [metasearch engine](https://en.wikipedia.org/wiki/Metasearch_engine).

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

If rate-limiting is enabled, then it also requires:

- a [Valkey](valkey.md) database

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# searxng                                                              #
#                                                                      #
########################################################################

searxng_enabled: true

searxng_instance_name: My Example Instance Name'

searxng_hostname: searxng.example.com

# If you want to server SearXNG under a subpath, you can specify it here.
#searxng_path_prefix: '/'

# Generate the secret key with "openssl rand -hex 32".
searxng_secret_key: 'MY_SECRET_KEY'

########################################################################
#                                                                      #
# /searxng                                                             #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://searxng.example.com`.

It is possible to host SearXNG under a subpath (by configuring the `searxng_path_prefix` variable).

### Configuring rate-limiting

If you want to enable rate-limiting, you will also need to enable Valkey. As described on the [Valkey](valkey.md) documentation page, if you're hosting additional services which require Valkey on the same server, you'd better go for installing a separate Valkey instance for each service. See [Creating a Valkey instance dedicated to SearXNG](...).

You will also need to enable rate-limiting for SearXNG by setting:

```yaml
searxng_enable_rate_limiter: true
```

#### Creating a Valkey instance dedicated to SearXNG

The following instructions are based on the [Running multiple instances of the same service on the same host](running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts) documentation.

Adjust your `inventory/hosts` file as described in [Re-do your inventory to add supplementary hosts](../running-multiple-instances.md#re-do-your-inventory-to-add-supplementary-hosts), adding a new supplementary host (e.g. if `searxng.example.com` is your main one, create `searxng.example.com-deps`).

Then, create a new `vars.yml` file for the `inventory/host_vars/searxng.example.com-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-searxng-'
mash_playbook_service_base_directory_name_prefix: 'searxng-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

This will create a `mash-searxng-valkey` instance on this host with its data in `/mash/searxng-valkey`.

Then, adjust your main inventory host's variables file (`inventory/host_vars/searxng.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# searxng                                                              #
#                                                                      #
########################################################################

# Base configuration as shown above

# Point Searxng to its dedicated Valkey instance
searxng_rate_limiter_config_valkey_hostname: mash-searxng-valkey

# Make sure the Searxng service (mash-searxng.service) starts after its dedicated KeyDB service (mash-searxng-valkey.service)
searxng_systemd_required_services_list_custom:
  - "mash-searxng-valkey.service"

# Make sure the Searxng container is connected to the container network of its dedicated KeyDB service (mash-searxng-valkey)
searxng_container_additional_networks_custom:
  - "mash-searxng-valkey"

########################################################################
#                                                                      #
# /searxng                                                             #
#                                                                      #
########################################################################
```

### Configuring basic authentication

If you are running a private instance, you might want to protect it with so that only authorized people can use it. An easy option is to choose a non-trivial subpath by modifying the `searxng_path_prefix`. Another, more complete option is to enable basic authentication for the instance.

To do the latter, you need to set the following variables:

```yaml
searxng_basic_auth_enabled: true
searxng_basic_auth_username: 'my_username'
searxng_basic_auth_password: 'my_password'
```
````

## File: docs/services/semaphore.md
````markdown
# Semaphore

[Semaphore](https://www.ansible-semaphore.com/) is a responsive web UI for running Ansible playbooks. Installing it is powered by the [mother-of-all-self-hosting/ansible-role-semaphore](https://github.com/mother-of-all-self-hosting/ansible-role-semaphore) Ansible role.

## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# semaphore                                                            #
#                                                                      #
########################################################################

semaphore_enabled: true

semaphore_hostname: semaphore.example.com

# Despite the confusing naming, semaphore_admin_name needs to hold a username, not a name!
semaphore_admin_name: ''
semaphore_admin_email: ''
# You can generate a strong password with a command like: `pwgen -s 64 1`
semaphore_admin_password: ''

# Key for encrypting access keys in database.
# It must be generated by using the following command: head -c32 /dev/urandom | base64
semaphore_access_key_encryption: ''

########################################################################
#                                                                      #
# /semaphore                                                           #
#                                                                      #
########################################################################
```


## Usage

After [installing](../installing.md), you can log into you admin account by visiting the URL specified in `semaphore_hostname`.
````

## File: docs/services/soft-serve.md
````markdown
# Soft Serve

[Soft Serve](https://github.com/charmbracelet/soft-serve) is a tasty, self-hostable [Git](https://git-scm.com/) server for the command line.

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# soft-serve                                                           #
#                                                                      #
########################################################################

soft_serve_enabled: true

# The hostname of this system.
# It will be used for generating git clone URLs (e.g. ssh://mash.example.com/repository.git)
soft_serve_hostname: mash.example.com

# Expose Soft Serve's port. For git servers the usual git-over-ssh port is 22
soft_serve_container_bind_port: 2222

# This key will be able to authenticate with ANY user until you configure Soft Serve
soft_serve_initial_admin_key: YOUR PUBLIC SSH KEY HERE

########################################################################
#                                                                      #
# /soft-serve                                                          #
#                                                                      #
########################################################################
```

## Usage

After you've installed Soft Serve, you can `ssh your-user@mash.example.com -p 2222` with the SSH key defined in `soft_serve_initial_admin_key` to see its [TUI](https://en.wikipedia.org/wiki/Text-based_user_interface) and follow the instructions to configure Soft Serve further.

Note that you have to [finish the configuration yourself](https://github.com/charmbracelet/soft-serve#configuration), otherwise any user with `soft_serve_initial_admin_key` will work as an admin.
````

## File: docs/services/sonarr.md
````markdown
# Sonarr

[Sonarr](https://sonarr.tv/) is a smart PVR for newsgroup and bittorrent users.

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# sonarr                                                               #
#                                                                      #
########################################################################

sonarr_enabled: true

sonarr_hostname: sonarr.example.com

# To mount additional data directories, use `sonarr_container_additional_volumes`
#
# Example:
# sonarr_container_additional_volumes:
#   - type: bind
#     src: /path/on/the/host
#     dst: /data
#   - type: bind
#     src: /another-path/on/the/host
#     dst: /read-only
#     options: readonly

########################################################################
#                                                                      #
# /sonarr                                                              #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://sonarr.example.com`.

A `sonarr_path_prefix` variable can be adjusted to host under a subpath (e.g. `sonarr_path_prefix: /sonarr`), but this hasn't been tested yet.

## Usage

After [installation](../installing.md), you should access your new Sonarr instance at the URL you've chosen and configure a username and password. The recommended authenticaton method is `Forms (Login Page)`.

For additional configuration options, refer to [ansible-role-sonarr](https://github.com/spatterIight/ansible-role-sonarr)'s `defaults/main.yml` file.

## Recommended other services

Consider these other supported services that are also in the [*Arr stack](https://wiki.servarr.com/) of media automation tools:

- [Radarr](radarr.md)
- [Jackett](jackett.md)
  - For Jackett integration instructions, see the [setup guide](jackett.md#intergration-with-sonarrradarr)
- [qBittorrent](qbittorrent.md)
  - For qBittorrent integration instructions, see the [setup guide](qbittorrent.md#intergration-with-sonarrradarr)
- [Overseerr](overseerr.md)
````

## File: docs/services/stirling-pdf.md
````markdown
# Stirling PDF

Stirling PDF is an online PDF converter and editor with many functionalities. Visit the [official website](https://www.stirlingpdf.com) or [demo](https://stirlingpdf.io) to learn more.

## Dependencies

- a [Traefik](traefik.md) reverse-proxy server (optional)

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# stirling-pdf                                                         #
#                                                                      #
########################################################################

stirling_pdf_enabled: true

stirling_pdf_hostname: stirlingpdf.example.com

# The path at which stirling_pdf is served.
# This value must either be `/` or not end with a slash (e.g. `/pdf`).
stirling_pdf_path_prefix: /

# Set to true to download calibre onto stirling-pdf enabling pdf to/from book and advanced html conversion | default false
stirling_pdf_install_calibre: false

########################################################################
#                                                                      #
# /stirling-pdf                                                        #
#                                                                      #
########################################################################
```

### Optional Configuration

You can decide if you want to configure via environment variables or a configuration file. Environment variables outrank the configuration file.

To set addition environment variables use `stirling_pdf_environment_variables_extensions` in your `vars.yml` file.
To use the configuration file, use `stirling_pdf_extra_config` in your `vars.yml` file.

```yaml
stirling_pdf_extra_config: |
  	system:
    	defaultLocale: 'de-DE'

# OR

stirling_pdf_environment_variables_extensions: |
	SYSTEM_DEFAULTLOCALE=de-DE
```

Find all possible arguments in the [official documentation](https://docs.stirlingpdf.com/Advanced%20Configuration/How%20to%20add%20configurations).

All possible variables to configure the ansible-role can be found in its [defaults/main.yml](https://github.com/Bergruebe/ansible-role-stirling-pdf/blob/main/defaults/main.yml) file.
````

## File: docs/services/syncthing.md
````markdown
# Syncthing

[Syncthing](https://syncthing.net/) is a **continuous file synchronization** program which synchronizes files between two or more computers in real time, safely protected from prying eyes.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# syncthing                                                            #
#                                                                      #
########################################################################

syncthing_enabled: true

syncthing_hostname: mash.example.com
syncthing_path_prefix: /syncthing

# By default, the data directory is created at (`/mash/syncthing/data`), as defined below.
# If you'd like to put it elsewhere on the host, uncomment and edit the line below.
#
# Regardless of the location of the data directory on the host,
# it will be mounted into the Syncthing container at `/data`.
# syncthing_data_path: "{{ syncthing_base_path }}/data"

# To mount additional data directories, use `syncthing_container_additional_volumes`.

# Secure with HTTP Basic Auth (at the Traefik level)
syncthing_basicauth_enabled: true

# Syncthing is NOT a multi-user system.
# Whichever user you authenticate with later, you would get to the same shared system.
syncthing_basicauth_credentials:
  - username: someone
    password: secret-password
  - username: another
    password: more-secret-password

########################################################################
#                                                                      #
# /syncthing                                                           #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/syncthing`.

You can remove the `syncthing_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

### Authentication

You can log in with **any** of the Basic Auth credentials defined in `syncthing_basicauth_credentials`. Syncthing is **not a multi-user system**, so whichever user you authenticate with, you'd ultimately end up looking at the same shared system.

Authentication is **done at the reverse-proxy level** (Traefik), so upon logging in, Syncthing will show you scary warnings about **no GUI password being set**. You should ignore these warnings.

You can hide the warning permanently by going to **Actions** -> **Advanced** -> **GUI** section -> checking the **Insecure Admin Access** checkbox.

### Networking

By default, the following ports will be exposed by the container on **all network interfaces**:

- `22000` over **TCP**, controlled by `syncthing_container_sync_tcp_bind_port` and `syncthing_container_sync_tcp_port` — used for TCP based sync protocol traffic
- `22000` over **UDP**, controlled by `syncthing_container_sync_udp_bind_port` and `syncthing_container_sync_udp_port` — used for QUIC based sync protocol traffic
- `21027` over **UDP**, controlled by `syncthing_container_local_discovery_udp_bind_port` — used for discovery broadcasts on IPv4 and multicasts on IPv6

Docker automatically opens these ports in the server's firewall, so you **likely don't need to do anything**. If you use another firewall in front of the server, you may need to adjust it.

If you have multiple devices on the same LAN, you may wish to assign a unique port to each one as recommended in the [Local network setup section on ArchWiki](https://wiki.archlinux.org/title/Syncthing#Local_network_setup).

As the upstream [Firewall documentation](https://docs.syncthing.net/users/firewall.html) says:

> The external forwarded ports and the internal destination ports have to be the same (e.g. 22000/TCP).

Because of this, the Syncthing Ansible role makes the actually exposed ports (`syncthing_container_sync_*_bind_port` variables) the same as the ports that the Syncthing program in the container actually listens on (`syncthing_container_sync_tcp_port` or `syncthing_container_sync_udp_port`). That is to say, **the `_bind_port` variables are automatically adjusted** based on the values of `syncthing_container_sync_tcp_port` and `syncthing_container_sync_udp_port`.

However, changing `syncthing_container_sync_tcp_port` or `syncthing_container_sync_udp_port` in Ansible does not change the Syncthing configuration and the port Syncthing decides to listen, but merely tells the Ansible role which ports you'd like to use, so it can wire things correctly.

**To effectively change the Syncthing ports** being used:

1. Adjust `syncthing_container_sync_tcp_port` and `syncthing_container_sync_udp_port` in your `vars.yml`
2. Re-install the Syncthing service by re-running the Ansible playbook
3. Log in to the Syncthing Web UI (see [Usage](#usage))
4. Go to **Settings** -> **Connections** and put something like this in the **Sync Protocol Listen Addresses** configuration (inspired by the [Listen Addresses documentation](https://docs.syncthing.net/v1.27.0/users/config#listen-addresses)): `tcp://0.0.0.0:TCP_PORT_HERE, quic://0.0.0.0:UDP_PORT_HERE, dynamic+https://relays.syncthing.net/endpoint` (adjust `TCP_PORT_HERE` and `UDP_PORT_HERE` with the port numbers you've chosen for `syncthing_container_sync_tcp_port` and `syncthing_container_sync_udp_port`)


### Configuration & Data

The Syncthing configuration (stored in `syncthing_config_path` on the host) is mounted to the `/var/syncthing` directory in the container.
By default, Syncthing will create a default `Sync` directory underneath. We advise that you **don't use this** `Sync` directory and use the data directory (discussed below).

As mentioned above, the **data directory** (stored in `syncthing_data_path` on the host) is mounted to the `/data` directory in the container. We advise that you put data files underneath `/data` when you start using Syncthing.

If you'd like to **mount additional directories** into the container, look into the `syncthing_container_additional_volumes` variable part of the [`ansible-role-syncthing` role](https://github.com/mother-of-all-self-hosting/ansible-role-syncthing)'s [`defaults/main.yml` file](https://github.com/mother-of-all-self-hosting/ansible-role-syncthing/blob/main/defaults/main.yml).


## Usage

After installation, you can go to the Syncthing URL, as defined in `syncthing_hostname` and `syncthing_path_prefix`.

As mentioned in [Configuration & Data](#configuration--data) above, you should:

- get rid of the `Default Folder` directory that was automatically created in `/var/syncthing/Sync`
- change the default data directory, by going to **Actions** -> **Settings** -> **General** tab -> **Edit Folder Defaults** and changing **Folder Path** to `/data`

As mentioned in [Authentication](#authentication) above, you'd probably wish to permanently disable the "no GUI password set" security warnings as described there.
````

## File: docs/services/system.md
````markdown
# System-related configuration

This Ansible playbook can install and configure various system-related things for you.
All the sections below relate to the host OS instead of the managed containers.

### swap

To enable [swap](https://en.wikipedia.org/wiki/Memory_paging) management (also read more in the [Swap](https://wiki.archlinux.org/title/Swap) article in the [Arch Linux Wiki](https://wiki.archlinux.org/)), add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# system                                                               #
#                                                                      #
########################################################################

system_swap_enabled: true

########################################################################
#                                                                      #
# /system                                                              #
#                                                                      #
########################################################################
```

A swap file will be created in `/var/swap` (configured using the `system_swap_path` variable) and enabled in your `/etc/fstab` file.

By default, the swap file will have `1GB` size, but you can set the `system_swap_size` variable in megabytes, example (4gb):

```yaml
system_swap_size: 4096
```

> [!WARNING]
> Changing `system_swap_size` subsequently will not recreate the SWAP file with the new size. You will need to disable swap, re-run the playbook (to make it clean up), then enable it again with the new size.

### ssh

> [!WARNING]
> Advanced functionality! While the default config with a few adjustments was battle tested on hundreds of servers, you should use it with caution and verify everything before you apply the changes!

To enable [ssh server](https://www.openssh.com/) config and authorized/unauthorized keys management, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# system                                                               #
#                                                                      #
########################################################################

system_security_ssh_enabled: true

system_security_ssh_port: 22

system_security_ssh_authorizedkeys_host: [] # list of authorized public keys
system_security_ssh_unauthorizedkeys_host: [] # list of unauthorized/revoked public keys

########################################################################
#                                                                      #
# /system                                                              #
#                                                                      #
########################################################################
```

The [default configuration](https://gitlab.com/etke.cc/roles/ssh/-/blob/main/defaults/main.yml) is good enough as-is, but we strongly suggest you to **verify everything before applying any changes!**, otherwise you may lock yourself out of the server.

With this configuration, the default `/etc/ssh/sshd_config` file on your server will be replaced by a new one, managed by the [ssh role](https://gitlab.com/etke.cc/roles/ssh) (see its [templates/etc/ssh/sshd_config.j2](https://gitlab.com/etke.cc/roles/ssh/-/blob/main/templates/etc/ssh/sshd_config.j2) file).

There are various configuration options — check the defaults and adjust them to your needs.

### cleanup

Playbook may perform some housekeeping automatically, cleaning up unused docker resources, logs, even kernels (debian-only) and packages (debian-only). Here is how to enable different housekeeping tasks that will run on `setup-all`, `setup-cleanup`, `install-cleanup`:


```yaml
########################################################################
#                                                                      #
# system                                                               #
#                                                                      #
########################################################################

# runs `docker system prune -a -f --volumes` to remove unused images and containers
system_cleanup_docker: true

# configures a systemd unit (and timer) that runs `journalctl --vacuum-time=7d` daily, you can control schedules using system_cleanup_logs_* vars
system_cleanup_logs: true

# list of arbitrary absolute paths to remove on each invocation
system_cleanup_paths: []

# The following options are Debian only, will have no effect on any other distro family

# runs safe-upgrade, apt autoclean, aptautoremove, etc.
system_cleanup_apt: true

# WARNING: very dangerous! Purges old linux kernels, and their modules
system_cleanup_kernels: false

########################################################################
#                                                                      #
# /system                                                              #
#                                                                      #
########################################################################
```


### fail2ban

To enable [fail2ban](https://fail2ban.org/wiki/index.php/Main_Page) installation, management and integration with SSHd, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# system                                                               #
#                                                                      #
########################################################################

system_security_fail2ban_enabled: true

system_security_fail2ban_sshd_port: 22
# If you enabled playbook-managed ssh as described above,
# you can replace the line above with the following:
# system_security_fail2ban_sshd_port: "{{ system_security_ssh_port }}"

########################################################################
#                                                                      #
# /system                                                              #
#                                                                      #
########################################################################
```
````

## File: docs/services/tandoor.md
````markdown
# Tandoor

[Tandoor](https://docs.tandoor.dev/) is a self-hosted recipe manager, that this playbook can install, powered by the [ansible-role-tandoor](https://github.com/IUCCA/ansible-role-tandoor) Ansible role.


## Dependencies

This service requires the following other services:
- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# tandoor                                                              #
#                                                                      #
########################################################################

tandoor_enabled: true

tandoor_hostname: mash.example.com
tandoor_path_prefix: /tandoor

########################################################################
#                                                                      #
# /tandoor                                                             #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/tandoor`.

### Authentication

On first use (see [Usage](#usage) below), you'll be asked to create the first administrator user.

You can create additional users from the web UI after that.


## Usage

After installation, you can go to the Tandoor URL, as defined in `tandoor_hostname` and `tandoor_path_prefix`.

As mentioned in [Authentication](#authentication) above, you'll be asked to create the first administrator user the first time you open the web UI.
````

## File: docs/services/telegraf.md
````markdown
# Telegraf

[Telegraf](https://www.influxdata.com/) is an open source server agent to help you collect metrics from your stacks, sensors, and systems.

This playbook can install Telegraf, powered by the [mother-of-all-self-hosting/ansible-role-telegraf](https://github.com/mother-of-all-self-hosting/ansible-role-telegraf) Ansible role. It heavily depends on [InfluxDB](influxdb.md)

## Prerequisits

* A functioning [InfluxDB](influxdb.md) instance.

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

This role depends on InfluxDB. You need to obtain the influx token and config link in from InfluxDB.
In your browser, visit the InfluxDB instance and go to **Load Data** -> **Telegraf**.
There you need to add a Telegraf configuration. You can now obtain these values from the setup instructions and paste them here.

```yaml
telegraf_enabled: true
telegraf_influx_token: SUPERSECRETTOKEN
telegraf_config_link: https://influxdb.example.org/api/v2/telegrafs/0123456789
```

## Usage

In your InfluxDB instance, configure the Telegraf plugins as you like.
````

## File: docs/services/traefik.md
````markdown
# Traefik

[Traefik](https://doc.traefik.io/traefik/) is a container-aware reverse-proxy server.

Many of the services installed by this playbook need to be exposed to the web (HTTP/HTTPS). This is handled by Traefik, which is installed by default if you have used the [example `vars.yml` file](../../examples/vars.yml).

Enabling the Traefik service will automatically wire all other services to use it.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process.

### Traefik managed by the playbook

```yaml
########################################################################
#                                                                      #
# traefik                                                              #
#                                                                      #
########################################################################

mash_playbook_reverse_proxy_type: playbook-managed-traefik

########################################################################
#                                                                      #
# /traefik                                                             #
#                                                                      #
########################################################################
```

Enabling the Traefik service, as shown above, automatically installs a [tecnativa/docker-socket-proxy](https://github.com/Tecnativa/docker-socket-proxy) service/container (powered by the [com.devture.ansible.role.container_socket_proxy](https://github.com/devture/com.devture.ansible.role.container_socket_proxy) Ansible role) to improve security by not mounting a Docker socket into the Traefik container.

This [Ansible role we use for Traefik](https://github.com/mother-of-all-self-hosting/ansible-role-traefik) supports various configuration options. Feel free to consult [its `default/main.yml` variables file](https://github.com/mother-of-all-self-hosting/ansible-role-traefik/blob/main/defaults/main.yml).

Below, you can find some guidance about common tweaks you may wish to do.

### Traefik managed by you

Sometimes you may already have a Traefik instance running on the server and you may wish to not have the playbook install Traefik.

To tell the playbook that you're running a Traefik instance and you'd still like all services installed by the playbook to be connected to that Traefik instance, you need the following configuration:

```yml
# Tell the playbook you're using Traefik installed in another way.
# It won't bother installing Traefik.
mash_playbook_reverse_proxy_type: other-traefik-container

# Tell the playbook to attach services which require reverse-proxying to an additional network by default (e.g. traefik)
# This needs to match your Traefik network.
mash_playbook_reverse_proxyable_services_additional_network: traefik

# Uncomment and adjust the variables below if you'd like to enable HTTP-compression.
#
# For this to work, you will need to define a compress middleware (https://doc.traefik.io/traefik/middlewares/http/compress/) for your Traefik instance
# using a file (https://doc.traefik.io/traefik/providers/file/) or Docker (https://doc.traefik.io/traefik/providers/docker/) configuration provider.
#
# mash_playbook_reverse_proxy_traefik_middleware_compession_enabled: true
# mash_playbook_reverse_proxy_traefik_middleware_compession_name: my-compression-middleware@file
```

## Increase logging verbosity

```yaml
traefik_config_log_level: DEBUG
```

## Disable access logs

This will disable access logging.

```yaml
traefik_config_accessLog_enabled: false
```

## Enable Traefik Dashboard

This will enable a Traefik [Dashboard](https://doc.traefik.io/traefik/operations/dashboard/) UI at `https://traefik.mash.example.com/dashboard/` (note the trailing `/`).

```yaml
traefik_dashboard_enabled: true
traefik_dashboard_hostname: traefik.mash.example.com
traefik_dashboard_basicauth_enabled: true
traefik_dashboard_basicauth_user: YOUR_USERNAME_HERE
traefik_dashboard_basicauth_password: YOUR_PASSWORD_HERE
```

> [!WARNING]
> Enabling the dashboard on a hostname you use for something else (like `mash.example.com` in the configuration above) may cause conflicts. Enabling the Traefik Dashboard makes Traefik capture all `/dashboard` and `/api` requests and forward them to itself. If any of the services hosted on the same hostname requires any of these 2 URL prefixes, you will experience problems.

## Additional configuration

Use the `traefik_configuration_extension_yaml` variable provided by the Traefik Ansible role to override or inject additional settings, even when no dedicated variable exists.

```yaml
# This is a contrived example.
# You can enable and secure the Dashboard using dedicated variables. See above.
traefik_configuration_extension_yaml: |
  api:
    dashboard: true
```
````

## File: docs/services/tsdproxy.md
````markdown
# TSDProxy

[TSDProxy](https://almeidapaulopt.github.io/tsdproxy/) is an application that automatically creates a proxy to virtual addresses in your [Tailscale](https://tailscale.com/) network.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

It is mandatory to set the following variables:

```yaml
########################################################################
#                                                                      #
# tsdproxy                                                             #
#                                                                      #
########################################################################

tsdproxy_enabled: true

tsdproxy_tailscale_authkey: '' # OR
tsdproxy_tailscale_authkeyfile: '' # use this to load authkey from file. If this is defined, Authkey is ignored

########################################################################
#                                                                      #
# /tsdproxy                                                            #
#                                                                      #
########################################################################
```

If [com.devture.ansible.role.container_socket_proxy](https://github.com/devture/com.devture.ansible.role.container_socket_proxy) is installed by the playbook (default), the container will use the proxy.
If not, the container will mount the docker socket at `/var/run/docker.sock`, but you can change that by setting `tsdproxy_docker_socket` to something else. Don't forget to adjust the `tsdproxy_docker_endpoint_is_unix_socket` to false if you are using a tcp endpoint. 

## Usage

## Adding a new service

This proxy creates a separate Tailscale machine (node) in the Tailscale network for each service, without creating a sidecar container each time.

To add a new service, you have to make sure that the service and proxy are in a same container network. You can do this by adding the proxy to the network of the service or the other way round.

```yaml
tsdproxy_container_additional_networks_custom:
  - YOUR-SERVICE-NETWORK
# OR
YOUR-SERVICE_container_additional_networks_custom:
  - "{{ tsdproxy_container_network }}"
```

The next step is to add the service to the proxy.

### Connecting a service to the proxy via container labels

```yaml
YOUR-SERVICE_container_labels_additional_labels: |
  tsdproxy.enable: "true"
  tsdproxy.container_port: 8080
```

The following labels are optional, please read the [official TSDProxy documentation](https://almeidapaulopt.github.io/tsdproxy/docs/docker/) for more information.

```yaml	
  tsdproxy.name: "my-service"
  tsdproxy.autodetect: "false"
  tsdproxy.proxyprovider: "providername"
  tsdproxy.ephemeral: "false"
  tsdproxy.funnel: "false"
```

### Connecting a service to the proxy via a Proxy list

An alternative way to add a service to the proxy is to use Proxy files.

Please read the [official TSDProxy documentation](https://almeidapaulopt.github.io/tsdproxy/docs/files/) for more information.

You will need to use the `tsdproxy_config_files` variable and add your proxy list file into the config folder, most likely `/mash/tsdproxy/config/`.
This is possible manually or by using [AUX-Files](https://github.com/mother-of-all-self-hosting/mash-playbook/blob/main/docs/services/auxiliary.md).
````

## File: docs/services/uptime-kuma.md
````markdown
<!--
SPDX-FileCopyrightText: 2023 Slavi Pantaleev
SPDX-FileCopyrightText: 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Uptime Kuma

[Uptime Kuma](https://uptime.kuma.pet/) is a fancy self-hosted monitoring tool similar to [Uptime Robot](https://uptimerobot.com/). It has functions such as below:

- Monitoring uptime for HTTP(s) / TCP / HTTP(s) Keyword / HTTP(s) Json Query / Ping / DNS Record / Push / Steam Game Server / Docker Containers
- Fancy, Reactive, Fast UI/UX
- Notifications via Matrix, Telegram, Discord, Gotify, Slack, Pushover, Email (SMTP), and [90+ notification services](https://github.com/louislam/uptime-kuma/tree/master/src/components/notifications)
- 20-second intervals
- [Multi-Language](https://github.com/louislam/uptime-kuma/tree/master/src/lang)
- Multiple status pages
- Map status pages to specific domains
- Ping chart
- Certificate info
- Proxy support
- 2FA support

✨ Kuma (くま/熊) means bear 🐻 in Japanese.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# uptime-kuma                                                          #
#                                                                      #
########################################################################

uptime_kuma_enabled: true

uptime_kuma_hostname: mash.example.com

# For now, hosting Uptime Kuma under a path is not supported.
# See: https://github.com/louislam/uptime-kuma/issues/147
# uptime_kuma_path_prefix: /uptime-kuma

########################################################################
#                                                                      #
# /uptime-kuma                                                         #
#                                                                      #
########################################################################
```

## Usage

When you open the Uptime Kuma's web UI for the first time, it starts a setup wizard where you'll create your admin credentials. You can then add monitors for web services as many as you like.

If you have enabled a self-hosted [ntfy](ntfy.md) server, it is possible to set up the Uptime Kuma instance to have it send notifications to a ntfy's "topic" (channel) when the monitored web service is down, without relaying them through servers owned and controlled by third parties.
````

## File: docs/services/valkey.md
````markdown
# Valkey

[Valkey](https://valkey.io/) is a flexible distributed key-value datastore that is optimized for caching and other realtime workloads.

Some of the services installed by this playbook require a Valkey data store.

> [!WARNING]
> Because Valkey is not as flexible as [Postgres](postgres.md) when it comes to authentication and data separation, it's **recommended that you run separate Valkey instances** (one for each service). Valkey supports multiple database and a [SELECT](https://valkey.io/commands/select/) command for switching between them. However, **reusing the same Valkey instance is not good enough** because:

- if all services use the same Valkey instance and database (id = 0), services may conflict with one another
- the number of databases is limited to [16 by default](https://github.com/valkey-io/valkey/blob/33f42d7fb597ce28040f184ee57ed86d6f6ffbd8/valkey.conf#L396), which may or may not be enough. With configuration changes, this is solveable.
- some services do not support switching the KeyDB database and always insist on using the default one (id = 0)
- Valkey [does not support different authentication credentials for its different databases](https://stackoverflow.com/a/37262596), so each service can potentially read and modify other services' data

If you're only hosting a single service (like [PeerTube](peertube.md) or [NetBox](netbox.md)) on your server, you can get away with running a single instance. If you're hosting multiple services, you should prepare separate instances for each service.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process to **host a single instance of the KeyDB service**:

```yaml
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

To **host multiple instances of the Valkey service**, follow the [Running multiple instances of the same service on the same host](../running-multiple-instances.md) documentation or the **Valkey** section (if available) of the service you're installing.
````

## File: docs/services/vaultwarden.md
````markdown
# Vaultwarden

[Vaultwarden](https://github.com/dani-garcia/vaultwarden) (unofficial [Bitwarden](https://bitwarden.com/) compatible server) is a password manager server that you can use with the official **Bitwarden** apps and browser addons.


## Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server
- (optional) the [exim-relay](exim-relay.md) mailer


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# vaultwarden                                                          #
#                                                                      #
########################################################################

vaultwarden_enabled: true

vaultwarden_hostname: mash.example.com

# For additional security, we recommend hosting Vaultwarden at a subpath.
# See: https://github.com/dani-garcia/vaultwarden/wiki/Hardening-Guide#hiding-under-a-subdir
#
# Choose your own custom path below.
# When using a path prefix, Vaultwarden will be available at: https://VAULTWARDEN_DOMAIN/PATH_PREFIX
# while the homepage (/) shows a 404 HTTP error.
#
# If you'd like to host at the root (without a path prefix), remove this configuration line.
vaultwarden_path_prefix: /vaultwarden-secret-custom-prefix

# Configure a strong admin secret here (generated with `pwgen -s 64 1`, etc).
# You will need this for accessing the /admin section useful for creating your first user
# and for doing various maintenance tasks.
# In the future, you can also consider disabling the /admin section by removing this configuration line.
vaultwarden_config_admin_token: ''

# Require people to validate their email addresses. When enabled, SMTP settings (below) are required.
vaultwarden_config_signups_verify: true

# Example SMTP settings.
# If you keep `vaultwarden_config_signups_verify` enabled, you will need to specify them.
# There are more SMTP variables in `roles/custom/devture_vaultwarden/defaults/main.yml`, in case you need them.
# If you decide you won't set up SMTP, consider removing all these configuration lines below
# and removing `vaultwarden_config_signups_verify: true` above.
vaultwarden_config_smtp_from: vaultwarden@DOMAIN
vaultwarden_config_smtp_host: ''
vaultwarden_config_smtp_port: 587
vaultwarden_config_smtp_security: starttls
vaultwarden_config_smtp_username: ''
vaultwarden_config_smtp_password: ''

########################################################################
#                                                                      #
# /vaultwarden                                                         #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/vaultwarden-secret-custom-prefix`.

You can remove the `vaultwarden_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


## Usage

After installation, you should be able to access your new Vaultwarden instance at: `https://VAULTWARDEN_DOMAIN/PATH_PREFIX`, where:

- `VAULTWARDEN_DOMAIN` matches your domain, as specified in `vaultwarden_hostname` in your `vars.yml` file
- `PATH_PREFIX` matches your path prefix, as specified in `vaultwarden_path_prefix` in your `vars.yml` file

To set up your first user account, you should use the `/admin` page, available at `https://VAULTWARDEN_DOMAIN/PATH_PREFIX/admin` and accessible with an admin token, as specified in `vaultwarden_config_admin_token` in your `vars.yml` file.

If you hadn't enabled the `/admin` feature (by defining `vaultwarden_config_admin_token`), you would:

- **either** need to do so and [re-run the playbook](../installing.md) (you can do it quickly with `just install-service vaultwarden`)
- **or** to enable public registration (`vaultwarden_config_signups_enabled: true`) at least temporarily.
````

## File: docs/services/versatiles.md
````markdown
# Versatiles

![Versatiles Logo](../assets/versatiles/logo.png)

[Versatiles](https://versatiles.org) is a a free stack for generating and serving vector tiles based on [OpenStreetMap](https://openstreetmap.com) data.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# versatiles                                                           #
#                                                                      #
########################################################################

versatiles_enabled: true

versatiles_hostname: tiles.example.com

########################################################################
#                                                                      #
# /versatiles                                                          #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://tiles.example.com/`.


## Usage

After installation, you should be able to access your new Versatiles instance at: `https://tiles.example.com`.

![Map of Dresden](../assets/versatiles/map-example.jpeg)


To embed the map in a website Copy & Paste the following snippet and replace `tiles.example.com` with your `versatiles_hostname`.

```html
<!-- add MapLibre JavaScript and CSS -->
<script src="https://tiles.example.com/assets/maplibre-gl/maplibre-gl.js"></script>
<link href="https://tiles.example.com/assets/maplibre-gl/maplibre-gl.css" rel="stylesheet" />

<!-- add container for the map -->
<div id="map" style="width:100%;aspect-ratio:16/9"></div>

<!-- start map -->
<script>
  new maplibregl.Map({
    container: 'map',
    style: 'https://tiles.example.com/assets/styles/colorful.json'
  }).addControl(new maplibregl.NavigationControl());
</script>
```

For adjustments, check out the amazing [examples from maplibre](https://maplibre.org/maplibre-gl-js/docs/examples/).
````

## File: docs/services/wetty.md
````markdown
# Wetty

[Wetty](https://github.com/butlerx/wetty/tree/main) is an SSH terminal over HTTP/HTTPS, useful for when on a strict network which disallows outbound SSH traffic, or when only a browser can be used (like a managed chromebook).

## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# wetty                                                                #
#                                                                      #
########################################################################

wetty_enabled: true
wetty_hostname: mash.example.com
wetty_path_prefix: /wetty
wetty_ssh_host: example.com
wetty_ssh_port: 22

########################################################################
#                                                                      #
# /wetty                                                               #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/wetty` and connect to `example.com` on port `22`.

You can remove the `wetty_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.

## Usage

After installation, you should be able to access your new Wetty instance at: `https://WETTY_DOMAIN/PATH_PREFIX`, where:

- `WETTY_DOMAIN` matches your domain, as specified in `wetty_hostname` in your `vars.yml` file
- `PATH_PREFIX` matches your path prefix, as specified in `wetty_path_prefix` in your `vars.yml` file

Once connected, simply input the username and password to use. Keep in mind that Wetty only supports password authentication, so if the SSH daemon at `wetty_ssh_host` only allows pubkey authentication you will not be able to connect.
````

## File: docs/services/wg-easy.md
````markdown
# WireGuard Easy

[WireGuard Easy](https://github.com/wg-easy/wg-easy) is the easiest way to run [WireGuard](https://www.wireguard.com/) VPN + Web-based Admin UI.

Another more powerful alternative for a self-hosted WireGuard VPN server is [Firezone](firezone.md). WireGuard Easy is easier, lighter and more compatible with various ARM devices.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server
- a modern Linux kernel which supports WireGuard


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# wg-easy                                                              #
#                                                                      #
########################################################################

wg_easy_enabled: true

wg_easy_hostname: mash.example.com

wg_easy_path_prefix: /wg-easy

wg_easy_environment_variables_additional_variable_wg_host: mash.example.com

# Put a strong password below, generated with `pwgen -s 64 1` or in another way
wg_easy_environment_variables_additional_variable_password: ''

# The default WireGuard port is 51820.
# Uncomment and change the lines below to use another one.
#
# The port that wg-easy advertises for WireGuard connectivity in profile files.
# wg_easy_environment_variables_additional_variable_wg_port: 51820
#
# The port that is actually published from the container.
# wg_easy_container_wireguard_bind_port: 51820

# The default DNS is 1.1.1.1.
# Uncomment and change the line below to use another one.
# wg_easy_environment_variables_additional_variable_wg_default_dns: 1.1.1.1

########################################################################
#                                                                      #
# /wg-easy                                                             #
#                                                                      #
########################################################################
```

### URL

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/wg-easy`.

You can remove the `wg_easy_path_prefix` variable definition, to make it default to `/`, so that the service is served at `https://mash.example.com/`.


### Networking

**In addition** to ports `80` and `443` exposed by the [Traefik](traefik.md) reverse-proxy, the following ports will be exposed by the WireGuard containers on **all network interfaces**:

- `51820` over **UDP**, controlled by `wg_easy_environment_variables_additional_variable_wg_port` and `wg_easy_container_wireguard_bind_port` — used for [Wireguard](https://www.wireguard.com/) connections

Docker automatically opens these ports in the server's firewall, so you **likely don't need to do anything**. If you use another firewall in front of the server, you may need to adjust it.

### Adjusting the host's iptables configuration

If you're running `iptables`/`ip6tables` on the host with a custom config (which whitelists some traffic and denies everything else), you may find that WireGuard clients cannot reach certain ports on server where wg-easy runs via its LAN IP address (e.g. `192.168.1.50`).

You may wish to adjust your iptables configuration (typically `/etc/iptables/iptables.rules`) like this:

```iptables
# ... Additional configuration ...

# Allow all private IPv4 ranges (RFC1918 private addresses) to access us via SSH.
#
# This allows wg-easy WireGuard clients which try to speak to us via our LAN IP to be able to reach us.
# They "exit" through mash-wg-easy's container subnet (e.g. 172.18.0.1).
-A INPUT -m tcp -p tcp --dport 22 -s 10.0.0.0/8 -j ACCEPT
-A INPUT -m tcp -p tcp --dport 22 -s 172.16.0.0/12 -j ACCEPT
-A INPUT -m tcp -p tcp --dport 22 -s 192.168.0.0/16 -j ACCEPT

# Allow private IPv4 ranges to access us via HTTP.
# This is like the above private IPv4 range rules for SSH.
-A INPUT -m tcp -p tcp --dport 80 -s 10.0.0.0/8 -j ACCEPT
-A INPUT -m tcp -p tcp --dport 80 -s 172.16.0.0/12 -j ACCEPT
-A INPUT -m tcp -p tcp --dport 80 -s 192.168.0.0/16 -j ACCEPT

# ... Additional configuration ...
```

or for ip6tables (typically `/etc/iptables/ip6tables.rules`) like this:

```iptables
# ... Additional configuration ...

# Allow all private IPv6 ranges to access us via SSH.
#
# This allows wg-easy WireGuard clients which try to speak to us via our LAN IP to be able to reach us.
# They "exit" through mash-wg-easy's container subnet (e.g. 172.18.0.1).
# Unique Local Addresses (ULA)
-A INPUT -p tcp --dport 22 -s fc00::/7 -j ACCEPT
# Link-local addresses
-A INPUT -p tcp --dport 22 -s fe80::/10 -j ACCEPT

# Allow private IPv6 ranges to access us via HTTP.
# This is like the above private IPv6 range rules for SSH.
-A INPUT -m tcp -p tcp --dport 80 -s fc00::/7 -j ACCEPT
-A INPUT -m tcp -p tcp --dport 80 -s fe80::/10 -j ACCEPT

# ... Additional configuration ...
```

After doing so, you'll wish to restart `iptables`/`ip6tables`. Restarting iptables typically necessitates restarting Docker (`docker.service`) as well.

### Additional configuration

For additional configuration options, see the upstream documentation's [Options](https://github.com/WeeJeWel/wg-easy#options) section.

You can inject additional environment variables with this additional configuration:

```yaml
wg_easy_environment_variables_additional_variables: |
  WG_DEFAULT_ADDRESS: 10.6.0.x
  WG_MTU: 1420
```

## Usage

After installation, you can go to the WireGuard Easy URL, as defined in `wg_easy_hostname` and `wg_easy_path_prefix`.

You can authenticate with the password set in `wg_easy_environment_variables_additional_variable_password`.

You can then create various Clients and import the configuration for them onto your devices — either by downloading a file or by scanning a QR code.

## Recommended other services

- [AdGuard Home](adguard-home.md) — A network-wide DNS software for blocking ads & tracking
````

## File: docs/services/woodpecker-ci.md
````markdown
# Woodpecker CI

This playbook can install and configure [Woodpecker CI](https://woodpecker-ci.org/) for you.

Woodpecker CI is a [Continuous Integration](https://en.wikipedia.org/wiki/Continuous_integration) engine which can build and deploy your code automatically after pushing to a [Gitea](./gitea.md) or [Forgejo](./forgejo.md) repository.
If you are using [Forgejo](./forgejo.md), you might also be interested in [Forgejo Runner](https://code.forgejo.org/forgejo/runner) (that this playbook also [supports](forgejo-runner.md)).

A Woodpecker CI installation contains 2 components:

- one [Woodpecker CI **server**](#woodpecker-ci-server) (web interface, central management node)
- one or more [Woodpecker CI **agent**](#woodpecker-ci-agent) instances (which run your CI jobs)

It's better to run the **agent** instances elsewhere (not on the source-control server or a server serving anything of value) — on a machine that doesn't contain sensitive data.

Small installations which only run trusted CI jobs can afford to run an agent instance on the source-control server itself.

## Woodpecker CI Server

### Dependencies

This service requires the following other services:

- a [Postgres](postgres.md) database
- a [Traefik](traefik.md) reverse-proxy server

### Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# woodpecker-ci-server                                                 #
#                                                                      #
########################################################################

woodpecker_ci_server_enabled: true

woodpecker_ci_server_hostname: mash.example.com

woodpecker_ci_server_path_prefix: /ci

# Generate this secret with `openssl rand -hex 32`
#
# Note that this playbook only supports agent-specific secrets, which
# means that if you choose to share this secret with an agent, the
# server will register it as a non-persistent agent.
#
# See the definition of
# woodpecker_ci_agent_config_agent_secret below for more details.
woodpecker_ci_server_config_agent_secret: ''

woodpecker_ci_server_config_admins: [YOUR_USERNAME_HERE]

# Add one or more usernames that match your version control system (e.g. Gitea) below.
# These users will have admin privileges upon signup.
woodpecker_ci_server_config_admins:
  - YOUR_USERNAME_HERE
  - ANOTHER_USERNAME_HERE

# Uncomment the line below if you'll be running Woodpecker CI agents on remote machines.
# If you'll only run agents on the same machine as the server, you can keep gRPC expose disabled.
# woodpecker_ci_server_container_labels_traefik_grpc_enabled: true

########################################################################
#                                                                      #
# /woodpecker-ci-server                                                #
#                                                                      #
########################################################################
```

In the example configuration above, we configure the service to be hosted at `https://mash.example.com/ci`.

If you want to host the service at the root path, remove the `woodpecker_ci_server_path_prefix` variable override.

#### Gitea Integration

The Woodpecker CI server can integrate with [Gitea](gitea.md) using the following **additional** `vars.yml` configuration:

```yaml
woodpecker_ci_server_provider: gitea

# We must use the public URL here, because it's also used for login redirects
woodpecker_ci_server_config_gitea_url: "{{ gitea_config_root_url }}"

# Populate these with the OAuth 2 application information
# (see the Gitea configuration section above)
woodpecker_ci_server_config_gitea_client: GITEA_OAUTH_CLIENT_ID_HERE
woodpecker_ci_server_config_gitea_secret: GITEA_OAUTH_CLIENT_SECRET_HERE

woodpecker_ci_server_container_add_host_domain_name: "{{ gitea_hostname }}"
woodpecker_ci_server_container_add_host_ip_address: "{{ ansible_host }}"
```

#### Forgejo Integration

The Woodpecker CI server can integrate with [Forgejo](forgejo.md) using the following **additional** `vars.yml` configuration:

```yaml
woodpecker_ci_server_provider: forgejo

# We must use the public URL here, because it's also used for login redirects
woodpecker_ci_server_config_forgejo_url: "{{ forgejo_config_root_url }}"

# Populate these with the OAuth 2 application information
# (see the Forgejo configuration section above)
woodpecker_ci_server_config_forgejo_client: FORGEJO_OAUTH_CLIENT_ID_HERE
woodpecker_ci_server_config_forgejo_secret: FORGEJO_OAUTH_CLIENT_SECRET_HERE

woodpecker_ci_server_container_add_host_domain_name: "{{ forgejo_hostname }}"
woodpecker_ci_server_container_add_host_ip_address: "{{ ansible_host }}"
```

### Usage

After installation, you should be able to access the Woodpecker CI server instance at `https://mash.DOMAIN/ci` (matching the `woodpecker_ci_server_hostname` and `woodpecker_ci_server_path_prefix` values configured in `vars.yml`).

The **Log in** button should take you to Gitea, where you can authorize Woodpecker CI with the OAuth 2 application.

Follow the official Woodpecker CI [Getting started](https://woodpecker-ci.org/docs/usage/intro) documentation for additional usage details.


## Woodpecker CI Agent

As mentioned above, unless you completely trust your CI workloads, it's best to run the Woodpecker CI Agent on another machine.

### Dependencies

This service requires the following other services:

- a Woodpecker CI Server — installed via this playbook or otherwise

### Configuration

```yaml
########################################################################
#                                                                      #
# woodpecker-ci-agent                                                  #
#                                                                      #
########################################################################

woodpecker_ci_agent_enabled: true

# If the agent runs on the same machine as the server, enabling the agent
# is everything you need. The agent and server will be wired automatically.
#
# Otherwise, you'll need to configure the variables below:

# This needs to point to the server's gRPC host:port.
# If your Woodpecker CI Server is deployed using this playbook, its
# gRPC port will likely be 443. E.g., ci.example.com:443.
woodpecker_ci_agent_config_server: ''

# This playbook only supports agent-specific secrets, i.e., it is not recommended to use
# a shared secret between Woodpecker CI Server and all of its agents. Please refer to
# the following upstream documentation in order to learn how to register an agent and
# obtain a secret for it:
#
#   https://woodpecker-ci.org/docs/administration/agent-config#using-agent-token
#
# then, when you have the agent secret, uncomment the following line.
#woodpecker_ci_agent_config_agent_secret: ''

# Uncomment the line below if you want the agent to connect to the
# server over a secure gRPC channel (recommended).
#woodpecker_ci_agent_config_grpc_secure: true

# Uncomment the line below if you want the agent to verify the
# server's TLS certificate when connecting over a secure gRPC channel.
#woodpecker_ci_agent_config_grpc_verify: true

########################################################################
#                                                                      #
# /woodpecker-ci-agent                                                 #
#                                                                      #
########################################################################
```

### Usage

The agent should automatically register with the [Woodpecker CI server](#woodpecker-ci-server) and take jobs from it.
````

## File: docs/services/wordpress.md
````markdown
# Wordpress

[WordPress](https://wordpress.org/) is a widley used open source web content management system that this playbook can install, powered by the [mother-of-all-self-hosting/ansible-role-wordpress](https://github.com/mother-of-all-self-hosting/ansible-role-wordpress) Ansible role.

## Dependencies

This service requires the following other services:

- a [MariaDB](mariadb.md) database
- a [Traefik](traefik.md) reverse-proxy server

## Configuration

```yaml
########################################################################
#                                                                      #
# wordpress                                                            #
#                                                                      #
########################################################################

wordpress_enabled: true

wordpress_hostname: example.org

########################################################################
#                                                                      #
# /wordpress                                                           #
#                                                                      #
########################################################################
```

## Usage

Navigate to the domain you set as `wordpress_hostname`, select a language and create an admin user.

> **Make sure to create a user with a strong password**

You can now log in and fill your website with content!


## Advanced

### Basic authentication

If you don't want to have your website accessible to everyone (e.g. you first want to present it to a client) you can use

```yaml
wordpress_container_labels_middleware_basic_auth_enabled: true
# Use `htpasswd -nb USERNAME PASSSWORD` to generate the users below.
# See: https://doc.traefik.io/traefik/middlewares/http/basicauth/#users
wordpress_container_labels_middleware_basic_auth_users: ''
```

### Increase upload limit

By default we set the upload limit to `64M`. Increasing or decreasing the upload limit can be done by adding the following to your `vars.yml`

```yaml
wordpress_max_upload_size: '64M'
```
````

## File: docs/services/writefreely.md
````markdown
# WriteFreely

[WriteFreely](https://github.com/writefreely/writefreely) is a clean, minimalist publishing platform made for writers, federated via ActivityPub.


## Dependencies

This service requires the following other services:

- a [Traefik](traefik.md) reverse-proxy server

WriteFreely supports using a [MariaDB](./mariadb.md) database, but this Ansible role and playbook are not configured to make use of it.


## Configuration

To enable this service, add the following configuration to your `vars.yml` file and re-run the [installation](../installing.md) process:

```yaml
########################################################################
#                                                                      #
# writefreely                                                          #
#                                                                      #
########################################################################

writefreely_enabled: true

writefreely_hostname: writefreely.example.com

writefreely_instance_name: 'A Writefreely blog' # optional
writefreely_instance_description: 'My Writefreely blog' # optional

########################################################################
#                                                                      #
# /writefreely                                                         #
#                                                                      #
########################################################################
```

In the example above, we configure the service to be hosted at `writefreely.example.com`.

You can add the following variables to add an administrator user during the first setup process:

```yml
# You can use any username except "admin" (see below)
writefreely_env_admin_user: ''
writefreely_env_admin_password: ''
```

Alternatively you can add admins after [installation](../installing.md) with:

```sh
just run-tags writefreely-add-admin --extra-vars=username=<username> --extra-vars=password=<password>
```

**Note that the username `admin` is unavailable**, as `writefreely.example.com/admin` is already taken by the admin dashboard.

Additional user accounts can be added at any time once WriteFreely is running with:

```sh
just run-tags writefreely-add-user --extra-vars=username=<username> --extra-vars=password=<password>
```

Their respective blogs can then be accessed on `writefreely.example.com/<username>`.

## Settings

To customize your settings on first setup, you can adjust the `writefreely_env_*` default variables.
After installation, changes in environment variables will be ignored. But you can still change the settings at `writefreely.example.com/admin/settings` or by directly changing `/mash/writefreely/data/config.ini`.

## Maintenance

In case you need to run maintenance tasks as documented in [Admin commands](https://writefreely.org/docs/main/admin/commands), you can run the following commands on the server:

```sh
/usr/bin/docker exec mash-writefreely /writefreely/writefreely -c /data/config.ini [command]
```

For example, to delete an existing user, run:

```sh
/usr/bin/docker exec mash-writefreely /writefreely/writefreely -c /data/config.ini user delete [username]
```
````

## File: docs/alternative-architectures.md
````markdown
<!--
SPDX-FileCopyrightText: 2020 - 2023 Slavi Pantaleev
SPDX-FileCopyrightText: 2020 Horvath Gergely
SPDX-FileCopyrightText: 2024 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Alternative architectures

As stated in the [Prerequisites](prerequisites.md), currently only `amd64` (`x86_64`) is fully supported.

The playbook automatically determines the target server's architecture (the `mash_playbook_architecture` variable) to be one of the following:

- `amd64` (`x86_64`)
- `arm32`
- `arm64`

Some tools and container images can be built on the host or other measures can be used to install on that architecture.

## Implementation details

For `amd64`, prebuilt container images are used for all components.

For other architecture (`arm64`, `arm32`), components which have a prebuilt image make use of it. If the component is not available for the specific architecture, [self-building](self-building.md) will be used. Not all components support self-building though, so your mileage may vary.
````

## File: docs/ansible.md
````markdown
<!--
SPDX-FileCopyrightText: 2019 - 2025 Slavi Pantaleev
SPDX-FileCopyrightText: 2020 Hanno J. Gödecke
SPDX-FileCopyrightText: 2020 Aaron Raimist
SPDX-FileCopyrightText: 2022 Kai Biebel
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Using Ansible for the playbook

This playbook is meant to be run using [Ansible](https://www.ansible.com/).

Ansible typically runs on your local computer and carries out tasks on a remote server. If your local computer cannot run Ansible, you can also run Ansible on some server somewhere (including the server you wish to install to).

## Supported Ansible versions

To manually check which version of Ansible you're on, run: `ansible --version`.

For the **best experience**, we recommend getting the **latest version of Ansible available**.

We're not sure what's the minimum version of Ansible that can run this playbook successfully. The lowest version that we've confirmed (on 2022-11-26) to be working fine is: `ansible-core` (`2.11.7`) combined with `ansible` (`4.10.0`).

If your distro ships with an Ansible version older than this, you may run into issues. Consider [Upgrading Ansible](#upgrading-ansible) or [using Ansible via Docker](#using-ansible-via-docker).

## Upgrading Ansible

Depending on your distribution, you may be able to upgrade Ansible in a few different ways:

- by using an additional repository (PPA, etc.), which provides newer Ansible versions. See instructions for [CentOS](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#installing-ansible-on-rhel-centos-or-fedora), [Debian](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#installing-ansible-on-debian), or [Ubuntu](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#installing-ansible-on-ubuntu) on the Ansible website.

- by removing the Ansible package (`yum remove ansible` or `apt-get remove ansible`) and installing via [pip](https://pip.pypa.io/en/stable/installation/) (`pip install ansible`).

If using the `pip` method, do note that the `ansible-playbook` binary may not be on the `$PATH` (https://linuxconfig.org/linux-path-environment-variable), but in some more special location like `/usr/local/bin/ansible-playbook`. You may need to invoke it using the full path.

**Note**: Both of the above methods are a bad way to run system software such as Ansible. If you find yourself needing to resort to such hacks, please consider reporting a bug to your distribution and/or switching to a sane distribution, which provides up-to-date software.

## Using Ansible via Docker

Alternatively, you can run Ansible inside a Docker container (powered by the [ghcr.io/devture/ansible](https://github.com/devture/docker-ansible/pkgs/container/ansible) Docker image).

This ensures that:

- you're using a very recent Ansible version, which is less likely to be incompatible with the playbook
- you also get access to the [agru](https://github.com/etkecc/agru) tool for quicker Ansible role installation (when running `just roles`) compared to `ansible-galaxy`

You can either [run Ansible in a container on the server itself](#running-ansible-in-a-container-on-the-server-itself) or [run Ansible in a container on another computer (not the server)](#running-ansible-in-a-container-on-another-computer-not-the-server).

### Running Ansible in a container on the server itself

To run Ansible in a (Docker) container on the server itself, you need to have a working Docker installation. Docker is normally installed by the playbook, so this may be a bit of a chicken and egg problem. To solve it:

- you **either** need to install [Docker](services/ansible.md) manually first. Follow [the upstream instructions](https://docs.docker.com/engine/install/) for your distribution and consider setting `mash_playbook_docker_installation_enabled: false` in your `vars.yml` file, to prevent the playbook from installing Docker
- **or** you need to run the playbook in another way (e.g. [Running Ansible in a container on another computer (not the server)](#running-ansible-in-a-container-on-another-computer-not-the-server)) at least the first time around

Once you have a working Docker installation on the server, **clone the playbook** somewhere on the server and configure it as per usual (`inventory/hosts`, `inventory/host_vars/…`, etc.), as described in [configuring the playbook](configuring-playbook.md).

You would then need to add `ansible_connection=community.docker.nsenter` to the host line in `inventory/hosts`. This tells Ansible to connect to the "remote" machine by switching Linux namespaces with [nsenter](https://man7.org/linux/man-pages/man1/nsenter.1.html), instead of using SSH.

Alternatively, you can leave your `inventory/hosts` as is and specify the connection type in **each** `ansible-playbook` call you do later, like this: `just install-all --connection=community.docker.nsenter` (or `ansible-playbook --connection=community.docker.nsenter …`).

Run this from the playbook's directory:

```sh
docker run \
-it \
--rm \
--privileged \
--pid=host \
-w /work \
--mount type=bind,src=`pwd`,dst=/work \
--entrypoint=/bin/sh \
ghcr.io/devture/ansible:11.1.0-r0-0
```

Once you execute the above command, you'll be dropped into a `/work` directory inside a Docker container. The `/work` directory contains the playbook's code.

First, consider running `git config --global --add safe.directory /work` to [resolve directory ownership issues](#resolve-directory-ownership-issues).

Finally, you can execute `just` or `ansible-playbook …` (e.g. `ansible-playbook --connection=community.docker.nsenter …`) commands as per normal now.

### Running Ansible in a container on another computer (not the server)

Run this from the playbook's directory:

```sh
docker run \
-it \
--rm \
-w /work \
--mount type=bind,src=`pwd`,dst=/work \
--mount type=bind,src$HOME/.ssh/id_ed25519,dst=/root/.ssh/id_ed25519,ro \
--entrypoint=/bin/sh \
ghcr.io/devture/ansible:11.1.0-r0-0
```

The above command tries to mount an SSH key (`$HOME/.ssh/id_ed25519`) into the container (at `/root/.ssh/id_ed25519`). If your SSH key is at a different path (not in `$HOME/.ssh/id_ed25519`), adjust that part.

Once you execute the above command, you'll be dropped into a `/work` directory inside a Docker container. The `/work` directory contains the playbook's code.

First, consider running `git config --global --add safe.directory /work` to [resolve directory ownership issues](#resolve-directory-ownership-issues).

Finally, you execute `just` or `ansible-playbook …` commands as per normal now.

#### If you don't use SSH keys for authentication

If you don't use SSH keys for authentication, simply remove that whole line (`--mount type=bind,src$HOME/.ssh/id_ed25519,dst=/root/.ssh/id_ed25519,ro`).

To authenticate at your server using a password, you need to add a package. So, when you are in the shell of the ansible docker container (the previously used `docker run -it …` command), run:

```sh
apk add sshpass
```

Then, to be asked for the password whenever running an `ansible-playbook` command add `--ask-pass` to the arguments of the command.

#### Resolve directory ownership issues

Because you're `root` in the container running Ansible and this likely differs fom the owner (your regular user account) of the playbook directory outside of the container, certain playbook features which use `git` locally may report warnings such as:

> fatal: unsafe repository ('/work' is owned by someone else)
> To add an exception for this directory, call:
>  git config --global --add safe.directory /work

These errors can be resolved by making `git` trust the playbook directory by running `git config --global --add safe.directory /work`
````

## File: docs/configuring-dns.md
````markdown
<!--
SPDX-FileCopyrightText: 2018 - 2024 MDAD project contributors
SPDX-FileCopyrightText: 2023 Julian-Samuel Gebühr
SPDX-FileCopyrightText: 2023 Slavi Pantaleev
SPDX-FileCopyrightText: 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Configuring DNS settings

<sup>[Prerequisites](prerequisites.md) > Configuring DNS settings > [Getting the playbook](getting-the-playbook.md) > [Configuring the playbook](configuring-playbook.md) > [Installing](installing.md)</sup>

To reach your services, you'd need to do some DNS configuration.

**We recommend** that you:

- create at least one generic domain (e.g. `mash.example.com`) for easily hosting various services at different subpaths (e.g. `mash.example.com/miniflux`, `mash.example.com/radicale`, etc.). Even if you plan on hosting services at a dedicated subdomain, it's still convenient to have a generic domain and to use additional `CNAME` DNS records that point to it.

- create additional domains (`CNAME` DNS records pointing to the main generic domain) for large services or services that explicitly require their own dedicated domain

Some services (like [Uptime Kuma](services/uptime-kuma.md)) require being hosted at their own dedicated domain. Others, you can put on their own domain/subdomain or at subpaths on any domain you'd like.

## Example DNS settings

As an example setup, adjust DNS records as below.

| Service                    | Type  | Host    | Priority | Weight | Port | Target             |
|--------------------------- | ----- | ------- | -------- | ------ | ---- | -------------------|
| Miniflux, Radicale, others | A     | `mash`  | -        | -      | -    | `mash-server-IPv4` |
| Miniflux, Radicale, others | AAAA  | `mash`  | -        | -      | -    | `mash-server-IPv6` |
| Nextcloud                  | CNAME | `cloud` | -        | -      | -    | `mash.example.com` |

If you don't have IPv6 connectivity yet, you can skip the `AAAA` record. For more details about IPv6, see the [Configuring IPv6](./configuring-ipv6.md) documentation page.

With such a setup, you could reach:

- the feedreader [Miniflux](services/miniflux.md) at `https://mash.example.com/miniflux` (if you set `miniflux_hostname: mash.example.com` and `miniflux_path_prefix: /miniflux` in your `vars.yml`)

- the [Radicale](services/radicale.md) CalDAV/CardDAV sever at `https://mash.example.com/radicale` (if you set `radicale_hostname: mash.example.com` and `radicale_path_prefix: /radicale` in your `vars.yml`)

- Nextcloud at its own dedicated domain, at `https://cloud.example.com`

Hosting services at subpaths is more convenient, because it doesn't require you to create additional DNS records and no new SSL certificates need to be retrieved.

Still, if you'd like each service to have its own dedicated domain (or subdomain), feel free to configure services that way by making sure that you set `<service>_hostname` and `<service>_path_prefix` accordingly in your `vars.yml`.

Be mindful as to how long it will take for the DNS records to propagate.

**Note**: if you are using Cloudflare DNS, make sure to disable the proxy and set all records to "DNS only". Otherwise, fetching certificates will fail.

---------------------------------------------

[▶️](getting-the-playbook.md) When you're done with the DNS configuration and ready to proceed, continue with [Getting the playbook](getting-the-playbook.md).
````

## File: docs/configuring-ipv6.md
````markdown
<!--
SPDX-FileCopyrightText: 2025 Slavi Pantaleev

SPDX-License-Identifier: AGPL-3.0-or-later
-->
# Configuring IPv6

Since 2025-03-08, the [default example configuration](../examples/vars.yml) for the playbook recommends enabling [IPv6](https://en.wikipedia.org/wiki/IPv6) support for Docker's container networks.

**If you have IPv6 support on your server/network** (see [How do I check if my server has IPv6 connectivity?](#how-do-i-check-if-my-server-has-ipv6-connectivity)), then [enabling IPv6 support for the playbook](#enabling-ipv6-support-for-the-playbook) would give you:

- 📥 incoming IPv6 connectivity to the server via the server's IPv6 address/addresses (containers won't have their own individual publicly accessible IPs)
- 📤 outgoing IPv6 connectivity from the server via the server's IPv6 address/addresses (containers won't exit via their own individual IPv6 address)
- 🔄 IPv6 connectivity for cross-container communication

**If you still don't have IPv6 support on your server/network**, then enabling IPv6 support for the playbook will only enable IPv6 connectivity for cross-container communication and shouldn't affect your server's incoming/outgoing communication. You may also be interested in reading if [there's a performance penalty to enabling IPv6 if the server/network doesn't support IPv6 connectivity?](#is-there-a-performance-penalty-to-enabling-ipv6-if-the-server-network-doesn-t-support-ipv6-connectivity)

As such, **we recommend that you follow the default example configuration and leave IPv6 support for Docker enabled in all cases**.

Enabling IPv6 consists of 2 steps:

- [Enabling IPv6 support for the playbook](#enabling-ipv6-support-for-the-playbook)
- [Configuring DNS records for IPv6](#configuring-dns-records-for-ipv6)

💡 If you've followed a recent version of our documentation, you would have already done these steps, so there's nothing else to do.

> [!WARNING]
> Not all mash-playbook Ansible roles respect the `devture_systemd_docker_base_ipv6_enabled` setting yet.
> Even if you enable this setting, you may still see that some container networks and services aren't IPv6-enabled.
> **Consider sending pull requests** for the playbook roles that do not respect the `devture_systemd_docker_base_ipv6_enabled` seting yet.

## Enabling IPv6 support for the playbook

You can enable IPv6 support for all components' Docker container networks by using the following `vars.yml` configuration:

```yml
########################################################################
#                                                                      #
# Docker                                                               #
#                                                                      #
########################################################################

# Controls whether container networks will be created with IPv6 support.
#
# If you also have IPv6 support on your server/network and AAAA DNS records pointing to the server,
# enabling this will effectively give you full public IPv6 connectivity (powered by NAT66).
#
# We recommend leaving this enabled even if you don't currently have IPv6 connectivity on your server/network.
# This way, once you eventually get IPv6 connectivity, you won't have to change anything (besides DNS records).
#
# Flipping this setting later on requires manual work (stopping services, deleting and recreating all container networks).
#
# In the future, this setting will likely default to `true`, so if you really want IPv6 disabled, explicitly set this to `false`.
#
# People managing Docker themselves and running an older Docker version will need additional configuration.
#
# Learn more in `docs/configuring-ipv6.md`.
devture_systemd_docker_base_ipv6_enabled: true

########################################################################
#                                                                      #
# /Docker                                                              #
#                                                                      #
########################################################################
```

Doing this:

- all container networks will be IPv6-enabled. Note that not all mash-playbook Ansible roles respect the `devture_systemd_docker_base_ipv6_enabled` setting yet. See **WARNING** below for details.

- NAT66 will be used, so that:
  - containers will get [Unique Local Addresses (ULA)](https://en.wikipedia.org/wiki/Unique_local_address)
  - the outgoing IPv6 address for containers will be the same as the one on the server
  - traffic destined for the IPv6 address of the server will be forwarded to the containers that handle (and publish) that specific port

> [!WARNING]
> Without enabling this and assuming you have IPv6 `AAAA` DNS records pointing to the server (see [Configuring DNS records for IPv6](#configuring-dns-records-for-ipv6)), IPv6 traffic will still be handled, but NAT64 will be used instead of NAT66.
> As such, containers will only have an IPv4 address and all IPv6 traffic that reaches them will seem to originate from a local IP.

To confirm connectivity, see the following other resources:

- [How do I check if my server has IPv6 connectivity?](#how-do-i-check-if-my-server-has-ipv6-connectivity)
- [How do I check outgoing IPv6 connectivity for containers?](#how-do-i-check-outgoing-ipv6-connectivity-for-containers)
- [How do I check incoming IPv6 connectivity for containers?](#how-do-i-check-incoming-ipv6-connectivity-for-containers)
- [How do I confirm if my container networks are IPv6-enabled?](#how-do-i-confirm-if-my-container-networks-are-ipv6-enabled)

> [!WARNING]
> Not all mash-playbook Ansible roles respect the `devture_systemd_docker_base_ipv6_enabled` setting yet.
> Even if you enable this setting, you may still see that some container networks and services aren't IPv6-enabled.
> Even so, services should still be reachable over IPv6 (via NAT64), so they will not be able to see the client's actual IPv6 address.
> See the NAT64 warning above.

## Configuring DNS records for IPv6

[Enabling IPv6 support for the playbook](#enabling-ipv6-support-for-the-playbook) tells you how to prepare for IPv6 on the container (Docker) side.

For full public IPv6 connectivity (and not just IPv6 connectivity for containers inside the container networks) you also need to **ensure that your domain names** (e.g. `mash.example.com` and others) have IPv6 (`AAAA`) DNS records pointing to the server's IPv6 address.

Also see the [Configuring DNS settings](configuring-dns.md) documentation page for more details.

### A note about old Docker

With our [default example configuration](../examples/vars.yml), the playbook manages Docker for you and installs a modern-enough version.

Docker versions newer than 27.0.1 enable IPv6 integration at the Docker daemon level out of the box. This still requires that networks are created with IPv6 support as described in the [Enabling IPv6 support for the playbook](#enabling-ipv6-support-for-the-playbook) section above.

**If you're on an old Docker version** (Docker 27.0.0 or older) for some reason, it's likely that your Docker installation is not enabled for IPv6 at all. In such a case:

- if Docker is managed by the playbook, you can tell it to force-enable IPv6 via `devture_systemd_docker_base_ipv6_daemon_options_changing_enabled: true`

- if Docker is managed by you manually, you can add `{"experimental": true, "ip6tables": true}` to the Docker daemon options and restart the Docker service (`docker.service`).

### Frequently Asked Questions

#### How do I check if my server has IPv6 connectivity?

##### With curl

You can run `curl https://icanhazip.com` and see if it returns an [IPv6 address](https://en.wikipedia.org/wiki/IPv6_address) (an address with `:` characters in it, like `2001:db8:1234:5678::1`). If it does, then your server has IPv6 connectivity and prefers it over using IPv4. This is common.

If you see an IPv4 address instead (e.g. `1.2.3.4`), it may be that your server prefers IPv4 over IPv6 or that your network does not support IPv6. You can try forcing `curl` to use IPv6 by running `curl -6 https://icanhazip.com` and see if it returns an IPv6 address.

##### With other network utilities

You can run `ip -6 addr` to see if you have any IPv6 addresses assigned to your server, besides the link-local (`fe80::*`) addresses that everyone has (unless they have force-disabled IPv6 support on their system).

If you do have an IPv6 address, it's still worth [using curl](#with-curl) to confirm that your server can successfully make outgoing requests over IPv6.

#### What does the `devture_systemd_docker_base_ipv6_enabled` setting actually do?

The `devture_systemd_docker_base_ipv6_enabled` setting controls whether container networks will be created with IPv6 support.

Changing this setting subsequently requires manual work (deleting all container networks).
See [I've changed the `devture_systemd_docker_base_ipv6_enabled` setting, but it doesn't seem to have any effect](#i-ve-changed-the-devture_systemd_docker_base_ipv6_enabled-setting-but-it-doesn-t-seem-to-have-any-effect).

#### I've changed the `devture_systemd_docker_base_ipv6_enabled` setting, but it doesn't seem to have any effect.

If you're using an older Docker version (Docker 27.0.0 or older), see [A note about old Docker](#a-note-about-old-docker).

If you've previously installed with one `devture_systemd_docker_base_ipv6_enabled` value and then changed it to another, you need to:

- stop all services (`just stop-all`)
- delete all container networks on the server: `docker network rm $(docker network ls -q)`
- re-run the playbook fully: `just install-all`

#### How do I confirm if my container networks are IPv6-enabled?

You can list container networks by running `docker network ls` on the server.

For each container network (e.g. `traefik`), you can check if it has IPv6 connectivity by running a command like this: `docker network inspect traefik`.

Ensure that there's an IPv6 subnet/gateway in the `IPAM.Config` section. If yes, you may wish to proceed with [How do I check outgoing IPv6 connectivity for containers?](#how-do-i-check-outgoing-ipv6-connectivity-for-containers)

If there's no IPv6 subnet/gateway in the `IPAM.Config` section, this container network was not created with IPv6 support.
See [I've changed the `devture_systemd_docker_base_ipv6_enabled` setting, but it doesn't seem to have any effect](#i-ve-changed-the-devture_systemd_docker_base_ipv6_enabled-setting-but-it-doesn-t-seem-to-have-any-effect).

#### How do I check outgoing IPv6 connectivity for containers?

```sh
docker run --rm --network=traefik quay.io/curl/curl:latest curl -6 https://icanhazip.com
```

💡 This one-off container is connected to the `traefik` container network, not to the default Docker bridge network. The default Docker `bridge` network does not have IPv6 connectivity by default (yet) and is not influenced by the `devture_systemd_docker_base_ipv6_enabled` setting, so using that network (by omitting `--network=..` from the command above) will not show an IPv6 address

✅ If this command returns an IPv6 address, you're all good.

❌ If this command doesn't return an IPv6 address, it may be that:

- your container network does not have IPv6 connectivity. See [How do I confirm if my container networks are IPv6-enabled?](#how-do-i-confirm-if-my-container-networks-are-ipv6-enabled) for more details.

- your server does not have IPv6 connectivity. See [How do I check if my server has IPv6 connectivity?](#how-do-i-check-if-my-server-has-ipv6-connectivity) for more details. If you do have IPv6 connectivity, then the issue is with Docker's IPv6 configuration. Otherwise, you need to check your server's network configuration/firewall/routing and get back to configuring the playbook later on.

#### How do I check incoming IPv6 connectivity for containers?

Only containers that publish ports will be exposed (reachable) publicly on the server's own IPv6 address. Containers will not get their own individual public IPv6 address.

For this playbook, a commonly exposed container is the Traefik reverse-proxy container (unless [you're using your own webserver](./configuring-playbook-own-webserver.md)).

You can either do something like `curl -6 https://mash.example.com` from an IPv6-enabled host (including the server itself) and see if it works.

An alternative is to use the [IPv6 Port Checker](https://port.tools/port-checker-ipv6/) with a hostname of `mash.example.com` and a port of `443`.

💡 Trying to connect to `mash.example.com` via IPv6 requires that you have already [configured the DNS records for IPv6](#configuring-dns-records-for-ipv6) as described above. If you wish to eliminate DNS as a potential issue, you can also try connecting to the server's own IPv6 address directly: `curl -6 -H 'Host: mash.example.com' https://[2001:db8:1234:5678::1]` (we pass a `Host` header to tell Traefik which host we'd like it to serve).

#### Why enable IPv6 if my network doesn't support it yet?

Because when your network does get support for IPv6 later on (even if that's 5 years away), you won't have to change anything besides [configuring the DNS records for IPv6](#configuring-dns-records-for-ipv6).

#### Can I use a custom subnet for IPv6?

Not easily.

The playbook and the various roles only support passing an `enable_ipv6` flag (`true` or `false` value depending on the `devture_systemd_docker_base_ipv6_enabled` Ansible variable) when creating the Docker container networks.

There's no support for passing a custom subnet for IPv4 and IPv6. We let Docker auto-generate the subnets for us.

You can either create a Pull Request that adds support for this to the various playbook roles, or you can manually recreate the networks from the command-line (e.g. `docker network rm traefik && docker network create --ipv6 --subnet=2001:db8:1234:5678::/64 traefik`).

#### Can I use Global Unicast Addresses (GUA) for IPv6?

No. You cannot have GUA addresses where each container is individually addressable over the public internet.

The playbook only supports NAT66, which should be good enough for most use cases.

Having containers get IPv6 addresses from your own GUA subnet requires complex configuration (ndp-proxy, etc.) and is not supported.

You may find [this Reddit post](https://www.reddit.com/r/ipv6/comments/1alpzmb/comment/kphpw11/) interesting.

#### Is there a performance penalty to enabling IPv6 if the server/network doesn't support IPv6 connectivity?

Probably a tiny one, as services may try to make (unsuccessful) outgoing requests over IPv6.

In practice, it's probably negligible.

#### How do I add IPv6 support to an Ansible role that doesn't support it yet?

It's as simple as adjusting its container-network-creation Ansible task in its `tasks/install.yml` file to add the `enable_ipv6` flag like [this](https://github.com/mother-of-all-self-hosting/ansible-role-adguard-home/commit/b23a1f8671c147484e77b4821134766f5d3aab2d).

Feel free to submit Pull Requests.

Note that changing the `enable_ipv6` flag requires that the container network be recreated. See [I've changed the `devture_systemd_docker_base_ipv6_enabled` setting, but it doesn't seem to have any effect](#i-ve-changed-the-devture_systemd_docker_base_ipv6_enabled-setting-but-it-doesn-t-seem-to-have-any-effect).
````

## File: docs/configuring-playbook.md
````markdown
<!--
SPDX-FileCopyrightText: 2018 - 2025 Slavi Pantaleev
SPDX-FileCopyrightText: 2018 - 2024 MDAD project contributors
SPDX-FileCopyrightText: 2020 Sabine Laszakovits
SPDX-FileCopyrightText: 2021 Cody Neiman
SPDX-FileCopyrightText: 2021 Matthew Cengia
SPDX-FileCopyrightText: 2021 Toni Spets
SPDX-FileCopyrightText: 2022 Julian Foad
SPDX-FileCopyrightText: 2022 Vladimir Panteleev
SPDX-FileCopyrightText: 2022 - 2023 Julian-Samuel Gebühr
SPDX-FileCopyrightText: 2023 MASH project contributors
SPDX-FileCopyrightText: 2023 Shreyas Ajjarapu
SPDX-FileCopyrightText: 2023 Nikita Chernyi
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Configuring the playbook

<sup>[Prerequisites](prerequisites.md) > [Configuring DNS settings](configuring-dns.md) > [Getting the playbook](getting-the-playbook.md) > Configuring the playbook > [Installing](installing.md)</sup>

If you've configured your DNS records and retrieved the playbook's source code to your computer, you can start configuring the playbook. To do so, follow these steps inside the playbook directory:

1. create a directory to hold your configuration (`mkdir -p inventory/host_vars/mash.example.com`)

2. copy the sample configuration file (`cp examples/vars.yml inventory/host_vars/mash.example.com/vars.yml`)

3. edit the configuration file (`inventory/host_vars/mash.example.com/vars.yml`) to your liking. You should [enable one or more services](supported-services.md) in your `vars.yml` file. You may also take a look at the various `roles/*/ROLE_NAME_HERE/defaults/main.yml` files (after importing external roles with `just update` into `roles/galaxy`) and see if there's something you'd like to copy over and override in your `vars.yml` configuration file.

4. copy the sample inventory hosts file (`cp examples/hosts inventory/hosts`)

5. edit the inventory hosts file (`inventory/hosts`) to your liking

6. (optional, advanced) you may wish to keep your `inventory` directory under version control with [git](https://git-scm.com/) or any other version-control system. The `inventory` directory path is ignored via `.gitignore`, so it won't be part of the playbook repository. You can safely create a new git repository inside that directory with `git init`, etc.

For a basic installation, that's all you need.

If you're installing services on the same server using another playbook (like [matrix-docker-ansible-deploy](https://github.com/spantaleev/matrix-docker-ansible-deploy)) or you already have [Traefik](./services/traefik.md) or [Docker](./services/docker.md) installed on the server, consult our [Interoperability](./interoperability.md) documentation.

---------------------------------------------

[▶️](installing.md) When you're done with all the configuration you'd like to do, continue with [Installing](installing.md).
````

## File: docs/developer-documentation.md
````markdown
# Developer documentation

## Support a new service | Create your own role

### 1. Check if

- the role doesn't exist already in [`supported-services.md`](supported-services.md) and no one else is already [working on it](https://github.com/mother-of-all-self-hosting/mash-playbook/pulls)

- the service you wish to add can run in a Docker container

- a container image already exists

### 2. Create the Ansible role in a public git repository.
You can follow the structure of existing roles, like the [`ansible-role-gitea`](https://github.com/mother-of-all-self-hosting/ansible-role-gitea) or the [`ansible-role-gotosocial`](https://github.com/mother-of-all-self-hosting/ansible-role-gotosocial).

Some advice:
 - Your file structure will probably look something like this:

```
.
├── defaults/
│   └── main.yml
├── meta/
│   └── main.yml
├── tasks/
│   ├── main.yml
│   ├── install.yml
│   ├── uninstall.yml
│   └── validate_config.yml
├── templates/
│   ├── env.j2
│   ├── labels.j2
│   └── NEW-SERVICE.service.j2
├── .gitignore
├── LICENSE
└── README.md
```
- You will need to decide on a licence, without it, ansible-galaxy won't work. We recommend AGPLv3, like all of MASH.

### 3. Update the MASH-Playbook to support your created Ansible role

There are a few files that you need to adapt:
```
.
├── docs/
│   ├── supported-services.md  -> Add your service
│   └── services/
│       └── YOUR-SERVICE.md  -> document how to use it
├── templates/
│   ├── group_vars_mash_servers  -> Add default config
│   └── requirements.yml  -> add your Ansible role
│   └── setup.yml  -> add your Ansible role
```
<details>

<summary> file: templates / group_vars_mash_servers </summary>
In this file you wire your role with the rest of the playbook — integrating with the service manager or potentially with other roles.

```yaml
# role-specific:systemd_service_manager
########################################################################
#                                                                      #
# systemd_service_manager                                              #
#                                                                      #
########################################################################

mash_playbook_devture_systemd_service_manager_services_list_auto_itemized:
  [...]
  # role-specific:YOUR-SERVICE
  - |-
    {{ ({'name': (YOUR-SERVICE_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'YOUR-SERVICE']} if YOUR-SERVICE_enabled else omit) }}
  # /role-specific:YOUR-SERVICE

[...]
########################################################################
#                                                                      #
# /systemd_service_manager                                             #
#                                                                      #
########################################################################
# /role-specific:systemd_service_manager

```
</details>

<details>
<summary>Support Postgres</summary>

```yaml
# role-specific:postgres
########################################################################
#                                                                      #
# postgres                                                             #
#                                                                      #
########################################################################
[...]

mash_playbook_postgres_managed_databases_auto_itemized:
  [...]
  # role-specific:YOUR-SERVICE
  - |-
    {{
      ({
        'name': YOUR-SERVICE_database_name,
        'username': YOUR-SERVICE_database_username,
        'password': YOUR-SERVICE_database_password,
      } if YOUR-SERVICE_enabled else omit)
    }}
  # /role-specific:YOUR-SERVICE

  [...]
########################################################################
#                                                                      #
# /postgres                                                            #
#                                                                      #
########################################################################
# /role-specific:postgres

[...]

# role-specific:YOUR-SERVICE
########################################################################
#                                                                      #
# YOUR-SERVICE                                                         #
#                                                                      #
########################################################################

[...]

# role-specific:postgres
YOUR-SERVICE_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
YOUR-SERVICE_database_port: "{{ '5432' if postgres_enabled else '' }}"
YOUR-SERVICE_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.authentik', rounds=655555) | to_uuid }}"
YOUR-SERVICE_database_username: "{{ authentik_identifier }}"
# /role-specific:postgres

YOUR-SERVICE_container_additional_networks_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and YOUR-SERVICE_database_hostname == postgres_identifier else [])
  }}

########################################################################
#                                                                      #
# /YOUR-SERVICE                                                        #
#                                                                      #
########################################################################
# /role-specific:YOUR-SERVICE
```
</details><details>
<summary>Support exim-relay</summary>

The [exim-relay](https://github.com/devture/exim-relay) is an easy way to configure for all services a way for outgoing mail.
```yaml
[...]

# role-specific:YOUR-SERVICE
########################################################################
#                                                                      #
# YOUR-SERVICE                                                         #
#                                                                      #
########################################################################

[...]

YOUR-SERVICE_container_additional_networks_auto: |
  {{
	[...]
	+
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and YOUR-SERVICE_config_mailer_smtp_addr == exim_relay_identifier | default('mash-exim-relay') and YOUR-SERVICE_container_network != exim_relay_container_network) else [])
  }}

# role-specific:exim_relay
YOUR-SERVICE_config_mailer_enabled: "{{ 'true' if exim_relay_enabled else '' }}"
YOUR-SERVICE_config_mailer_smtp_addr: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
YOUR-SERVICE_config_mailer_smtp_port: 8025
YOUR-SERVICE_config_mailer_from: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
YOUR-SERVICE_config_mailer_protocol: "{{ 'smtp' if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /YOUR-SERVICE                                                        #
#                                                                      #
########################################################################
# /role-specific:YOUR-SERVICE
```
</details><details>

<summary>Support hubsite</summary>

- Add the logo of your Service to [`ansible-role-hubsite/assets`](https://github.com/mother-of-all-self-hosting/ansible-role-hubsite/tree/main/assets) via a pull request.
- configure the `group_vars_mash_servers` file:

```yaml
[...]
# role-specific:hubsite
########################################################################
#                                                                      #
# hubsite                                                              #
#                                                                      #
########################################################################
[...]

# Services
##########
[...]

# role-specific:YOUR-SERVICE
# YOUR-SERVICE
hubsite_service_YOUR-SERVICE_enabled: "{{ YOUR-SERVICE_enabled }}"
hubsite_service_YOUR-SERVICE_name: Adguard Home
hubsite_service_YOUR-SERVICE_url: "https://{{ YOUR-SERVICE_hostname }}{{ YOUR-SERVICE_path_prefix }}"
hubsite_service_YOUR-SERVICE_logo_location: "{{ role_path }}/assets/YOUR-SERVICE.png"
hubsite_service_YOUR-SERVICE_description: "YOUR-SERVICE Description"
hubsite_service_YOUR-SERVICE_priority: 1000
# /role-specific:YOUR-SERVICE
[...]

mash_playbook_hubsite_service_list_auto_itemized:
  [...]
  # role-specific:YOUR-SERVICE
  - |-
    {{
      ({
        'name': hubsite_service_YOUR-SERVICE_name,
        'url': hubsite_service_YOUR-SERVICE_url,
        'logo_location': hubsite_service_YOUR-SERVICE_logo_location,
        'description': hubsite_service_YOUR-SERVICE_description,
        'priority': hubsite_service_YOUR-SERVICE_priority,
      } if hubsite_service_YOUR-SERVICE_enabled else omit)
    }}
  # /role-specific:YOUR-SERVICE
[...]
```

</details>

### Additional hints

- Add a line like `# Project source code URL: YOUR-SERVICE-GIT-REPO` to your Ansible role's `defaults/main.yml` file, so that [`bin/feeds.py`](/bin/feeds.py) can automatically find the Atom/RSS feed for new releases.
````

## File: docs/getting-the-playbook.md
````markdown
<!--
SPDX-FileCopyrightText: 2019 - 2023 Slavi Pantaleev
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Getting the playbook

<sup>[Prerequisites](prerequisites.md) > [Configuring DNS settings](configuring-dns.md) > Getting the playbook > [Configuring the playbook](configuring-playbook.md) > [Installing](installing.md)</sup>

This Ansible playbook is meant to be executed on your own computer (not on the server).

In special cases (if your computer cannot run Ansible, etc.) you may put the playbook on the server as well.

You can retrieve the playbook's source code by:
- [Using git to get the playbook](#using-git-to-get-the-playbook) (recommended)
- [Downloading the playbook as a ZIP archive](#downloading-the-playbook-as-a-zip-archive) (not recommended)

## Using git to get the playbook

We recommend using the [git](https://git-scm.com/) tool to get the playbook's source code, because it lets you easily keep up to date in the future when [Maintaining services](maintenance-upgrading-services.md).

Once you've installed git on your computer, you can go to any directory of your choosing and run the following command to retrieve the playbook's source code:

```sh
git clone https://github.com/mother-of-all-self-hosting/mash-playbook.git
```

This will create a new `mash-playbook` directory. You're supposed to execute all other installation commands inside that directory.

## Downloading the playbook as a ZIP archive

Alternatively, you can download the playbook as a ZIP archive. This is not recommended, as it's not easy to keep up to date with future updates. We suggest you [use git](#using-git-to-get-the-playbook) instead.

The latest version is always at the following URL: https://github.com/mother-of-all-self-hosting/mash-playbook/archive/master.zip

You can extract this archive anywhere. You'll get a directory called `mash-playbook-master`. You're supposed to execute all other installation commands inside that directory.

---------------------------------------------

[▶️](configuring-playbook.md) No matter which method you've used to download the playbook, you can proceed by [Configuring the playbook](configuring-playbook.md).
````

## File: docs/installing.md
````markdown
<!--
SPDX-FileCopyrightText: 2018 - 2025 Slavi Pantaleev
SPDX-FileCopyrightText: 2018 Aaron Raimist
SPDX-FileCopyrightText: 2018 - 2024 MDAD project contributors
SPDX-FileCopyrightText: 2019 Edgars Voroboks
SPDX-FileCopyrightText: 2019 Michael Haak
SPDX-FileCopyrightText: 2020 Kevin Lanni
SPDX-FileCopyrightText: 2024 Nikita Chernyi
SPDX-FileCopyrightText: 2024 Mitja Jež
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Installing

<sup>[Prerequisites](prerequisites.md) > [Configuring DNS settings](configuring-dns.md) > [Getting the playbook](getting-the-playbook.md) > [Configuring the playbook](configuring-playbook.md) > Installing</sup>

If you've configured your DNS records and the playbook, you can start the installation procedure.

## Update Ansible roles

Before installing, you need to update the Ansible roles that this playbook uses and fetches from outside.

To update your playbook directory and all upstream Ansible roles (defined in the `requirements.yml` file), run:

- either: `just update`
- or: a combination of `git pull` and `just roles`

If you don't have either `just` tool, you can run the `ansible-galaxy` tool directly: `rm -rf roles/galaxy; ansible-galaxy install -r requirements.yml -p roles/galaxy/ --force`

For details about `just` commands, take a look at: [Running `just` commands](just.md).

**Note**: if you're not using the [`just`](https://github.com/casey/just) utility, you need to create `setup.yml`, `requirements.yml` and `group_vars/mash_servers` based on the up-to-date templates found in the [`templates/` directory](../templates). If you are using `just`, these files are created and maintained up-to-date automatically.

## Installing services

The Ansible playbook's tasks are tagged, so that certain parts of the Ansible playbook can be run without running all other tasks.

The general command syntax is:
- (**recommended**) when using `just`: `just run-tags COMMA_SEPARATED_TAGS_GO_HERE`
- when not using `just`: `ansible-playbook -i inventory/hosts setup.yml --tags=COMMA_SEPARATED_TAGS_GO_HERE`

It is recommended to get yourself familiar with the [playbook tags](playbook-tags.md) before proceeding.

If you **don't** use SSH keys for authentication, but rather a regular password, you may need to add `--ask-pass` to the all Ansible (or `just`) commands.

If you **do** use SSH keys for authentication, **and** use a non-root user to *become* root (sudo), you may need to add `-K` (`--ask-become-pass`) to all Ansible commands.

There 2 ways to start the installation process — depending on whether you're [Installing a brand new server (without importing data)](#installing-a-brand-new-server-without-importing-data) or [Installing a server into which you'll import old data](#installing-a-server-into-which-youll-import-old-data).

### Installing a brand new server (without importing data)

If this is **a brand new** server and you **won't be importing old data into it**, run all these tags:

```sh
# This is equivalent to: just run-tags install-all,start
just install-all

# Or, when not using just, you can use this instead:
# ansible-playbook -i inventory/hosts setup.yml --tags=install-all,start
```

This will do a full installation and start all services.

**Note**: if the command does not work as expected, make sure that you have properly installed and configured software required to run the playbook, as described on [Prerequisites](prerequisites.md).

### Installing a server into which you'll import old data

If you will be importing data into your newly created server, install it, but **do not** start its services just yet. Starting its services or messing with its database now will affect your data import later on.

To do the installation **without** starting services, run only the `install-all` tag:

```sh
just run-tags install-all

# Or, when not using just, you can use this instead:
# ansible-playbook -i inventory/hosts setup.yml --tags=install-all
```

> [!WARNING]
> Do not run the just "recipe" `just install-all` instead, because it automatically starts services at the end of execution. See: [Difference between playbook tags and shortcuts](just.md#difference-between-playbook-tags-and-shortcuts)

When this command completes, services won't be running yet.

You can now:

- [Importing an existing Postgres database (from another installation)](services/postgres.md#importing) (optional)

… and then proceed to starting all services:

```sh
# This is equivalent to: just run-tags start (or: just run-tags start-all)
just start-all

# Or, when not using just, you can use this instead:
# ansible-playbook -i inventory/hosts setup.yml --tags=start
```

Regardless of the installation way you have chosen, **if an error is not returned, the installation has completed and the services have been started successfully**🎉

## Things to do next

After you have started the services, you can:

- start using the configured services
- or set up additional services
- or learn how to [upgrade services when new versions are released](maintenance-upgrading-services.md)
- or come say Hi in our [Matrix](https://matrix.org) support room — [#mash-playbook:devture.com](https://matrix.to/#/#mash-playbook:devture.com). You might learn something or get to help someone else new to hosting services with this playbook.
- or help make this playbook better by contributing (code, documentation, or [coffee/beer](https://liberapay.com/mother-of-all-self-hosting/donate))

### ⚠️ Keep the playbook and services up-to-date

While this playbook helps you to set up services and maintain them, it will **not** automatically run the maintenance task for you. You will need to update the playbook and re-run it **manually**.

The upstream projects, which this playbook makes use of, occasionally if not often suffer from security vulnerabilities.

Since it is unsafe to keep outdated services running on the server connected to the internet, please consider to update the playbook and re-run it periodically, in order to keep the services up-to-date.

For more information about upgrading or maintaining services with the playbook, take a look at this page: [Upgrading services](maintenance-upgrading-services.md)

Feel free to **re-run the setup command any time** you think something is wrong with the server configuration. Ansible will take your configuration and update your server to match.

```sh
just install-all

# Or, to run potential uninstallation tasks too:
# just setup-all
```

To do it without `just`:

```sh
ansible-playbook -i inventory/hosts setup.yml --tags=install-all,start

# Or, to run potential uninstallation tasks too:
ansible-playbook -i inventory/hosts setup.yml --tags=setup-all,start
```

**Note**: see [this page on the playbook tags](playbook-tags.md) for more information about those tags.
````

## File: docs/interoperability.md
````markdown
<!--
SPDX-License-Identifier: 2023 - 2024 Slavi Pantaleev
SPDX-License-Identifier: 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Configuring interoperability with other services

*If you are installing the services on the [Matrix](https://matrix.org) server configured and managed with the [matrix-docker-ansible-deploy (MDAD)](https://github.com/spantaleev/matrix-docker-ansible-deploy/) Ansible playbook, you probably might want to check [this guide](setting-up-services-on-mdad-server.md).*

This playbook tries to get you up and running with minimal effort and provided you have followed the [example `vars.yml` file](../examples/vars.yml), will install the [Traefik](services/traefik.md) reverse-proxy server by default.

Sometimes, you're using a server which already has Traefik. In such cases these are undesirable:

- the playbook trying to run its own Traefik instance and running into a conflict with your other Traefik instance over ports (`tcp/80` and `tcp/443`)

- multiple playbooks trying to install Docker, etc.

Below, we offer some suggestions for how to make this playbook more interoperable. Feel free to cherry-pick the parts that make sense for your setup.


## Disabling Traefik installation

If you're installing [Traefik](services/traefik.md) on your server in another way, you can use your already installed Traefik instance by pointing MASH to your existing Traefik reverse-proxy (see the [Traefik managed by you](services/traefik.md#traefik-managed-by-you) guide).

If you are using the [matrix-docker-ansible-deploy](https://github.com/spantaleev/matrix-docker-ansible-deploy) playbook against the same server where you'd like MASH services installed, it already runs its own Traefik instance (`matrix-traefik`). In this case, we recommend following the same [Traefik managed by you](services/traefik.md#traefik-managed-by-you) guide, because `matrix-docker-ansible-deploy` installs Traefik the same way, but also injects additional configuration for handling the Matrix federation port (`8448` on a `matrix-federation` entrypoint) and internal communication between services (a `matrix-internal-matrix-client-api` entrypoint).


## Disabling Docker installation

If you're installing [Docker](https://www.docker.com/) on your server in another way, remove the variable `mash_playbook_docker_installation_enabled` from your `vars.yml`.

## Disabling timesyncing (systemd-timesyncd / ntp) installation

If you're installing `systemd-timesyncd` or `ntp` on your server in another way, disable this component from the playbook:

```yaml
devture_timesync_installation_enabled: false
```
````

## File: docs/just.md
````markdown
<!--
SPDX-FileCopyrightText: 2023 - 2024 Slavi Pantaleev
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Running `just` commands

This playbook supports running playbook commands via [`just`](https://github.com/casey/just) — a more modern command-runner alternative to `make`. It can be used to invoke `ansible-playbook` commands with less typing.

The `just` utility executes shortcut commands (called as "recipes"), which invoke `ansible-playbook`, [`ansible-galaxy`](https://docs.ansible.com/ansible/latest/cli/ansible-galaxy.html) or [`agru`](https://github.com/etkecc/agru) (depending on what is available in your system). The targets of the recipes are defined in [`justfile`](../justfile).

For some recipes such as `just update`, our `justfile` recommends installing `agru` (a faster alternative to `ansible-galaxy`) to speed up the process.

Here are some examples of shortcuts:

| Shortcut                                       | Result                                                                                                         |
|------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| `just roles`                                   | Install the necessary Ansible roles pinned in [`requirements.yml`](../templates/requirements.yml)              |
| `just update`                                  | Run `git pull` (to update the playbook) and install the Ansible roles                                          |
| `just install-all`                             | Run `ansible-playbook -i inventory/hosts setup.yml --tags=install-all,start`                                   |
| `just setup-all`                               | Run `ansible-playbook -i inventory/hosts setup.yml --tags=setup-all,start`                                     |
| `just install-all --ask-vault-pass`            | Run commands with additional arguments (`--ask-vault-pass` will be appended to the above installation command) |
| `just run-tags install-miniflux,start`         | Run specific playbook tags (here `install-miniflux` and `start`)                                               |
| `just install-service miniflux`                | Run `just run-tags install-miniflux,start` with even less typing                                               |
| `just start-all`                               | (Re-)starts all services                                                                                       |
| `just stop-group postgres`                     | Stop only the Postgres service                                                                                 |

While [our documentation on prerequisites](prerequisites.md) lists `just` as one of the requirements for installation, using `just` is optional. If you find it difficult to install it, do not find it useful, or want to prefer raw `ansible-playbook` commands for some reason, feel free to run all commands manually. For example, you can run `ansible-galaxy` directly to install the Ansible roles: `rm -rf roles/galaxy; ansible-galaxy install -r requirements.yml -p roles/galaxy/ --force`.

## Difference between playbook tags and shortcuts

It is worth noting that `just` "recipes" are different from [playbook tags](playbook-tags.md). The recipes are shortcuts of commands defined in `justfile` and can be executed by the `just` program only, while the playbook tags are available for the raw `ansible-playbook` commands as well. Please be careful not to confuse them.

For example, these two commands are different:
- `just install-all`
- `ansible-playbook -i inventory/hosts setup.yml --tags=install-all`

The just recipe runs `start` tag after `install-all`, while the latter runs only `install-all` tag. The correct shortcut of the latter is `just run-tags install-all`.

Such kind of difference sometimes matters. For example, when you install a server into which you will import old data (see [here](installing.md#installing-a-server-into-which-youll-import-old-data)), you are not supposed to run `just install-all` or `just setup-all`, because these commands start services immediately after installing components, which may prevent you from importing the data.
````

## File: docs/maintenance-and-troubleshooting.md
````markdown
# Maintenance and Troubleshooting

## How to see the current status of your services

You can check the status of your services by using `systemctl status`. Example:
```
sudo systemctl status mash-miniflux

● mash-miniflux.service - Miniflux (mash-miniflux)
   Loaded: loaded (/etc/systemd/system/mash-miniflux.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2023-03-14 17:41:59 EET; 15h ago
```

You can see the logs by using journalctl. Example:
```
sudo journalctl -fu mash-miniflux
```


## Increasing logging

Various Ansible roles for various services supported by this playbook support a `*_log_level` variable or some debug mode which you can enable in your configuration and get extended logs.

[Re-run the playbook](installing.md) after making such  configuration changes.


## Remove unused Docker data

You can free some disk space from Docker by running `docker system prune -a` on the server.


## Postgres

See the dedicated [PostgreSQL Maintenance](services/postgres.md#maintenance) documentation page.
````

## File: docs/maintenance-upgrading-services.md
````markdown
<!--
SPDX-FileCopyrightText: 2018 - 2023 Slavi Pantaleev
SPDX-FileCopyrightText: 2018 Aaron Raimist
SPDX-FileCopyrightText: 2024 MDAD project contributors
SPDX-FileCopyrightText: 2024 Nikita Chernyi
SPDX-FileCopyrightText: 2024 Felix Stupp
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Upgrading services

This playbook not only installs various services for you, but can also upgrade them as new versions are made available.

While this playbook helps you to set up services and maintain them, it will **not** automatically run the maintenance task for you. You will need to update the playbook and re-run it **manually**.

The upstream projects, which this playbook makes use of, occasionally if not often suffer from security vulnerabilities.

Since it is unsafe to keep outdated services running on the server connected to the internet, please consider to update the playbook and re-run it periodically, in order to keep the services up-to-date.

The developers of this playbook strive to maintain the playbook updated, so that you can re-run the playbook to address such vulnerabilities. It is **your responsibility** to keep your server and the services on it up-to-date.

## Steps to upgrade the services

### Check the changelog

Before updating the playbook and the Ansible roles in the playbook, take a look at [the changelog](../CHANGELOG.md) to see if there have been any backward-incompatible changes that you need to take care of.

### Update the playbook and the Ansible roles

If it looks good to you, go to the `mash-playbook` directory, update your playbook directory and all upstream Ansible roles (defined in the `requirements.yml` file) by running:

- either: `just update`
- or: a combination of `git pull` and `just roles`

If you don't have either `just` tool, you can run the `ansible-galaxy` tool directly: `rm -rf roles/galaxy; ansible-galaxy install -r requirements.yml -p roles/galaxy/ --force`

**Note**: for details about `just` commands, take a look at: [Running `just` commands](just.md).

### Re-run the playbook setup

After updating the Ansible roles, then re-run the [playbook setup](installing.md#2-maintaining-your-setup-in-the-future) and restart all services:

```sh
just install-all
```

If you remove components from `vars.yml`, or if we switch some component from being installed by default to not being installed by default anymore, you'd need to run the setup command with the `setup-all` shortcut as below:

```sh
just setup-all
```

## PostgreSQL major version upgrade

Major version upgrades to the internal PostgreSQL database are not done automatically. Upgrades must be performed manually.

For details about upgrading it, refer to the [upgrading PostgreSQL guide](services/postgres.md#upgrading-postgresql).
````

## File: docs/playbook-tags.md
````markdown
<!--
SPDX-FileCopyrightText: 2018 - 2023 Slavi Pantaleev
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Playbook tags

The Ansible playbook's tasks are tagged, so that certain parts of the Ansible playbook can be run without running all other tasks.

The general command syntax is:
- (**recommended**) when using `just`: `just run-tags COMMA_SEPARATED_TAGS_GO_HERE`
- when not using `just`: `ansible-playbook -i inventory/hosts setup.yml --tags=COMMA_SEPARATED_TAGS_GO_HERE`

Here are some playbook tags that you should be familiar with:

- `setup-all` — runs all setup tasks (installation and uninstallation) for all components, but does not start/restart services

- `install-all` — like `setup-all`, but skips uninstallation tasks. Useful for maintaining your setup quickly when its components remain unchanged. If you adjust your `vars.yml` to remove components, you'd need to run `setup-all` though, or these components will still remain installed

- `setup-SERVICE` (e.g. `setup-miniflux`) — runs the setup tasks only for a given role, but does not start/restart services. You can discover these additional tags in each role (`roles/**/tasks/main.yml`). Running per-component setup tasks is **not recommended**, as components sometimes depend on each other and running just the setup tasks for a given component may not be enough. For example, setting up the [Miniflux](services/miniflux.md) service, in addition to the `setup-miniflux` tag, requires database changes (the `setup-postgres` tag) as well.

- `install-SERVICE` (e.g. `install-miniflux`) — like `setup-SERVICE`, but skips uninstallation tasks. See `install-all` above for additional information.

- `start` — starts all systemd services and makes them start automatically in the future

- `stop` — stops all systemd services

**Notes**:
- `setup-*` tags and `install-*` tags **do not start services** automatically, because you may wish to do things before starting services, such as importing a database dump, restoring data from another server, etc.
- Please be careful not to confuse the playbook tags with the `just` shortcut commands ("recipes"). For details about `just` commands, see: [Running `just` commands](just.md)
````

## File: docs/prerequisites.md
````markdown
<!--
SPDX-FileCopyrightText: 2018 - 2025 Slavi Pantaleev
SPDX-FileCopyrightText: 2019 - 2022 Aaron Raimist
SPDX-FileCopyrightText: 2019 - 2023 MDAD project contributors
SPDX-FileCopyrightText: 2023 QEDeD
SPDX-FileCopyrightText: 2024 Nikita Chernyi
SPDX-FileCopyrightText: 2024 Fabio Bonelli
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Prerequisites

<sup>Prerequisites > [Configuring DNS settings](configuring-dns.md) > [Getting the playbook](getting-the-playbook.md) > [Configuring the playbook](configuring-playbook.md) > [Installing](installing.md)</sup>

To install services using this Ansible playbook, you need to prepare several requirements both on your local computer (where you will run the playbook to configure the server) and the server (where the playbook will install the services for you). **These requirements need to be set up manually** before proceeding to the next step.

We will be using `example.com` as the domain in the following instruction. Please remember to replace it with your own domain before running any commands.

## Your local computer

- [Ansible](http://ansible.com/) program. It's used to run this playbook and configures your server for you. Take a look at [our guide about Ansible](ansible.md) for more information, as well as [version requirements](ansible.md#supported-ansible-versions) and alternative ways to run Ansible.

- [passlib](https://passlib.readthedocs.io/en/stable/index.html) Python library. See [this official documentation](https://passlib.readthedocs.io/en/stable/install.html#installation-instructions) for an instruction to install it. On most distros, you need to install some `python-passlib` or `py3-passlib` package, etc.

- [`git`](https://git-scm.com/) as the recommended way to download the playbook. `git` may also be required on the server if you will be [self-building](self-building.md) components.

- [`just`](https://github.com/casey/just) for running `just roles`, `just update`, etc. (see [`justfile`](../justfile)), although you can also run these commands manually. Take a look at this documentation for more information: [Running `just` commands](just.md).

- Strong password (random strings) generator. The playbook often requires you to create a strong password and use it for settings on `vars.yml`, components, etc. As any tools should be fine, this playbook has adopted [`pwgen`](https://linux.die.net/man/1/pwgen) (running `pwgen -s 64 1`). [Password Tech](https://pwgen-win.sourceforge.io/), formerly known as "PWGen for Windows", is available as free and open source password generator for Windows. Generally, using a random generator available on the internet is not recommended.

## Server

- (Recommended) An **x86** server running one of these operating systems that make use of [systemd](https://systemd.io/):
  - **Archlinux**
  - **CentOS**, **Rocky Linux**, **AlmaLinux**, or possibly other RHEL alternatives (although your mileage may vary)
  - **Debian** (10/Buster or newer)
  - **Ubuntu** (18.04 or newer, although [20.04 may be problematic](ansible.md#supported-ansible-versions) if you run the Ansible playbook on it)

  Generally, newer is better. We only strive to support released stable versions of distributions, not betas or pre-releases. The playbook can take over your whole server or co-exist with other services that you have there.

  This playbook somewhat supports running on non-`amd64` architectures like ARM. See [Alternative Architectures](alternative-architectures.md).

  If your distro runs within an [LXC container](https://linuxcontainers.org/), you may hit [this issue](https://github.com/spantaleev/matrix-docker-ansible-deploy/issues/703). It can be worked around, if absolutely necessary, but we suggest that you avoid running from within an LXC container.

- `root` access to your server (or a user capable of elevating to `root` via `sudo`).

- [Python](https://www.python.org/). Most distributions install Python by default, but some don't (e.g. Ubuntu 18.04) and require manual installation (something like `apt-get install python3`). On some distros, Ansible may incorrectly [detect the Python version](https://docs.ansible.com/ansible/latest/reference_appendices/interpreter_discovery.html) (2 vs 3) and you may need to explicitly specify the interpreter path in `inventory/hosts` during installation (e.g. `ansible_python_interpreter=/usr/bin/python3`)

- [sudo](https://www.sudo.ws/), even when you've configured Ansible to log in as `root`. Some distributions, like a minimal Debian net install, do not include the `sudo` package by default.

- Properly configured DNS records for `example.com` (details in [Configuring DNS](configuring-dns.md)).

- Some TCP/UDP ports open. This playbook (actually [Docker itself](https://docs.docker.com/network/iptables/)) configures the server's internal firewall for you. In most cases, you don't need to do anything special. But **if your server is running behind another firewall**, you'd need to open these ports:

  - `80/tcp`: HTTP webserver
  - `443/tcp`: HTTPS webserver
  - potentially some other ports, depending on the services that you enable in the **configuring the playbook** step (later on). Consult each service's documentation page in `docs/` for that.

---------------------------------------------

[▶️](configuring-dns.md) When ready to proceed, continue with [Configuring DNS](configuring-dns.md).
````

## File: docs/README.md
````markdown
<!--
SPDX-FileCopyrightText: 2018 - 2023 Slavi Pantaleev
SPDX-FileCopyrightText: 2018 Aaron Raimist
SPDX-FileCopyrightText: 2019 Lyubomir Popov
SPDX-FileCopyrightText: 2024 - 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Table of Contents

## ⬇️ Installaton guide <!-- NOTE: the 🚀 emoji is used by "Getting started" on README.md -->

<!-- TODO: consider to add a quick start guide like the MDAD project has done. -->

Follow this guide to install services on your server using this Ansible playbook.

- [Prerequisites](prerequisites.md)

- [Configuring DNS settings](configuring-dns.md)

- [Getting the playbook](getting-the-playbook.md)

- [Configuring the playbook](configuring-playbook.md)

- [Installing](installing.md)

## 🛠️ Configuration options

You can check useful documentation for configuring components in [`services`](services/) directory. See [this page](supported-services.md) for a list of all supported services.

## 👨‍🔧 Maintenance

If your server and services experience issues, feel free to come to [our support room](https://matrix.to/#/#mash-playbook:devture.com) on Matrix and ask for help.

<!-- NOTE: sort list items alphabetically -->

- [Maintenance and Troubleshooting](maintenance-and-troubleshooting.md)

- [PostgreSQL maintenance](services/postgres.md#maintenance)

- [Upgrading services](maintenance-upgrading-services.md)

## Other documentation pages <!-- NOTE: this header's title and the section below need optimization -->

<!-- NOTE: sort list items under faq.md alphabetically -->

- [Alternative architectures](alternative-architectures.md)

- [Configuring interoperability with other services](interoperability.md)

- [Configuring IPv6](configuring-ipv6.md)

- [Developer documentation](developer-documentation.md)

- [Playbook tags](playbook-tags.md)

- [Running `just` commands](just.md)

- [Running multiple instances of the same service on the same host](running-multiple-instances.md)

- [Self-building](self-building.md)

- [Setting up MASH services on a Matrix server configured with the matrix-docker-ansible-deploy Ansible playbook](setting-up-services-on-mdad-server.md)

- [Supported services](supported-services.md)

- [Uninstalling](uninstalling.md)

- [Using Ansible for the playbook](ansible.md)
````

## File: docs/running-multiple-instances.md
````markdown
## Running multiple instances of the same service on the same host

The way this playbook is structured, each Ansible role can only be invoked once and made to install one instance of the service it's responsible for.

If you need multiple instances (of whichever service), you'll need some workarounds as described below.

The example below focuses on hosting multiple [Valkey](services/valkey.md) instances, but you can apply it to hosting multiple instances or whole stacks of any kind.

Let's say you're managing a host called `mash.example.com` which installs both [PeerTube](services/peertube.md) and [NetBox](services/netbox.md). Both of these services require a [Valkey](services/valkey.md) instance. If you simply add `valkey_enabled: true` to your `mash.example.com` host's `vars.yml` file, you'd get a Valkey instance (`mash-valkey`), but it's just one instance. As described in our [Valkey](services/valkey.md) documentation, this is a security problem and potentially fragile as both services may try to read/write the same data and get in conflict with one another.

We propose that you **don't** add `valkey_enabled: true` to your main `mash.example.com` file, but do the following:

## Re-do your inventory to add supplementary hosts

Create multiple hosts in your inventory (`inventory/hosts`) which target the same server, like this:

```ini
[mash_servers]
[mash_servers:children]
mash_example_com

[mash_example_com]
mash.example.com-netbox-deps ansible_host=1.2.3.4
mash.example.com-peertube-deps ansible_host=1.2.3.4
mash.example.com ansible_host=1.2.3.4
```

This creates a new group (called `mash_example_com`) which groups all 3 hosts:

- (**new**) `mash.example.com-netbox-deps` — a new host, for your [NetBox](services/netbox.md) dependencies
- (**new**) `mash.example.com-peertube-deps` — a new host, for your [PeerTube](services/peertube.md) dependencies
- (old) `mash.example.com` — your regular inventory host

When running Ansible commands later on, you can use the `-l` flag to limit which host to run them against. Here are a few examples:

- `just install-all` — runs the [installation](installing.md) process on all hosts (3 hosts in this case)
- `just install-all -l mash_example_com` — runs the installation process on all hosts in the `mash_example_com` group (same 3 hosts as `just install-all` in this case)
- `just install-all -l mash.example.com-netbox-deps` — runs the installation process on the `mash.example.com-netbox-deps` host


## Adjust the configuration of the supplementary hosts to use a new "namespace"

Multiple hosts targetting the same server as described above still causes conflicts, because services will use the same paths (e.g. `/mash/valkey`) and service/container names (`mash-valkey`) everywhere.

To avoid conflicts, adjust the `vars.yml` file for the new hosts (`mash.example.com-netbox-deps` and `mash.example.com-peertube-deps`)
and set non-default and unique values in the `mash_playbook_service_identifier_prefix` and `mash_playbook_service_base_directory_name_prefix` variables. Examples below:

`inventory/host_vars/mash.example.com-netbox-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-netbox-'
mash_playbook_service_base_directory_name_prefix: 'netbox-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

`inventory/host_vars/mash.example.com-peertube-deps/vars.yml`:

```yaml
---

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

# Override service names and directory path prefixes
mash_playbook_service_identifier_prefix: 'mash-peertube-'
mash_playbook_service_base_directory_name_prefix: 'peertube-'

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: true

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
```

The above configuration will create **2** Valkey instances:

- `mash-netbox-valkey` with its base data path in `/mash/netbox-valkey`
- `mash-peertube-valkey` with its base data path in `/mash/peertube-valkey`

These instances reuse the `mash` user and group and the `/mash` data path, but are not in conflict with each other.


## Adjust the configuration of the base host

Now that we've created separate Valkey instances for both PeerTube and NetBox, we need to put them to use by editing the `vars.yml` file of the main host (the one that installs PeerTbue and NetBox) to wire them to their Valkey instances.

You'll need configuration (`inventory/host_vars/mash.example.com/vars.yml`) like this:

```yaml
########################################################################
#                                                                      #
# netbox                                                               #
#                                                                      #
########################################################################

netbox_enabled: true

# Other NetBox configuration here

# Point NetBox to its dedicated Valkey instance
netbox_environment_variable_redis_host: mash-netbox-valkey
netbox_environment_variable_redis_cache_host: mash-netbox-valkey

# Make sure the NetBox service (mash-netbox.service) starts after its dedicated Valkey service (mash-netbox-valkey.service)
netbox_systemd_required_services_list_custom:
  - mash-netbox-valkey.service

# Make sure the NetBox container is connected to the container network of its dedicated Valkey service (mash-netbox-valkey)
netbox_container_additional_networks_custom:
  - mash-netbox-valkey

########################################################################
#                                                                      #
# /netbox                                                              #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# peertube                                                             #
#                                                                      #
########################################################################

# Other PeerTube configuration here

# Point PeerTube to its dedicated Valkey instance
peertube_config_redis_hostname: mash-peertube-valkey

# Make sure the PeerTube service (mash-peertube.service) starts after its dedicated Valkey service (mash-peertube-valkey.service)
peertube_systemd_required_services_list_custom:
  - "mash-peertube-valkey.service"

# Make sure the PeerTube container is connected to the container network of its dedicated Valkey service (mash-peertube-valkey)
peertube_container_additional_networks_custom:
  - "mash-peertube-valkey"

########################################################################
#                                                                      #
# /peertube                                                            #
#                                                                      #
########################################################################
```


## Questions & Answers

**Can't I just use the same Valkey instance for multiple services?**

> You may or you may not. See the [Valkey](services/valkey.md) documentation for why you shouldn't do this.

**Can't I just create one host and a separate stack for each service** (e.g. Nextcloud + all dependencies on one inventory host; PeerTube + all dependencies on another inventory host; with both inventory hosts targetting the same server)?

> That's a possibility which is somewhat clean. The downside is that each "full stack" comes with its own Postgres database which needs to be maintained and upgraded separately.
````

## File: docs/self-building.md
````markdown
# Self-building

**Caution: self-building does not have to be used on its own. See the [Alternative Architectures](alternative-architectures.md) page.**

The playbook supports self-building of various components, which don't have a container image for your architecture. For `amd64`, self-building is not required.

For other architectures (e.g. `arm32`, `arm64`), ready-made container images are used when available. If there's no ready-made image for a specific component and said component supports self-building, an image will be built on the host. Building images like this takes more time and resources (some build tools need to get installed by the playbook to assist building).

To make use of self-building, you don't need to do anything. If a component has an image for the specified architecture, the playbook will use it directly. If not, it will build the image on the server itself.

Note that **not all components support self-building yet**.

Adding self-building support to other roles is welcome. Feel free to contribute!

If you'd like **to force self-building** even if an image is available for your architecture, look into the `*_self_build` variables provided by individual roles.
````

## File: docs/setting-up-services-on-mdad-server.md
````markdown
<!--
SPDX-FileCopyrightText: 2025 Slavi Pantaleev
SPDX-FileCopyrightText: 2025 Suguru Hirahara

SPDX-License-Identifier: AGPL-3.0-or-later
-->

# Setting up services on the Matrix server configured with the MDAD playbook

If you use the [matrix-docker-ansible-deploy (MDAD)](https://github.com/spantaleev/matrix-docker-ansible-deploy/) Ansible playbook to manage [Matrix](https://matrix.org/) services on your server, you can configure the MASH playbook to set up services such as [Forgejo](services/forgejo.md), [GoToSocial](services/gotosocial.md), [Nextcloud](services/nextcloud.md), [PeerTube](services/peertube.md) and tons of other [supported services](./supported-services.md), along with the Matrix services on the same server. This page explains how to do so.

The basic steps to configure the MASH playbook and use it to install services are pretty same as doing so with the MDAD playbook: **setting up prerequisites (if running this playbook on a different computer), retrieving the MASH playbook, configuring it as well as the DNS records, and installing the services on the server**. If you have been accustomed to maintain Matrix services with the MDAD playbook, it should not be difficult to set up and use this playbook too.

💡 This article intends to be a guide for setting up the MASH playbook to install services on your Matrix server. If you want to know exact steps from setting up the prerequisites to running the installation command, please go to [this page](prerequisites.md) and read through the installation guide.

## Set up prerequisites

Most [prerequisites](prerequisites.md) for the MASH playbook are common to the MDAD playbook (you can check ones for the MDAD playbook [here](https://github.com/spantaleev/matrix-docker-ansible-deploy/blob/master/docs/prerequisites.md)), so it is not likely that you would need to configure something special to run the MASH playbook.

Depending on the services to enable, you might have to open ports of the server. Consult each service's documentation page in `docs/` for details. By default, you do not have to open them by yourself.

For the local environment, please make sure that you have installed and configured prerequisites such as [Ansible](ansible.md), if you run the MASH playbook on a different computer than the one you run the MDAD playbook.

## Retrieve the MASH playbook

While it is technically not impossible to integrate the roles used by the MASH playbook to the MDAD playbook, you can just retrieve the MASH playbook and run it against the same server. This way is straightforward and recommended for most cases.

See [this page](getting-the-playbook.md) for details about how to get the playbook's source code. In the same way as for the MDAD playbook, you can retrieve the playbook with git or by downloading its ZIP archive.

## Configure DNS settings

After making sure that you have configured prerequisites and retrieving the MASH playbook, let's configure DNS records for the services which you are installing with this playbook.

To configure them, go to your DNS service provider, and adjust DNS records as described [here](configuring-dns.md).

If you are adding the common `mash.example.com` domain, set the same IP address as the Matrix server (e.g. `matrix.example.com`) to its A record.

💡 As [Uptime Kuma](services/uptime-kuma.md), which needs its subdomain for now, will be enabled by default with the sample configuration file, you can go ahead and set its CNAME record in advance. The record should match with `uptime_kuma_hostname`, which will be specified when configuring `vars.yml` file next. If you will not enable the service, you do not have to add the CNAME record, of course.

## Configure the MASH playbook

Next, you'll need to copy the sample inventory hosts file ([`hosts`](../examples/hosts)) and configuration file ([`vars.yml`](../examples/mash-for-matrix-docker-ansible-deploy-users/vars.yml)) modified for installing services on your Matrix server.

To do so, run the following commands inside the playbook directory. Before creating a directory, make sure to replace `mash.example.com` with your domain. Here `example.com` should be the same one where the Matrix server is hosted.

```sh
mkdir -p inventory/host_vars/mash.example.com

cp examples/hosts inventory/hosts

cp examples/mash-for-matrix-docker-ansible-deploy-users/vars.yml inventory/host_vars/mash.example.com/vars.yml
```

### Configure `hosts` file

After copying the files, let's edit your `hosts` file.

Because you are running the playbook against the same server where the Matrix services run, you can just copy the server's external IP address specified on `hosts` file on the MDAD playbook.

If you have edited the MDAD's `hosts` file on your preference (such as adjusting the SSH port), you might probably want to copy the entire line and replace the domain with the one for this playbook such as `mash.example.com`. This should work for most case, as you should have already connected to your Matrix server with such preference.

### Configure `vars.yml` file

Having edited the `hosts` file, you need to edit the `vars.yml` file by setting passwords, etc. Check the comments on the file for details about how to configure it.

Note that `example.com` is specified as hostname values for services enabled by default such as [exim-relay](services/exim-relay.md), [Uptime Kuma](services/uptime-kuma.md), etc., so do not forget to replace them with your domain.

💡 The modified sample `vars.yml` file is not configured to install basic services such as [Docker](services/docker.md) and [Traefik](services/traefik.md), as both have been installed and configured on your Matrix server with the MDAD playbook. Installing them with this playbook will cause conflicts on the server. You can check [this page on interoperability](interoperability.md) for more information.

## Install services by running the MASH playbook

After configuring the playbook, you can proceed to installing the services.

The step for installation is common to both MASH and MDAD playbooks (ie. fetching the Ansible roles and running the installation command), so there should not be a problem. If you do not feel confident pretty much, please see [this page](installing.md) to make sure what needs to be done.

You can see [this page](supported-services.md) for a full list of the supported services and pick services which you want to install. When enabling a service, please check its documentation for the instruction.

If you want to install services, you can do so whenever you want by running the playbook. However, it is generally not recommended to install a lot of services all at once, since it can overflow the server, which can already be suffering from heavy load of [Synapse](https://github.com/spantaleev/matrix-docker-ansible-deploy/blob/master/docs/configuring-playbook-synapse.md), its de-facto Matrix homeserver.

After running the installation command, make sure to check the installed services can be accessed. **If you can access to them, the installation has completed and you can use the services along with the Matrix services**🎉

See [this section](installing.md#things-to-do-next) for details about what to do after successful installation. The MASH playbook, like the MDAD playbook, will **not** automatically run the maintenance task for you, so do not forget to update the playbook and re-run it **manually**.
````

## File: docs/supported-services.md
````markdown
# Supported services

|              Name              |              Description              | Documentation |
| ------------------------------ | ------------------------------------- | ------------- |
| [AUX](https://github.com/mother-of-all-self-hosting/ansible-role-aux) | Auxiliary file/directory management on your server via Ansible | [Link](services/auxiliary.md) |
| [AdGuard Home](https://adguard.com/en/adguard-home/overview.html/) | A network-wide DNS software for blocking ads & tracking | [Link](services/adguard-home.md) |
| [APISIX Dashboard](https://apisix.apache.org/docs/dashboard/USER_GUIDE/) | A web UI for [APISIX Gateway](services/apisix-gateway.md) | [Link](services/apisix-dashboard.md) |
| [APISIX Gateway](https://apisix.apache.org/docs/apisix/getting-started/README/) | An API Gateway, Ingress Controller, etc | [Link](services/apisix-gateway.md) |
| [Appsmith](https://www.appsmith.com/) | Platform for building and deploying custom internal tools and applications without writing code | [Link](services/appsmith.md) |
| [Authelia](https://www.authelia.com/) | An open-source authentication and authorization server that can work as a companion to [common reverse proxies](https://www.authelia.com/overview/prologue/supported-proxies/) (like [Traefik](services/traefik.md) frequently used by this playbook) | [Link](services/authelia.md) |
| [authentik](https://goauthentik.io/) | An open-source Identity Provider focused on flexibility and versatility. | [Link](services/authentik.md) |
| [borgbackup](https://www.borgbackup.org/) (via [borgmatic](https://torsion.org/borgmatic/)) | A deduplicating backup program with optional compression and encryption| [Link](services/backup-borg.md) |
| [Calibre-Web](https://github.com/janeczku/calibre-web) | Web app for browsing, reading and downloading eBooks stored in a [Calibre](https://calibre-ebook.com/) database | [Link](services/calibre-web.md) |
| [Changedetection.io](https://github.com/dgtlmoon/changedetection.io) | A simple website change detection and restock monitoring solution. | [Link](services/changedetection.md) |
| [ClickHouse](https://clickhouse.com/) | An open-source column-oriented DBMS for online analytical processing (OLAP) that allows users to generate analytical reports using SQL queries in real-time. | [Link](services/clickhouse.md) |
| [Collabora Online](https://www.collaboraoffice.com/) | Your Private Office Suite In The Cloud | [Link](services/collabora-online.md) |
| [CouchDB](https://couchdb.apache.org/) | An open-source document-oriented NoSQL database, implemented in Erlang. | [Link](services/couchdb.md) |
| [Docker](https://www.docker.com/) | Open-source software for deploying containerized applications | [Link](services/docker.md) |
| [Docker Registry](https://docs.docker.com/registry/) | A container image distribution registry | [Link](services/docker-registry.md) |
| [Docker Registry Browser](https://github.com/klausmeyer/docker-registry-browser) | Web Interface for the Docker Registry HTTP API V2 written in Ruby on Rails | [Link](services/docker-registry-browser.md) |
| [Docker Registry Proxy](https://gitlab.com/etke.cc/docker-registry-proxy/) | Pass-through docker registry (distribution) proxy with metadata caching, docker-compatible errors, prometheus metrics, etc. | [Link](services/docker-registry-proxy.md) |
| [Docker Registry Purger](https://github.com/devture/docker-registry-purger) | A small tool used for purging a private Docker Registry's old tags | [Link](services/docker-registry-purger.md) |
| [DokuWiki](https://dokuwiki.org/) | A lightweight, file-based wiki engine with intuitive syntax and no database requirements | [Link](services/dokuwiki.md) |
| [Echo IP](https://github.com/mpolden/echoip) | A simple service for looking up your IP address | [Link](services/echoip.md) |
| [Endlessh-go](https://github.com/shizunge/endlessh-go) | A golang implementation of endlessh, a ssh trapit | [Link](services/endlessh.md) |
| [etcd](https://etcd.io/) | A distributed, reliable key-value store for the most critical data of a distributed system | [Link](services/etcd.md) |
| [Etherpad](https://etherpad.org) | Open source collaborative text editor | [Link](services/etherpad.md) |
| [exim-relay](https://github.com/devture/exim-relay) | A lightweight [Exim](https://www.exim.org/) SMTP mail relay server | [Link](services/exim-relay.md) |
| [Firezone](https://www.firezone.dev/) | A self-hosted VPN server (based on [WireGuard](https://www.wireguard.com/)) with a Web UI | [Link](services/firezone.md) |
| [Focalboard](https://www.focalboard.com/) | An open source, self-hosted alternative to [Trello](https://trello.com/), [Notion](https://www.notion.so/), and [Asana](https://asana.com/). | [Link](services/focalboard.md) |
| [Forgejo](https://forgejo.org/) | A self-hosted lightweight software forge (Git hosting service, etc). An alternative to [Gitea](./services/gitea.md). | [Link](services/forgejo.md) |
| [Forgejo Runner](https://code.forgejo.org/forgejo/runner) | A runner to use with Forgejo Actions | [Link](services/forgejo-runner.md) |
| [Freescout](https://freescout.net/) | A free help desk software | [Link](services/freescout.md) |
| [FreshRSS](https://freshrss.org/) | RSS and Atom feed aggregator. | [Link](services/freshrss.md) |
| [Funkwhale](https://funkwhale.audio/) | Listen and share music with a selfhosted streaming server.| [Link](services/funkwhale.md) |
| [Gitea](https://gitea.io/) | A painless self-hosted [Git](https://git-scm.com/) service. | [Link](services/gitea.md) |
| [GoToSocial](https://gotosocial.org/) | A self-hosted [ActivityPub](https://activitypub.rocks/) social network server | [Link](services/gotosocial.md) |
| [Grafana](https://grafana.com/) | An open and composable observability and data visualization platform, often used with [Prometheus](services/prometheus.md) | [Link](services/grafana.md) |
| [Grafana Loki](https://grafana.com/docs/loki/latest/) | Open-source log aggregation system that helps collect, store, and analyze logs in a scalable and efficient manner | [Link](services/grafana-loki.md) |
| [Headscale](https://headscale.net/) | A Tailscale-compatible control server for managing Tailscale devices | [Link](services/headscale.md) |
| [Healthchecks](https://healthchecks.io/) | A simple and Effective Cron Job Monitoring solution | [Link](services/healthchecks.md) |
| [Hubsite](https://github.com/moan0s/hubsite) | A simple, static site that shows an overview of the available services | [Link](services/hubsite.md) |
| [ILMO](https://github.com/moan0s/ILMO2) | An open source library management tool. | [Link](services/ilmo.md) |
| [Infisical](https://infisical.com/) | An open-source end-to-end encrypted platform for securely managing secrets and configs across your team, devices, and infrastructure. | [Link](services/infisical.md) |
| [InfluxDB](https://www.influxdata.com/) | A self-hosted time-series database. | [Link](services/influxdb.md) |
| [Jackett](https://github.com/Jackett/Jackett) | An API for your favorite torrent trackers | [Link](services/jackett.md) |
| [Jitsi](https://jitsi.org/) | A fully encrypted, 100% Open Source video conferencing solution | [Link](services/jitsi.md) |
| [Keycloak](https://www.keycloak.org/) | An open source identity and access management solution. | [Link](services/keycloak.md) |
| [KeyDB](https://docs.keydb.dev/) | An in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker. | [Link](services/keydb.md) |
| [LabelStudio](https://labelstud.io/) | Label Studio is an open source data labeling tool that supports multiple projects, users, and data types in one platform | [Link](services/labelstudio.md) |
| [Lago](https://www.getlago.com/) | Open-source metering and usage-based billing | [Link](services/lago.md) |
| [languageTool](https://languagetool.org/) | An open source online grammar, style and spell checker | [Link](services/languagetool.md) |
| [linkding](https://github.com/sissbruecker/linkding/) | Bookmark manager designed to be minimal and fast. | [Link](services/linkding.md) |
| [MariaDB](https://mariadb.org/) | A powerful, open source object-relational database system | [Link](services/mariadb.md) |
| [Matrix Rooms Search API](https://gitlab.com/etke.cc/mrs/api) | A fully-featured, standalone, matrix rooms search service. | [Link](services/mrs.md) |
| [Matterbridge](https://github.com/42wim/matterbridge) | Bridges Messenger Chatrooms | [Link](services/matterbridge.md) |
| [Miniflux](https://miniflux.app/) | Minimalist and opinionated feed reader. | [Link](services/miniflux.md) |
| [Mobilizon](https://joinmobilizon.org/en/) | An ActivityPub/Fediverse server to create and share events. | [Link](services/mobilizon.md) |
| [MongoDB](https://www.mongodb.com/) | A source-available cross-platform document-oriented (NoSQL) database program. | [Link](services/mongodb.md) |
| [Mosquitto](https://mosquitto.org/) | An open-source MQTT broker | [Link](services/mosquitto.md) |
| [n8n](https://n8n.io/) | Workflow automation for technical people. | [Link](services/n8n.md) |
| [Navidrome](https://www.navidrome.org/) | [Subsonic-API](http://www.subsonic.org/pages/api.jsp) compatible music server | [Link](services/navidrome.md)
| [n.eko](https://neko.m1k1o.net/) | A self-hosted virtual browser or even desktop environment | [Link](services/neko.md) |
| [NetBox](https://docs.netbox.dev/en/stable/) | Web application that provides [IP address management (IPAM)](https://en.wikipedia.org/wiki/IP_address_management) and [data center infrastructure management (DCIM)](https://en.wikipedia.org/wiki/Data_center_management#Data_center_infrastructure_management) functionality | [Link](services/netbox.md) |
| [Nextcloud](https://nextcloud.com/) | The most popular self-hosted collaboration solution for tens of millions of users at thousands of organizations across the globe. | [Link](services/nextcloud.md) |
| [ntfy](https://ntfy.sh/) | Simple HTTP-based pub-sub notification service to send you push notifications from any computer, using simple HTTP PUT or POST requests. | [Link](services/ntfy.md) |
| [OAuth2-Proxy](https://oauth2-proxy.github.io/oauth2-proxy/) | A reverse proxy and static file server that provides authentication using OpenID Connect Providers (Google, GitHub, [Keycloak](services/keycloak.md), and others) to SSO-protect services which do not support SSO natively. | [Link](services/oauth2-proxy.md) |
| [Outline](https://www.getoutline.com/) | An open-source knowledge base for growing teams. | [Link](services/outline.md) |
| [Overseerr](https://overseerr.dev/) | A request management and media discovery tool for the Plex ecosystem | [Link](services/overseerr.md) |
| [Owncast](https://owncast.online/) | Owncast is a free and open source live video and web chat server for use with existing popular broadcasting software. | [Link](services/owncast.md) |
| [OxiTraffic](https://codeberg.org/mo8it/oxitraffic) | [OxiTraffic](https://codeberg.org/mo8it/oxitraffic) is a self-hosted, simple and privacy respecting website traffic tracker. | [Link](services/oxitraffic.md) |
| [Paperless-ngx](https://paperless-ngx.com) | [Paperless-ngx](https://paperless-ngx.com) is a community-supported open-source document management system that transforms your physical documents into a searchable online archive so you can keep, well, less paper. | [Link](services/paperless-ngx.md) |
| [PeerTube](https://joinpeertube.org/) | A tool for sharing online videos | [Link](services/peertube.md) |
| [Plausible Analytics](https://plausible.io/) | Intuitive, lightweight and open source web analytics | [Link](services/plausible.md) |
| [Postgis](https://postgis.net/) | A spatial database extender for PostgreSQL object-relational database | [Link](services/postgis.md) |
| [Postgres](https://www.postgresql.org) | A powerful, open source object-relational database system | [Link](services/postgres.md) |
| [Postgres Backup](https://github.com/prodrigestivill/docker-postgres-backup-local) | A solution for backing up PostgresSQL to local filesystem with periodic backups. | [Link](services/postgres-backup.md) |
| [Prometheus](https://prometheus.io/) | A metrics collection and alerting monitoring solution | [Link](services/prometheus.md) |
| [Prometheus Blackbox Exporter](https://github.com/prometheus/blackbox_exporter) | Blackbox probing of HTTP/HTTPS/DNS/TCP/ICMP and gRPC endpoints | [Link](services/prometheus-blackbox-exporter.md) |
| [Prometheus Node Exporter](https://github.com/prometheus/node_exporter) | Exporter for machine metrics | [Link](services/prometheus-node-exporter.md) |
| [Prometheus Postgres Exporter](https://github.com/prometheus-community/postgres_exporter) | A PostgreSQL metric exporter for Prometheus | [Link](services/prometheus-postgres-exporter.md) |
| [Prometheus SSH Exporter](https://github.com/treydock/ssh_exporter) | SSH probes | [Link](services/prometheus-ssh-exporter.md) |
| [Promtail](https://grafana.com/docs/loki/latest/send-data/promtail/) | An agent which ships the contents of local logs to a private [Grafana Loki](services/grafana-loki.md) instance | [Link](services/promtail.md) |
| [qBittorrent](https://www.qbittorrent.org/) | A BitTorrent client written in native C++ | [Link](services/qbittorrent.md) |
| [Radarr](https://radarr.video/) | A movie organizer/manager for usenet and torrent users | [Link](services/radarr.md) |
| [Radicale](https://radicale.org/) | A Free and Open-Source CalDAV and CardDAV Server (solution for hosting contacts and calendars) | [Link](services/radicale.md) |
| [Readeck](https://readeck.org) | A bookmark manager and a read-later tool combined in one. | [Link](services/readeck.md) |
| [Redis](https://redis.io/) | An in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker. | [Link](services/redis.md) |
| [Redmine](https://redmine.org/) | A flexible project management web application. | [Link](services/redmine.md) |
| [Roundcube](https://roundcube.net/) | A browser-based multilingual IMAP client with an application-like user interface | [Link](services/roundcube.md) |
| [rumqttd](https://github.com/bytebeamio/rumqtt) | A high performance, embeddable [MQTT](https://en.wikipedia.org/wiki/MQTT) broker | [Link](services/rumqttd.md) |
| [SearXNG](https://github.com/searxng/searxng) | A privacy-respecting, hackable [metasearch engine](https://en.wikipedia.org/wiki/Metasearch_engine) | [Link](services/searxng.md) |
| [Semaphore](https://www.ansible-semaphore.com/) | A responsive web UI for running Ansible playbooks | [Link](services/semaphore.md) |
| [Soft Serve](https://github.com/charmbracelet/soft-serve) | A tasty, self-hostable [Git](https://git-scm.com/) server for the command line | [Link](services/soft-serve.md) |
| [Sonarr](https://sonarr.tv/) | A smart PVR for newsgroup and bittorrent users | [Link](services/sonarr.md) |
| [Stirling PDF](https://github.com/Stirling-Tools/Stirling-PDF) | A self-hosted PDF converter | [Link](services/stirling-pdf.md) |
| [Syncthing](https://syncthing.net/) | A continuous file synchronization program which synchronizes files between two or more computers in real time | [Link](services/syncthing.md) |
| [Tandoor](https://docs.tandoor.dev/) | The recipe manager that allows you to manage your ever growing collection of digital recipes.| [Link](services/tandoor.md)
| [Telegraf](https://www.influxdata.com/time-series-platform/telegraf/) | An open source server agent to help you collect metrics from your stacks, sensors, and systems. | [Link](services/telegraf.md) |
| [Traefik](https://doc.traefik.io/traefik/) | A container-aware reverse-proxy server | [Link](services/traefik.md) |
| [TSDProxy](https://almeidapaulopt.github.io/tsdproxy/) | A proxy for virtual services in Tailscale | [Link](services/tsdproxy.md) |
| [Uptime Kuma](https://uptime.kuma.pet/) | A fancy self-hosted monitoring tool | [Link](services/uptime-kuma.md) |
| [Valkey](https://valkey.io/) | A flexible distributed key-value datastore that is optimized for caching and other realtime workloads. | [Link](services/valkey.md) |
| [Vaultwarden](https://github.com/dani-garcia/vaultwarden) | A lightweight unofficial and compatible implementation of the [Bitwarden](https://bitwarden.com/)password manager | [Link](services/vaultwarden.md) |
| [Versatiles](https://versatiles.org) | A free stack for generating and serving vector tiles from OpenStreetMap. | [Link](services/versatiles.md) |
| [Wetty](https://github.com/butlerx/wetty) | An SSH terminal over HTTP/HTTPS | [Link](services/wetty.md) |
| [WireGuard Easy](https://github.com/wg-easy/wg-easy) | The easiest way to run [WireGuard](https://www.wireguard.com/) VPN + Web-based Admin UI. | [Link](services/wg-easy.md) |
| [Woodpecker CI](https://woodpecker-ci.org/) | A simple Continuous Integration (CI) engine with great extensibility. | [Link](services/woodpecker-ci.md) |
| [WordPress](https://wordpress.org/) | A widley used open source web content management system | [Link](services/wordpress.md) |
| [Writefreely](https://writefreely.org) | A clean, minimalist publishing platform made for writers with optional federation via ActivityPub. | [Link](services/writefreely.md) |
| System-related | A collection of various system-related components | [Link](services/system.md) |


## Related playbooks

- [matrix-docker-ansible-deploy (MDAD)](https://github.com/spantaleev/matrix-docker-ansible-deploy) — for deploying a fully-featured [Matrix](https://matrix.org) homeserver. This playbook will remain independent, because the Matrix ecosystem is incredibly large — lots of bots, bridges and other pieces of software. It deserves its own dedicated playbook.

  The basic steps to configure the MDAD playbook and use it to install services are pretty same as doing so with the MASH playbook: **setting up prerequisites (if running this playbook on a different computer), retrieving the MDAD playbook, configuring it as well as the DNS records, and installing the services on the server**. If you have been accustomed to use the MASH playbook, it should not be difficult to set up and use this playbook too.
````

## File: docs/uninstalling.md
````markdown
# Uninstalling

> [!WARNING]
> If you have some trouble with your installation, you can just [re-run the playbook](installing.md) and it will try to set things up again. **Uninstalling and then installing anew rarely solves anything**.

To uninstall, run these commands (most are meant to be executed on the server itself):

- ensure all services are stopped: `just stop` (if you can't get Ansible working to run this command, you can run `systemctl stop 'mash-*'` manually on the server)

- delete the systemd `.service` and `.timer` files (`rm -f /etc/systemd/system/mash-*.{service,timer}`) and reload systemd (`systemctl daemon-reload`)

- delete some cached Docker images (`docker system prune -a`) or just delete them all (`docker rmi $(docker images -aq)`)

- uninstall Docker itself, if necessary

- delete the `/mash` directory (`rm -rf /mash`)
````

## File: examples/mash-for-matrix-docker-ansible-deploy-users/vars.yml
````yaml
# SPDX-License-Identifier: 2023 - 2025 Slavi Pantaleev
# SPDX-License-Identifier: 2023 Julian-Samuel Gebühr
# SPDX-License-Identifier: 2025 Suguru Hirahara
#
# SPDX-License-Identifier: AGPL-3.0-or-later

---

# Below is an example which installs a few services on the same server
# as your Matrix services have been installed and configured with
# matrix-docker-ansible-deploy (MDAD) playbook.
#
# If you are installing services on the server where neither Docker
# nor Traefik is installed, see vars.yml on the parent directory instead.
#
# You should tweak this example as you see fit and enable the services that you need.

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# Docker                                                               #
#                                                                      #
########################################################################

# To disable Docker SDK for Python installation (in case you'd be installing the SDK in another way),
# remove the line below.
devture_docker_sdk_for_python_installation_enabled: true

# Controls whether container networks will be created with IPv6 support.
#
# If you also have IPv6 support on your server/network and AAAA DNS records pointing to the server,
# enabling this will effectively give you full public IPv6 connectivity (powered by NAT66).
#
# We recommend leaving this enabled even if you don't currently have IPv6 connectivity on your server/network.
# This way, once you eventually get IPv6 connectivity, you won't have to change anything (besides DNS records).
#
# Flipping this setting later on requires manual work (stopping services, deleting and recreating all container networks).
#
# In the future, this setting will likely default to `true`, so if you really want IPv6 disabled, explicitly set this to `false`.
#
# People managing Docker themselves and running an older Docker version will need additional configuration.
#
# Learn more in `docs/configuring-ipv6.md`.
devture_systemd_docker_base_ipv6_enabled: true

########################################################################
#                                                                      #
# /Docker                                                              #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# com.devture.ansible.role.timesync                                    #
#                                                                      #
########################################################################

# To ensure the server's clock is synchronized (using systemd-timesyncd/ntpd),
# we enable the timesync service.

devture_timesync_installation_enabled: true

########################################################################
#                                                                      #
# /com.devture.ansible.role.timesync                                   #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# traefik                                                              #
#                                                                      #
########################################################################

# We're relying on the matrix-docker-ansible-deploy playbook to manage Traefik,
# so tell mash-playbook about it.
mash_playbook_reverse_proxy_type: other-traefik-container

# Tell the playbook to attach services which require reverse-proxying to an additional network by default (e.g. traefik)
# This needs to match your Traefik network.
mash_playbook_reverse_proxyable_services_additional_network: traefik

# Uncomment and adjust the variables below if you'd like to enable HTTP-compression.
#
# For this to work, you will need to define a compress middleware (https://doc.traefik.io/traefik/middlewares/http/compress/) for your Traefik instance
# using a file (https://doc.traefik.io/traefik/providers/file/) or Docker (https://doc.traefik.io/traefik/providers/docker/) configuration provider.
#
# mash_playbook_reverse_proxy_traefik_middleware_compession_enabled: true
# mash_playbook_reverse_proxy_traefik_middleware_compession_name: my-compression-middleware@file

########################################################################
#                                                                      #
# /traefik                                                             #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# postgres                                                             #
#                                                                      #
########################################################################

# Most services require a Postgres database, so we enable Postgres here.
#
# Learn more about the Postgres service in docs/services/postgres.md

postgres_enabled: true

# Put a strong password below, generated with `pwgen -s 64 1` or in another way
postgres_connection_password: ''

########################################################################
#                                                                      #
# /postgres                                                            #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# exim_relay                                                           #
#                                                                      #
########################################################################

# Various services need to send out email.
#
# Enabling this Exim relay SMTP mailer service automatically wires
# all other services to send email through it.
#
# exim-relay then gives you a centralized place for configuring email-sending.

# By default, exim-relay attempts to deliver emails directly. It may or
# may not work, depending on your domain configuration.
#
# Exim-relay supports DomainKeys Identified Mail (DKIM), and you may
# probably want to consider enabling it in order to improve deliverability.
# Without proper authentication setting, your outgoing email is most
# likely to be quarantined as spam at recipient's mail servers.
#
# Alternatively, you can have the exim-relay use via an external SMTP
# server to relay emails.
#
# See docs/services/exim-relay.md for details about configuration.

exim_relay_enabled: true

exim_relay_hostname: mash.example.com

exim_relay_sender_address: "someone@{{ exim_relay_hostname }}"

########################################################################
#                                                                      #
# /exim_relay                                                          #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# miniflux                                                             #
#                                                                      #
########################################################################

# Learn more about the Miniflux service in docs/services/miniflux.md
#
# This service is only here as an example. If you don't wish to use the
# Miniflux service, remove the whole section.

miniflux_enabled: true

miniflux_hostname: mash.example.com
miniflux_path_prefix: /miniflux

miniflux_admin_login: your-username-here
miniflux_admin_password: a-strong-password-here

########################################################################
#                                                                      #
# /miniflux                                                            #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# uptime-kuma                                                          #
#                                                                      #
########################################################################

# Learn more about the Uptime Kuma service in docs/services/uptime-kuma.md
#
# This service is only here as an example. If you don't wish to use the
# Uptime Kuma service, remove the whole section.

uptime_kuma_enabled: true

uptime_kuma_hostname: uptime-kuma.example.com

# For now, hosting Uptime Kuma under a path is not supported.
# See: https://github.com/louislam/uptime-kuma/issues/147
# uptime_kuma_path_prefix: /uptime-kuma

########################################################################
#                                                                      #
# /uptime-kuma                                                         #
#                                                                      #
########################################################################


# You can add additional services here, as you see fit.
# To discover new services and configuration, see docs/supported-services.md
````

## File: examples/hosts
````
# To connect using a non-root user (and elevate to root with sudo later),
# replace `ansible_ssh_user=root` with something like this: `ansible_ssh_user=username become=true become_user=root`.
# If sudo requires a password, either add `become_password=PASSWORD_HERE` to the host line
# or tell Ansible to ask you for the password interactively by adding a `--ask-become-pass` (`-K`) flag to all `ansible-playbook` (or `just`) commands.
#
# For improved Ansible performance, SSH pipelining is enabled by default in `ansible.cfg`.
# If this causes SSH connection troubles, disable it by adding `ansible_ssh_pipelining=False`
# to the host line below or by adding `ansible_ssh_pipelining: False` to your variables file.
#
# If SSH is configured to listen to a non-standard port (i.e. something different than port 22), you need to add `ansible_port=<your configured SSH port>`.
#
# If you're running this Ansible playbook on the same server as the one you're installing to,
# consider adding an additional `ansible_connection=local` argument to the host line below.
#
# Ansible may fail to discover which Python interpreter to use on the host for some distros (like Ubuntu 20.04).
# You may sometimes need to explicitly add the argument `ansible_python_interpreter=/usr/bin/python3`
# to the host line below.

[mash_servers]
<your-domain> ansible_host=<your-server's external IP address> ansible_ssh_user=root
````

## File: examples/vars.yml
````yaml
# SPDX-License-Identifier: 2023 - 2025 Slavi Pantaleev
# SPDX-License-Identifier: 2023 Julian-Samuel Gebühr
#
# SPDX-License-Identifier: AGPL-3.0-or-later

---

# Below is an example which installs a few services on the host, in different configuration.
# You should tweak this example as you see fit and enable the services that you need.

########################################################################
#                                                                      #
# Playbook                                                             #
#                                                                      #
########################################################################

# Put a strong secret below, generated with `pwgen -s 64 1` or in another way
# Various other secrets will be derived from this secret automatically.
mash_playbook_generic_secret_key: ''

########################################################################
#                                                                      #
# /Playbook                                                            #
#                                                                      #
########################################################################


########################################################################
#                                                                      #
# Docker                                                               #
#                                                                      #
########################################################################

# To disable Docker installation (in case you'd be installing Docker in another way),
# remove the line below.
mash_playbook_docker_installation_enabled: true

# To disable Docker SDK for Python installation (in case you'd be installing the SDK in another way),
# remove the line below.
devture_docker_sdk_for_python_installation_enabled: true

# Controls whether container networks will be created with IPv6 support.
#
# If you also have IPv6 support on your server/network and AAAA DNS records pointing to the server,
# enabling this will effectively give you full public IPv6 connectivity (powered by NAT66).
#
# We recommend leaving this enabled even if you don't currently have IPv6 connectivity on your server/network.
# This way, once you eventually get IPv6 connectivity, you won't have to change anything (besides DNS records).
#
# Flipping this setting later on requires manual work (stopping services, deleting and recreating all container networks).
#
# In the future, this setting will likely default to `true`, so if you really want IPv6 disabled, explicitly set this to `false`.
#
# People managing Docker themselves and running an older Docker version will need additional configuration.
#
# Learn more in `docs/configuring-ipv6.md`.
devture_systemd_docker_base_ipv6_enabled: true

########################################################################
#                                                                      #
# /Docker                                                              #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# com.devture.ansible.role.timesync                                    #
#                                                                      #
########################################################################

# To ensure the server's clock is synchronized (using systemd-timesyncd/ntpd),
# we enable the timesync service.

devture_timesync_installation_enabled: true

########################################################################
#                                                                      #
# /com.devture.ansible.role.timesync                                   #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# traefik                                                              #
#                                                                      #
########################################################################

# Most services require a reverse-proxy, so we enable Traefik here.
#
# Learn more about the Traefik service in docs/services/traefik.md
#
# If your server already runs Traefik, you will run into port conflicts by installing it twice.
# See docs/interoperability.md for solutions.

mash_playbook_reverse_proxy_type: playbook-managed-traefik

########################################################################
#                                                                      #
# /traefik                                                             #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# postgres                                                             #
#                                                                      #
########################################################################

# Most services require a Postgres database, so we enable Postgres here.
#
# Learn more about the Postgres service in docs/services/postgres.md

postgres_enabled: true

# Put a strong password below, generated with `pwgen -s 64 1` or in another way
postgres_connection_password: ''

########################################################################
#                                                                      #
# /postgres                                                            #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# exim_relay                                                           #
#                                                                      #
########################################################################

# Various services need to send out email.
#
# Enabling this Exim relay SMTP mailer service automatically wires
# all other services to send email through it.
#
# exim-relay then gives you a centralized place for configuring email-sending.

# By default, exim-relay attempts to deliver emails directly. It may or
# may not work, depending on your domain configuration.
#
# Exim-relay supports DomainKeys Identified Mail (DKIM), and you may
# probably want to consider enabling it in order to improve deliverability.
# Without proper authentication setting, your outgoing email is most
# likely to be quarantined as spam at recipient's mail servers.
#
# Alternatively, you can have the exim-relay use via an external SMTP
# server to relay emails.
#
# See docs/services/exim-relay.md for details about configuration.

exim_relay_enabled: true

exim_relay_hostname: mash.example.com

exim_relay_sender_address: "someone@{{ exim_relay_hostname }}"

########################################################################
#                                                                      #
# /exim_relay                                                          #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# miniflux                                                             #
#                                                                      #
########################################################################

# Learn more about the Miniflux service in docs/services/miniflux.md
#
# This service is only here as an example. If you don't wish to use the
# Miniflux service, remove the whole section.

miniflux_enabled: true

miniflux_hostname: mash.example.com
miniflux_path_prefix: /miniflux

miniflux_admin_login: your-username-here
miniflux_admin_password: a-strong-password-here

########################################################################
#                                                                      #
# /miniflux                                                            #
#                                                                      #
########################################################################



########################################################################
#                                                                      #
# uptime-kuma                                                          #
#                                                                      #
########################################################################

# Learn more about the Uptime Kuma service in docs/services/uptime-kuma.md
#
# This service is only here as an example. If you don't wish to use the
# Uptime Kuma service, remove the whole section.

uptime_kuma_enabled: true

uptime_kuma_hostname: uptime-kuma.example.com

# For now, hosting Uptime Kuma under a path is not supported.
# See: https://github.com/louislam/uptime-kuma/issues/147
# uptime_kuma_path_prefix: /uptime-kuma

########################################################################
#                                                                      #
# /uptime-kuma                                                         #
#                                                                      #
########################################################################


# You can add additional services here, as you see fit.
# To discover new services and configuration, see docs/supported-services.md
````

## File: roles/mash/playbook_base/defaults/main.yml
````yaml
---

# Controls the identifier for this MASH stack.
# This affects user/groups, systemd service names, container names, container networks, base installation path, etc.
mash_playbook_identifier: mash

mash_playbook_user_username: "{{ mash_playbook_identifier }}"
mash_playbook_user_groupname: "{{ mash_playbook_identifier }}"

mash_playbook_user_home: "{{ mash_playbook_base_path }}"

# By default, the playbook creates the user (`mash_playbook_user_username`)
# and group (`mash_playbook_user_groupname`) with a random id.
# To use a specific user/group id, override these variables.
mash_playbook_uid: ~
mash_playbook_gid: ~

# A secret used as a base, for generating various other secrets.
# You can put any string here, but generating a strong one is preferred (e.g. `pwgen -s 64 1`).
mash_playbook_generic_secret_key: ''

# Controls the prefix used for all service identifiers.
# This affects systemd service names, container names, container networks, etc.
mash_playbook_service_identifier_prefix: "{{ mash_playbook_identifier }}-"

# Controls the prefix of the base directory for all services.
# Example: `/mash/{PREFIX}traefik`.
# If `mash_playbook_identifier` is the default (mash), we intentionally use an empty prefix.
mash_playbook_service_base_directory_name_prefix: "{{ '' if mash_playbook_identifier == 'mash' else (mash_playbook_identifier + '-') }}"

# Controls the base path where all services will be installed
mash_playbook_base_path: "/{{ mash_playbook_identifier }}"
mash_playbook_base_path_mode: "750"

# The architecture that your server runs.
# Recognized values by us are 'amd64', 'arm32' and 'arm64'.
# Not all architectures support all services, so your experience (on non-amd64) may vary.
mash_playbook_architecture: "{{ 'amd64' if ansible_architecture == 'x86_64' else ('arm64' if ansible_architecture == 'aarch64' else ('arm32' if ansible_architecture.startswith('armv') else '')) }}"

# Specifies the type of reverse-proxy used by the playbook.
#
# Changing this has an effect on whether a reverse-proxy is installed at all and what its type is,
# as well as how all other services are configured.
#
# Valid options and a description of their behavior:
#
# - `playbook-managed-traefik`
#     - the playbook will run a managed Traefik instance (mash-traefik)
#     - Traefik will do SSL termination, unless you disable it (e.g. `traefik_config_entrypoint_web_secure_enabled: false`)
#
# - `other-traefik-container`
#     - this playbook will not install Traefik
#     - nevertheless, the playbook expects that you would install Traefik yourself via other means
#     - you should make sure your Traefik configuration is compatible with what the playbook would have configured (web, web-secure entrypoints, etc.)
#     - you need to set `mash_playbook_reverse_proxyable_services_additional_network` to the name of your Traefik network
#
# - `none`
#     - no reverse-proxy will be installed
#     - no port exposure will be done for any of the container services
#     - it's up to you to expose the ports you want, etc.
mash_playbook_reverse_proxy_type: none

# Controls whether to install Docker or not
# Also see `devture_docker_sdk_for_python_installation_enabled`.
mash_playbook_docker_installation_enabled: false

mash_playbook_docker_installation_daemon_options: "{{ mash_playbook_docker_installation_daemon_options_auto | combine(mash_playbook_docker_installation_daemon_options_custom, recursive=True) }}"

# Since Docker 27.0.1, Docker daemon options do not need to be changed to enable IPv6 support on the daemon side.
# See: https://docs.docker.com/engine/release-notes/27/#ipv6
# We only enable `ip6tables` and `experimental` for people who explicitly request it (perhaps due to running an old Docker version).
#
# Despite IPv6-enablement at the Docker level being a thing, for IPv6 to work for containers
# networks need to be created with IPv6 explicitly enabled.
# This is controlled by the `devture_systemd_docker_base_ipv6_enabled` variable and it's up to the various roles to
# respect this variable when creating their networks.
mash_playbook_docker_installation_daemon_options_auto: |
  {{
    ({'experimental': true, 'ip6tables': true} if devture_systemd_docker_base_ipv6_daemon_options_changing_enabled else {})
  }}

mash_playbook_docker_installation_daemon_options_custom: {}

mash_playbook_docker_installation_daemon_options_file_path: /etc/docker/daemon.json

# Controls whether to attach Traefik labels to services.
# This is separate from `traefik_enabled`, because you may wish to disable Traefik installation by the playbook,
# yet still use Traefik installed in another way.
mash_playbook_traefik_labels_enabled: "{{ mash_playbook_reverse_proxy_type in ['playbook-managed-traefik', 'other-traefik-container'] }}"

# Specifies whether the Traefik reverse-proxy (if `mash_playbook_reverse_proxy_type` indicates that Traefik is being used) defines a compression middleware.
mash_playbook_reverse_proxy_traefik_middleware_compession_enabled: "{{ traefik_config_http_middlewares_compression_enabled if (traefik_enabled and traefik_config_http_middlewares_compression_enabled) else false }}"

# Specifies the name of the compression middleware defined for the Traefik reverse-proxy (if `mash_playbook_reverse_proxy_type` indicates that Traefik is being used).
# It's better to use a fully-qualified middleware name (e.g. `compression@docker` or `compression@file`) here to prevent ambiguity.
mash_playbook_reverse_proxy_traefik_middleware_compession_name: "{{ (traefik_config_http_middlewares_compression_middleware_name + '@file') if traefik_enabled else '' }}"

# Controls the additional network that reverse-proxyable services will be connected to.
mash_playbook_reverse_proxyable_services_additional_network: "{{ traefik_container_network if traefik_enabled | default(false) else '' }}"

# Controls whether various services should expose metrics publicly.
# If Prometheus is operating on the same machine, exposing metrics publicly is not necessary.
mash_playbook_metrics_exposure_enabled: false
mash_playbook_metrics_exposure_hostname: ''
mash_playbook_metrics_exposure_path_prefix: /metrics
mash_playbook_metrics_exposure_http_basic_auth_enabled: false
# See https://doc.traefik.io/traefik/middlewares/http/basicauth/#users
mash_playbook_metrics_exposure_http_basic_auth_users: ''
````

## File: roles/mash/playbook_base/tasks/main.yml
````yaml
---

# This needs to always run, because it populates `mash_playbook_uid` and `mash_playbook_gid`,
# which are required by many other roles.
- block:
    - ansible.builtin.include_tasks: "{{ role_path }}/tasks/setup_user.yml"
  tags:
    - always

- block:
    - ansible.builtin.include_tasks: "{{ role_path }}/tasks/validate_config.yml"

    - ansible.builtin.include_tasks: "{{ role_path }}/tasks/setup_base_dir.yml"
  tags:
    - setup-all
    - install-all
````

## File: roles/mash/playbook_base/tasks/setup_base_dir.yml
````yaml
---

- name: Ensure mash base paths exists
  ansible.builtin.file:
    path: "{{ mash_playbook_base_path }}"
    state: directory
    mode: "{{ mash_playbook_base_path_mode }}"
    owner: "{{ mash_playbook_uid }}"
    group: "{{ mash_playbook_gid }}"
````

## File: roles/mash/playbook_base/tasks/setup_user.yml
````yaml
---

- when: not mash_playbook_uid and not mash_playbook_uid
  block:
    - name: Ensure mash group is created
      ansible.builtin.group:
        name: "{{ mash_playbook_user_groupname }}"
        gid: "{{ omit if mash_playbook_gid is none else mash_playbook_gid }}"
        state: present
      register: mash_base_group_result

    - name: Ensure mash user is created
      ansible.builtin.user:
        name: "{{ mash_playbook_user_username }}"
        uid: "{{ omit if mash_playbook_uid is none else mash_playbook_uid }}"
        state: present
        group: "{{ mash_playbook_user_groupname }}"
        home: "{{ mash_playbook_user_home }}"
        create_home: false
        system: true
      register: mash_base_user_result

    - name: Initialize mash_playbook_uid and mash_playbook_gid
      ansible.builtin.set_fact:
        mash_playbook_uid: "{{ mash_base_user_result.uid }}"
        mash_playbook_gid: "{{ mash_base_group_result.gid }}"
````

## File: roles/mash/playbook_base/tasks/validate_config.yml
````yaml
---

- name: Fail if required mash-playbook settings not defined
  ansible.builtin.fail:
    msg: >-
      You need to define a required configuration setting (`{{ item.name }}`).
  when: "item.when | bool and vars[item.name] == ''"
  with_items:
    - {'name': 'mash_playbook_generic_secret_key', 'when': true}
    - {'name': 'mash_playbook_generic_secret_key', 'when': true}
    - {'name': 'mash_playbook_metrics_exposure_hostname', 'when': "{{ mash_playbook_metrics_exposure_enabled }}"}

- name: Fail if mash_playbook_reverse_proxy_type is set incorrectly
  ansible.builtin.fail:
    msg: "Detected that variable mash_playbook_reverse_proxy_type (current value: `{{ mash_playbook_reverse_proxy_type }}`) appears to be set incorrectly. See roles/custom/mash_playbook_base/defaults/main.yml for valid choices."
  when: mash_playbook_reverse_proxy_type not in ['playbook-managed-traefik', 'other-traefik-container', 'none']
````

## File: roles/mash/playbook_migration/defaults/main.yml
````yaml
---

# Controls if the old apt repository for Docker (`signed-by=/etc/apt/trusted.gpg.d/docker.asc`) will be removed,
# so that the Docker role (7.2.0+) can install a new non-conflicting one (`signed-by=/etc/apt/keyrings/docker.asc`).
#
# Without this migration, the role would choke at the "galaxy/docker : Add Docker repository." task when trying to add the repository again:
# > An exception occurred during task execution. To see the full traceback, use -vvv. The error was: apt_pkg.Error: E:Conflicting values set for option Signed-By regarding source https://download.docker.com/linux/ubuntu/ focal: /etc/apt/trusted.gpg.d/docker.asc != /etc/apt/keyrings/docker.asc, E:The list of sources could not be read.
#
# Related to: https://github.com/geerlingguy/ansible-role-docker/pull/436
mash_playbook_migration_docker_trusted_gpg_d_migration_enabled: true
mash_playbook_migration_docker_trusted_gpg_d_migration_repository_path: "/etc/apt/sources.list.d/docker.list"
````

## File: roles/mash/playbook_migration/tasks/debian_docker_trusted_gpg_d_migration_migration.yml
````yaml
---

- name: Check if the Docker apt repository file exists
  ansible.builtin.stat:
    path: "{{ mash_playbook_migration_docker_trusted_gpg_d_migration_repository_path }}"
  register: mash_playbook_migration_docker_trusted_gpg_d_migration_repository_path_status

- when: mash_playbook_migration_docker_trusted_gpg_d_migration_repository_path_status.stat.exists | bool
  block:
    - name: Read repository file
      ansible.builtin.slurp:
        path: "{{ mash_playbook_migration_docker_trusted_gpg_d_migration_repository_path }}"
      register: mash_playbook_migration_docker_trusted_gpg_d_migration_repository_path_content

    - name: Remove Docker apt repository file if old key path found
      when: "'/etc/apt/trusted.gpg.d/docker.asc' in mash_playbook_migration_docker_trusted_gpg_d_migration_repository_path_content.content | b64decode"
      ansible.builtin.file:
        path: "{{ mash_playbook_migration_docker_trusted_gpg_d_migration_repository_path }}"
        state: absent
````

## File: roles/mash/playbook_migration/tasks/docker_daemon_options_file_cleanup.yml
````yaml
# SPDX-FileCopyrightText: 2025 Slavi Pantaleev
#
# SPDX-License-Identifier: AGPL-3.0-or-later

---

# ansible-role-docker creates the Docker daemon options file (`/etc/docker/daemon.json`) when options are set
# via `mash_playbook_docker_installation_daemon_options` (which influences the `docker_daemon_options` variable of the role).
# See: https://github.com/geerlingguy/ansible-role-docker/blob/acade8d01f11bcd5efecba6d8211138d7567ce4b/tasks/main.yml#L53-L66
#
# However, it doesn't delete the file when the options list is empty.
#
# This means that people who previously force-disabled IPv6 (and injected `{'ipv6': false}` options, etc)
# or had some other custom options had that file created for them.
# Later, when they stopped setting these options, they were stuck with the configuration file that still retained them.
#
# Here, we make the file go away of no options are set.
# Idealy, this task would be part of the `ansible-role-docker` role, but it's not (yet).
# See: https://github.com/geerlingguy/ansible-role-docker/pull/498
- name: Ensure the Docker daemon options file is deleted when no longer needed
  when: mash_playbook_docker_installation_daemon_options.keys() | length == 0
  ansible.builtin.file:
    path: "{{ mash_playbook_docker_installation_daemon_options_file_path }}"
    state: absent
  notify: restart docker
````

## File: roles/mash/playbook_migration/tasks/main.yml
````yaml
---

- tags:
    - setup-all
    - install-all
  block:
    - ansible.builtin.include_tasks: "{{ role_path }}/tasks/validate_config.yml"

- when: ansible_os_family == 'Debian' and mash_playbook_docker_installation_enabled | bool and mash_playbook_migration_docker_trusted_gpg_d_migration_enabled | bool
  tags:
    - setup-all
    - install-all
    - setup-docker
    - install-docker
  block:
    - ansible.builtin.include_tasks: "{{ role_path }}/tasks/debian_docker_trusted_gpg_d_migration_migration.yml"

- when: mash_playbook_docker_installation_enabled | bool
  tags:
    - setup-all
    - install-all
    - setup-docker
    - install-docker
  block:
    - ansible.builtin.include_tasks: "{{ role_path }}/tasks/docker_daemon_options_file_cleanup.yml"
````

## File: roles/mash/playbook_migration/tasks/validate_config.yml
````yaml
---

- name: (Deprecation) Catch and report devture_postgres_backup variables
  ansible.builtin.fail:
    msg: |-
      The postgres-backup role in the playbook now lives under the MASH organization (https://github.com/mother-of-all-self-hosting/ansible-role-postgres-backup).
      The new role is pretty much the same, but uses differently named variables.

      Please change your configuration (vars.yml) to rename all `devture_postgres_backup_`-prefixed variables (`devture_postgres_backup_*` -> `postgres_backup_*`).

      The following variables in your configuration need to be renamed: {{ vars | dict2items | selectattr('key', 'match', 'devture_postgres_backup_.*') | map (attribute='key') | join(', ') }}
  when: "vars | dict2items | selectattr('key', 'match', 'devture_postgres_backup_.*') | list | items2dict"

- name: (Deprecation) Catch and report devture_postgres variables
  ansible.builtin.fail:
    msg: |-
      The postgres role in the playbook now lives under the MASH organization (https://github.com/mother-of-all-self-hosting/ansible-role-postgres).
      The new role is pretty much the same, but uses differently named variables.

      Please change your configuration (vars.yml) to rename all `devture_postgres_`-prefixed variables (`devture_postgres_*` -> `postgres_*`).

      The following variables in your configuration need to be renamed: {{ vars | dict2items | selectattr('key', 'match', 'devture_postgres_.*') | map (attribute='key') | join(', ') }}
  when: "vars | dict2items | selectattr('key', 'match', 'devture_postgres_.*') | list | items2dict"

- name: (Deprecation) Catch and report traefik_certs_dumper variables
  ansible.builtin.fail:
    msg: |-
      The traefik-certs-dumper role in the playbook now lives under the MASH organization (https://github.com/mother-of-all-self-hosting/ansible-role-traefik-certs-dumper).
      The new role is pretty much the same, but uses differently named variables.

      Please change your configuration (vars.yml) to rename all `devture_traefik_certs_dumper_`-prefixed variables (`devture_traefik_certs_dumper_*` -> `traefik_certs_dumper_*`).

      The following variables in your configuration need to be renamed: {{ vars | dict2items | selectattr('key', 'match', 'devture_traefik_certs_dumper_.*') | map (attribute='key') | join(', ') }}
  when: "vars | dict2items | selectattr('key', 'match', 'devture_traefik_certs_dumper_.*') | list | items2dict"

- name: (Deprecation) Catch and report devture_traefik variables
  ansible.builtin.fail:
    msg: |-
      The traefik role in the playbook now lives under the MASH organization (https://github.com/mother-of-all-self-hosting/ansible-role-traefik).
      The new role is pretty much the same, but uses differently named variables.

      Please change your configuration (vars.yml) to rename all `devture_traefik_`-prefixed variables (`devture_traefik_*` -> `traefik_*`).

      The following variables in your configuration need to be renamed: {{ vars | dict2items | selectattr('key', 'match', 'devture_traefik_.*') | map (attribute='key') | join(', ') }}
  when: "vars | dict2items | selectattr('key', 'match', 'devture_traefik_.*') | list | items2dict"

- name: (Deprecation) Catch and report devture_container_socket_proxy variables
  ansible.builtin.fail:
    msg: |-
      The container-socket-proxy role in the playbook now lives under the MASH organization (https://github.com/mother-of-all-self-hosting/ansible-role-container-socket-proxy).
      The new role is pretty much the same, but uses differently named variables.

      Please change your configuration (vars.yml) to rename all `devture_container_socket_proxy_`-prefixed variables (`devture_container_socket_proxy_*` -> `container_socket_proxy_*`).

      The following variables in your configuration need to be renamed: {{ vars | dict2items | selectattr('key', 'match', 'devture_container_socket_proxy_.*') | map (attribute='key') | join(', ') }}
  when: "vars | dict2items | selectattr('key', 'match', 'devture_container_socket_proxy_.*') | list | items2dict"

- name: (Deprecation) Catch and report devture_woodpecker_server variables
  ansible.builtin.fail:
    msg: |-
      The woodpecker-ci-server role in the playbook now lives under the MASH organization (https://github.com/mother-of-all-self-hosting/ansible-role-woodpecker-ci-server).
      The new role is pretty much the same, but uses differently named variables.

      Please change your configuration (vars.yml) to rename all `devture_woodpecker_ci_server_`-prefixed variables (`devture_woodpecker_ci_server_*` -> `woodpecker_ci_server_*`).

      The following variables in your configuration need to be renamed: {{ vars | dict2items | selectattr('key', 'match', 'devture_woodpecker_ci_server_.*') | map (attribute='key') | join(', ') }}
  when: "vars | dict2items | selectattr('key', 'match', 'devture_woodpecker_ci_server_.*') | list | items2dict"

- name: (Deprecation) Catch and report devture_woodpecker_agent variables
  ansible.builtin.fail:
    msg: |-
      The woodpecker-ci-agent role in the playbook now lives under the MASH organization (https://github.com/mother-of-all-self-hosting/ansible-role-woodpecker-ci-agent).
      The new role is pretty much the same, but uses differently named variables.

      Please change your configuration (vars.yml) to rename all `devture_woodpecker_ci_agent_`-prefixed variables (`devture_woodpecker_ci_agent_*` -> `woodpecker_ci_agent_*`).

      The following variables in your configuration need to be renamed: {{ vars | dict2items | selectattr('key', 'match', 'devture_woodpecker_ci_agent_.*') | map (attribute='key') | join(', ') }}
  when: "vars | dict2items | selectattr('key', 'match', 'devture_woodpecker_ci_agent_.*') | list | items2dict"
````

## File: templates/group_vars_mash_servers
````
---

# role-specific:auxiliary
########################################################################
#                                                                      #
# aux                                                                  #
#                                                                      #
########################################################################

aux_directory_default_owner: "{{ mash_playbook_user_username }}"
aux_directory_default_group: "{{ mash_playbook_user_groupname }}"

aux_file_default_owner: "{{ mash_playbook_user_username }}"
aux_file_default_group: "{{ mash_playbook_user_groupname }}"

########################################################################
#                                                                      #
# /aux                                                                 #
#                                                                      #
########################################################################
# /role-specific:auxiliary


# role-specific:authelia
########################################################################
#                                                                      #
# authelia                                                             #
#                                                                      #
########################################################################

authelia_enabled: false

authelia_identifier: "{{ mash_playbook_service_identifier_prefix }}authelia"

authelia_uid: "{{ mash_playbook_uid }}"
authelia_gid: "{{ mash_playbook_gid }}"

authelia_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}authelia"

authelia_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and authelia_config_storage_postgres_host == postgres_identifier else [])
  }}

authelia_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and authelia_config_storage_postgres_host == postgres_identifier and authelia_container_network != postgres_container_network else [])
  }}

authelia_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
authelia_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
authelia_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
authelia_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

authelia_config_jwt_secret: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'jwt.authelia', rounds=655555) | to_uuid }}"

authelia_config_session_secret: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'ses.authelia', rounds=655555) | to_uuid }}"

authelia_config_identity_providers_oidc_hmac_secret: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'hm.authelia', rounds=655555) | to_uuid }}"

# role-specific:postgres
authelia_config_storage_postgres_host: "{{ postgres_identifier if postgres_enabled else '' }}"
authelia_config_storage_postgres_port: "{{ '5432' if postgres_enabled else '' }}"
authelia_config_storage_postgres_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.authelia', rounds=655555) | to_uuid }}"
# /role-specific:postgres

# role-specific:mariadb
# If Postgres and MariaDB are not enabled, we favor Postgres.
# We only enable MySQL if it's the only enabled component (that is, if Postgres is not enabled at the same time).
authelia_config_storage_mysql_host: "{{ mariadb_identifier if mariadb_enabled and not postgres_enabled | default(false) else '' }}"
authelia_config_storage_mysql_port: "{{ '3306' if mariadb_enabled else '' }}"
authelia_config_storage_mysql_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.authelia', rounds=655555) | to_uuid }}"
# /role-specific:mariadb

########################################################################
#                                                                      #
# /authelia                                                            #
#                                                                      #
########################################################################
# /role-specific:authelia



# role-specific:ssh
########################################################################
#                                                                      #
# ssh                                                                  #
#                                                                      #
########################################################################

system_security_ssh_enabled: false

########################################################################
#                                                                      #
# /ssh                                                                 #
#                                                                      #
########################################################################
# /role-specific:ssh



# role-specific:fail2ban
########################################################################
#                                                                      #
# fail2ban                                                             #
#                                                                      #
########################################################################

system_security_fail2ban_enabled: false

########################################################################
#                                                                      #
# /fail2ban                                                            #
#                                                                      #
########################################################################
# /role-specific:fail2ban



# role-specific:swap
########################################################################
#                                                                      #
# swap                                                                 #
#                                                                      #
########################################################################

system_swap_enabled: false

########################################################################
#                                                                      #
# /swap                                                                #
#                                                                      #
########################################################################
# /role-specific:swap



# role-specific:systemd_service_manager
########################################################################
#                                                                      #
# systemd_service_manager                                              #
#                                                                      #
########################################################################

mash_playbook_devture_systemd_service_manager_services_list_auto_itemized:
  # Dummy entry, which is not role-specific.
  # Ensures there's at least one entry defined in the list.
  - "{{ omit }}"

  # role-specific:backup_borg
  - |-
    {{ ({'name': (backup_borg_identifier + '.timer'), 'priority': 5000, 'groups': ['mash', 'backup', 'borg']} if backup_borg_enabled else omit) }}
  # /role-specific:backup_borg

  # role-specific:adguard_home
  - |-
    {{ ({'name': (adguard_home_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'adguard-home']} if adguard_home_enabled else omit) }}
  # /role-specific:adguard_home

  # role-specific:apisix_dashboard
  - |-
    {{ ({'name': (apisix_dashboard_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'apisix-dashboard']} if apisix_dashboard_enabled else omit) }}
  # /role-specific:apisix_dashboard

  # role-specific:apisix_gateway
  - |-
    {{ ({'name': (apisix_gateway_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'apisix-gateway']} if apisix_gateway_enabled else omit) }}
  # /role-specific:apisix_gateway

  # role-specific:appsmith
  - |-
    {{ ({'name': (appsmith_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'appsmith']} if appsmith_enabled else omit) }}
  # /role-specific:appsmith

  # role-specific:authentik
  - |-
    {{ ({'name': (authentik_server_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'authentik']} if authentik_enabled else omit) }}
  - |-
    {{ ({'name': (authentik_worker_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'authentik']} if authentik_enabled else omit) }}
  # /role-specific:authentik

  # role-specific:authelia
  - |-
    {{ ({'name': (authelia_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'authelia']} if authelia_enabled else omit) }}
  # /role-specific:authelia

  # role-specific:calibre-web
  - |-
    {{ ({'name': (calibre_web_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'calibre-web']} if calibre_web_enabled else omit) }}
  # /role-specific:calibre-web

  # role-specific:readeck
  - |-
    {{ ({'name': (readeck_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'readeck']} if readeck_enabled else omit) }}
  # /role-specific:readeck

  # role-specific:changedetection
  - |-
    {{ ({'name': (changedetection_identifier + '.service'), 'priority': 2100, 'groups': ['mash', 'changedetection']} if changedetection_enabled else omit) }}
  - |-
    {{ ({'name': (changedetection_playwright_driver_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'changedetection']} if changedetection_playwright_driver_enabled else omit) }}
  # /role-specific:changedetection

  # role-specific:wetty
  - |-
    {{ ({'name': (wetty_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'wetty']} if wetty_enabled else omit) }}
  # /role-specific:wetty

  # role-specific:clickhouse
  - |-
    {{ ({'name': (clickhouse_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'clickhouse']} if clickhouse_enabled else omit) }}
  # /role-specific:clickhouse

  # role-specific:collabora_online
  - |-
    {{ ({'name': (collabora_online_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'collabora-online']} if collabora_online_enabled else omit) }}
  # /role-specific:collabora_online

  # role-specific:couchdb
  - |-
    {{ ({'name': (couchdb_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'couchdb']} if couchdb_enabled else omit) }}
  # /role-specific:couchdb

  # role-specific:postgres
  - |-
    {{ ({'name': (postgres_identifier + '.service'), 'priority': 500, 'groups': ['mash', 'postgres']} if postgres_enabled else omit) }}
  # /role-specific:postgres

  # role-specific:postgres_backup
  - |-
    {{ ({'name': (postgres_backup_identifier + '.service'), 'priority': 5000, 'groups': ['mash', 'backup', 'postgres-backup']} if postgres_backup_enabled else omit) }}
  # /role-specific:postgres_backup

  # role-specific:container_socket_proxy
  - |-
    {{ ({'name': (container_socket_proxy_identifier + '.service'), 'priority': 200, 'groups': ['mash', 'reverse-proxies', 'container-socket-proxy']} if container_socket_proxy_enabled else omit) }}
  # /role-specific:container_socket_proxy

  # role-specific:traefik
  - |-
    {{ ({'name': (traefik_identifier + '.service'), 'priority': 250, 'groups': ['mash', 'traefik', 'reverse-proxies']} if traefik_enabled else omit) }}
  # /role-specific:traefik

  # role-specific:woodpecker_ci_server
  - |-
    {{ ({'name': (woodpecker_ci_server_identifier + '.service'), 'priority': 4000, 'groups': ['mash', 'woodpecker', 'ci', 'woodpecker-ci-server']} if woodpecker_ci_server_enabled else omit) }}
  # /role-specific:woodpecker_ci_server

  # role-specific:woodpecker_ci_agent
  - |-
    {{ ({'name': (woodpecker_ci_agent_identifier + '.service'), 'priority': 4100, 'groups': ['mash', 'woodpecker', 'ci', 'woodpecker-ci-agent']} if woodpecker_ci_agent_enabled else omit) }}
  # /role-specific:woodpecker_ci_agent

  # role-specific:docker_registry
  - |-
    {{ ({'name': (docker_registry_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'docker-registry']} if docker_registry_enabled else omit) }}
  - |-
    {{ ({'name': (docker_registry_identifier + '-garbage-collect.timer'), 'priority': 2500, 'groups': ['mash', 'docker-registry', 'docker-registry-gc']} if docker_registry_enabled else omit) }}
  # /role-specific:docker_registry

  # role-specific:docker_registry_proxy
  - |-
    {{ ({'name': (docker_registry_proxy_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'docker-registry-proxy']} if docker_registry_proxy_enabled else omit) }}
  # /role-specific:docker_registry_proxy

  # role-specific:docker_registry_browser
  - |-
    {{ ({'name': (docker_registry_browser_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'docker-registry-browser']} if docker_registry_browser_enabled else omit) }}
  # /role-specific:docker_registry_browser

  # role-specific:docker_registry_purger
  - |-
    {{ ({'name': (docker_registry_purger_identifier + '.timer'), 'priority': 3000, 'groups': ['mash', 'docker-registry-purger']} if docker_registry_purger_enabled else omit) }}
  # /role-specific:docker_registry_purger

  # role-specific:dokuwiki
  - |-
    {{ ({'name': (dokuwiki_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'dokuwiki']} if dokuwiki_enabled else omit) }}
  # /role-specific:dokuwiki

  # role-specific:echoip
  - |-
    {{ ({'name': (echoip_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'echoip']} if echoip_enabled else omit) }}
  # /role-specific:echoip

  # role-specific:endlessh
  - |-
    {{ ({'name': (endlessh_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'endlessh']} if endlessh_enabled else omit) }}
  # /role-specific:endlessh

  # role-specific:etcd
  - |-
    {{ ({'name': (etcd_identifier + '.service'), 'priority': 500, 'groups': ['mash', 'etcd']} if etcd_enabled else omit) }}
  # /role-specific:etcd

  # role-specific:etherpad
  - |-
    {{ ({'name': (etherpad_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'etherpad']} if etherpad_enabled else omit) }}
  # /role-specific:etherpad

  # role-specific:exim_relay
  - |-
    {{ ({'name': (exim_relay_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'exim-relay']} if exim_relay_enabled else omit) }}
  # /role-specific:exim_relay

  # role-specific:firezone
  - |-
    {{ ({'name': (firezone_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'firezone']} if firezone_enabled else omit) }}
  # /role-specific:firezone

  # role-specific:focalboard
  - |-
    {{ ({'name': (focalboard_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'focalboard']} if focalboard_enabled else omit) }}
  # /role-specific:focalboard

  # role-specific:freshrss
  - |-
    {{ ({'name': (freshrss_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'freshrss']} if freshrss_enabled else omit) }}
  # /role-specific:freshrss

  # role-specific:funkwhale
  - |-
    {{ ({'name': (funkwhale_api_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'funkwhale']} if funkwhale_enabled else omit) }}
  - |-
    {{ ({'name': (funkwhale_frontend_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'funkwhale']} if funkwhale_enabled else omit) }}
  - |-
    {{ ({'name': (funkwhale_celery_beat_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'funkwhale']} if funkwhale_enabled else omit) }}
  - |-
    {{ ({'name': (funkwhale_celery_worker_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'funkwhale']} if funkwhale_enabled else omit) }}
  - |-
    {{ ({'name': (funkwhale_watch_imports_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'funkwhale']} if funkwhale_enabled and funkwhale_watch_imports_enabled else omit) }}
  # /role-specific:funkwhale

  # role-specific:gitea
  - |-
    {{ ({'name': (gitea_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'gitea', 'gitea-server']} if gitea_enabled else omit) }}
  # /role-specific:gitea

  # role-specific:gotosocial
  - |-
    {{ ({'name': (gotosocial_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'gotosocial']} if gotosocial_enabled else omit) }}
  # /role-specific:gotosocial

  # role-specific:grafana
  - |-
    {{ ({'name': (grafana_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'grafana']} if grafana_enabled else omit) }}
  # /role-specific:grafana

  # role-specific:hubsite
  - |-
    {{ ({'name': (hubsite_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'hubsite']} if hubsite_enabled else omit) }}
  # /role-specific:hubsite

  # role-specific:headscale
  - |-
    {{ ({'name': (headscale_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'headscale']} if headscale_enabled else omit) }}
  # /role-specific:headscale

  # role-specific:healthchecks
  - |-
    {{ ({'name': (healthchecks_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'healthchecks']} if healthchecks_enabled else omit) }}
  # /role-specific:healthchecks

  # role-specific:ilmo
  - |-
    {{ ({'name': (ilmo_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'ilmo']} if ilmo_enabled else omit) }}
  # /role-specific:ilmo

  # role-specific:infisical
  - |-
    {{ ({'name': (infisical_identifier + '-backend.service'), 'priority': 2000, 'groups': ['mash', 'infisical', 'infisical-backend']} if infisical_enabled else omit) }}
  - |-
    {{ ({'name': (infisical_identifier + '-frontend.service'), 'priority': 2000, 'groups': ['mash', 'infisical', 'infisical-frontend']} if infisical_enabled else omit) }}
  # /role-specific:infisical

  # role-specific:influxdb
  - |-
    {{ ({'name': (influxdb_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'influxdb']} if influxdb_enabled else omit) }}
  # /role-specific:influxdb

  # role-specific:jackett
  - |-
    {{ ({'name': (jackett_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'jackett']} if jackett_enabled else omit) }}
  # /role-specific:jackett

  # role-specific:jitsi
  - |-
    {{ ({'name': (jitsi_identifier + '-web.service'), 'priority': 4200, 'groups': ['mash', 'jitsi', 'jitsi-web']} if jitsi_enabled else omit) }}
  - |-
    {{ ({'name': (jitsi_identifier + '-prosody.service'), 'priority': 4000, 'groups': ['mash', 'jitsi', 'jitsi-prosody']} if jitsi_enabled else omit) }}
  - |-
    {{ ({'name': (jitsi_identifier + '-jicofo.service'), 'priority': 4100, 'groups': ['mash', 'jitsi', 'jitsi-jicofo']} if jitsi_enabled else omit) }}
  - |-
    {{ ({'name': (jitsi_identifier + '-jvb.service'), 'priority': 4100, 'groups': ['mash', 'jitsi', 'jitsi-jvb']} if jitsi_enabled else omit) }}
  # /role-specific:jitsi

  # role-specific:labelstudio
  - |-
    {{ ({'name': (labelstudio_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'labelstudio']} if labelstudio_enabled else omit) }}
  # /role-specific:labelstudio

  # role-specific:keycloak
  - |-
    {{ ({'name': (keycloak_identifier + '.service'), 'priority': 1000, 'groups': ['mash', 'keycloak']} if keycloak_enabled else omit) }}
  # /role-specific:keycloak

  # role-specific:lago
  - |-
    {{ ({'name': (lago_identifier + '-api.service'), 'priority': 2000, 'groups': ['mash', 'lago', 'lago-api']} if lago_enabled else omit) }}
  - |-
    {{ ({'name': (lago_identifier + '-api-worker.service'), 'priority': 2500, 'groups': ['mash', 'lago', 'lago-api-worker']} if lago_enabled else omit) }}
  - |-
    {{ ({'name': (lago_identifier + '-api-clock.service'), 'priority': 2500, 'groups': ['mash', 'lago', 'lago-api-clock']} if lago_enabled else omit) }}
  - |-
    {{ ({'name': (lago_identifier + '-front.service'), 'priority': 2200, 'groups': ['mash', 'lago', 'lago-front']} if lago_enabled else omit) }}
  - |-
    {{ ({'name': (lago_identifier + '-pdf.service'), 'priority': 1900, 'groups': ['mash', 'lago', 'lago-pdf']} if lago_enabled else omit) }}
  # /role-specific:lago

  # role-specific:languagetool
  - |-
    {{ ({'name': (languagetool_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'languagetool']} if languagetool_enabled else omit) }}
  # /role-specific:languagetool

  # role-specific:loki
  - |-
    {{ ({'name': (loki_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'loki']} if loki_enabled else omit) }}
  # /role-specific:loki

  # role-specific:linkding
  - |-
    {{ ({'name': (linkding_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'linkding']} if linkding_enabled else omit) }}
  # /role-specific:linkding

  # role-specific:freescout
  - |-
    {{ ({'name': (freescout_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'freescout']} if freescout_enabled else omit) }}
  # /role-specific:freescout

  # role-specific:miniflux
  - |-
    {{ ({'name': (miniflux_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'miniflux']} if miniflux_enabled else omit) }}
  # /role-specific:miniflux

  # role-specific:mobilizon
  - |-
    {{ ({'name': (mobilizon_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'mobilizon']} if mobilizon_enabled else omit) }}
  # /role-specific:mobilizon

  # role-specific:mongodb
  - |-
    {{ ({'name': (mongodb_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'mongodb']} if mongodb_enabled else omit) }}
  # /role-specific:mongodb

  # role-specific:mosquitto
  - |-
    {{ ({'name': (mosquitto_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'mosquitto']} if mosquitto_enabled else omit) }}
  # /role-specific:mosquitto

  # role-specific:mrs
  - |-
    {{ ({'name': (mrs_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'mrs']} if mrs_enabled else omit) }}
  # /role-specific:mrs

  # role-specific:n8n
  - |-
    {{ ({'name': (n8n_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'n8n']} if n8n_enabled else omit) }}
  # /role-specific:n8n

  # role-specific:navidrome
  - |-
    {{ ({'name': (navidrome_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'navidrome']} if navidrome_enabled else omit) }}
  # /role-specific:navidrome

  # role-specific:neko
  - |-
    {{ ({'name': (neko_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'neko']} if neko_enabled else omit) }}
  # /role-specific:neko

  # role-specific:netbox
  - |-
    {{ ({'name': (netbox_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'netbox', 'netbox-server']} if netbox_enabled else omit) }}
  - |-
    {{ ({'name': (netbox_identifier + '-worker.service'), 'priority': 2500, 'groups': ['mash', 'netbox', 'netbox-worker']} if netbox_enabled else omit) }}
  - |-
    {{ ({'name': (netbox_identifier + '-housekeeping.service'), 'priority': 2500, 'groups': ['mash', 'netbox', 'netbox-housekeeping']} if netbox_enabled else omit) }}
  # /role-specific:netbox

  # role-specific:nextcloud
  - |-
    {{ ({'name': (nextcloud_identifier + '-server.service'), 'priority': 2000, 'groups': ['mash', 'nextcloud', 'nextcloud-server']} if nextcloud_enabled else omit) }}
  - |-
    {{ ({'name': (nextcloud_identifier + '-cron.timer'), 'priority': 2500, 'groups': ['mash', 'nextcloud', 'nextcloud-cron']} if nextcloud_enabled else omit) }}
  - |-
    {{ ({'name': (nextcloud_identifier + '-app-update.timer'), 'priority': 2500, 'groups': ['mash', 'nextcloud', 'nextcloud-app-update']} if nextcloud_enabled and nextcloud_auto_app_update_enabled else omit) }}
  # /role-specific:nextcloud

  # role-specific:notfellchen
  - |-
    {{ ({'name': (notfellchen_identifier  + '.service'), 'priority': 2000, 'groups': ['mash', 'notfellchen']} if notfellchen_enabled else omit) }}
  - |-
    {{ ({'name': (notfellchen_sws_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'notfellchen', 'notfellchen-sws']} if notfellchen_enabled else omit) }}
  - |-
    {{ ({'name': (notfellchen_celery_beat_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'notfellchen']} if notfellchen_enabled else omit) }}
  - |-
    {{ ({'name': (notfellchen_celery_worker_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'notfellchen']} if notfellchen_enabled else omit) }}
  # /role-specific:notfellchen

  # role-specific:ntfy
  - |-
    {{ ({'name': (ntfy_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'ntfy']} if ntfy_enabled else omit) }}
  # /role-specific:ntfy

  # role-specific:mariadb
  - |-
    {{ ({'name': (mariadb_identifier + '.service'), 'priority': 500, 'groups': ['mash', 'mariadb']} if mariadb_enabled else omit) }}
  # /role-specific:mariadb

  # role-specific:oauth2_proxy
  - |-
    {{ ({'name': (oauth2_proxy_identifier + '.service'), 'priority': 1900, 'groups': ['mash', 'oauth2-proxy']} if oauth2_proxy_enabled else omit) }}
  # /role-specific:oauth2_proxy

  # role-specific:overseerr
  - |-
    {{ ({'name': (overseerr_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'overseerr']} if overseerr_enabled else omit) }}
  # /role-specific:overseerr

  # role-specific:outline
  - |-
    {{ ({'name': (outline_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'outline']} if outline_enabled else omit) }}
  # /role-specific:outline

  # role-specific:owncast
  - |-
    {{ ({'name': (owncast_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'owncast']} if owncast_enabled else omit) }}
  # /role-specific:owncast

  # role-specific:oxitraffic
  - |-
    {{ ({'name': (oxitraffic_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'oxitraffic']} if oxitraffic_enabled else omit) }}
  # /role-specific:oxitraffic

  # role-specific:paperless
  - |-
    {{ ({'name': (paperless_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'paperless']} if paperless_enabled else omit) }}
  - |-
    {{ ({'name': (paperless_tika_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'paperless']} if paperless_enabled and paperless_tika_enabled else omit) }}
  - |-
    {{ ({'name': (paperless_gotenberg_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'paperless']} if paperless_enabled and paperless_gotenberg_enabled else omit) }}
  # /role-specific:paperless

  # role-specific:peertube
  - |-
    {{ ({'name': (peertube_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'peertube']} if peertube_enabled else omit) }}
  # /role-specific:peertube

  # role-specific:postgis
  - |-
    {{ ({'name': (postgis_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'metrics', 'postgis']} if postgis_enabled else omit) }}
  # /role-specific:postgis

  # role-specific:plausible
  - |-
    {{ ({'name': (plausible_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'plausible']} if plausible_enabled else omit) }}
  # /role-specific:plausible

  # role-specific:prometheus
  - |-
    {{ ({'name': (prometheus_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'metrics', 'prometheus']} if prometheus_enabled else omit) }}
  # /role-specific:prometheus

  # role-specific:prometheus_blackbox_exporter
  - |-
    {{ ({'name': (prometheus_blackbox_exporter_identifier + '.service'), 'priority': 500, 'groups': ['mash', 'metrics', 'prometheus-blackbox-exporter']} if prometheus_blackbox_exporter_enabled else omit) }}
  # /role-specific:prometheus_blackbox_exporter

  # role-specific:prometheus_ssh_exporter
  - |-
    {{ ({'name': (prometheus_ssh_exporter_identifier + '.service'), 'priority': 500, 'groups': ['mash', 'metrics', 'prometheus-ssh-exporter']} if prometheus_ssh_exporter_enabled else omit) }}
  # /role-specific:prometheus_ssh_exporter

  # role-specific:prometheus_node_exporter
  - |-
    {{ ({'name': (prometheus_node_exporter_identifier + '.service'), 'priority': 500, 'groups': ['mash', 'metrics', 'prometheus-node-exporter']} if prometheus_node_exporter_enabled else omit) }}
  # /role-specific:prometheus_node_exporter

  # role-specific:prometheus_postgres_exporter
  - |-
    {{ ({'name': (prometheus_postgres_exporter_identifier + '.service'), 'priority': 500, 'groups': ['mash', 'metrics', 'prometheus-postgres-exporter']} if prometheus_postgres_exporter_enabled else omit) }}
  # /role-specific:prometheus_postgres_exporter

  # role-specific:promtail
  - |-
    {{ ({'name': (promtail_identifier + '.service'), 'priority': 500, 'groups': ['mash', 'logs', 'promtail']} if promtail_enabled else omit) }}
  # /role-specific:promtail

  # role-specific:qbittorrent
  - |-
    {{ ({'name': (qbittorrent_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'qbittorrent']} if qbittorrent_enabled else omit) }}
  # /role-specific:qbittorrent

  # role-specific:radarr
  - |-
    {{ ({'name': (radarr_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'radarr']} if radarr_enabled else omit) }}
  # /role-specific:radarr

  # role-specific:radicale
  - |-
    {{ ({'name': (radicale_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'radicale']} if radicale_enabled else omit) }}
  # /role-specific:radicale

  # role-specific:redmine
  - |-
    {{ ({'name': (redmine_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'redmine']} if redmine_enabled else omit) }}
  - |-
    {{ ({'name': (redmine_identifier + '-send-reminders.timer'), 'priority': 2000, 'groups': ['mash', 'redmine']} if redmine_enabled else omit) }}
  - |-
    {{ ({'name': (redmine_identifier + '-recurring-tasks.timer'), 'priority': 2000, 'groups': ['mash', 'redmine']} if redmine_enabled and redmine_recurring_tasks_enabled else omit) }}
  # /role-specific:redmine

  # role-specific:redis
  - |-
    {{ ({'name': (redis_identifier + '.service'), 'priority': 750, 'groups': ['mash', 'redis']} if redis_enabled else omit) }}
  # /role-specific:redis

  # role-specific:keydb
  - |-
    {{ ({'name': (keydb_identifier + '.service'), 'priority': 750, 'groups': ['mash', 'keydb']} if keydb_enabled else omit) }}
  # /role-specific:keydb

  # role-specific:roundcube
  - |-
    {{ ({'name': (roundcube_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'roundcube']} if roundcube_enabled else omit) }}
  # /role-specific:roundcube

  # role-specific:rumqttd
  - |-
    {{ ({'name': (rumqttd_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'rumqttd']} if rumqttd_enabled else omit) }}
  # /role-specific:rumqttd

  # role-specific:searxng
  - |-
    {{ ({'name': (searxng_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'searxng']} if searxng_enabled else omit) }}
  # /role-specific:searxng

  # role-specific:semaphore
  - |-
    {{ ({'name': (semaphore_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'semaphore']} if semaphore_enabled else omit) }}
  # /role-specific:semaphore

  # role-specific:soft_serve
  - |-
    {{ ({'name': (soft_serve_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'soft-serve']} if soft_serve_enabled else omit) }}
  # /role-specific:soft_serve

  # role-specific:sonarr
  - |-
    {{ ({'name': (sonarr_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'sonarr']} if sonarr_enabled else omit) }}
  # /role-specific:sonarr

  # role-specific:stirling_pdf
  - |-
    {{ ({'name': (stirling_pdf_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'stirling_pdf']} if stirling_pdf_enabled else omit) }}
  # /role-specific:stirling_pdf

  # role-specific:syncthing
  - |-
    {{ ({'name': (syncthing_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'syncthing']} if syncthing_enabled else omit) }}
  # /role-specific:syncthing

  # role-specific:tandoor
  - |-
    {{ ({'name': (tandoor_api_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'tandoor']} if tandoor_enabled else omit) }}
  - |-
    {{ ({'name': (tandoor_frontend_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'tandoor']} if tandoor_enabled else omit) }}
  # /role-specific:tandoor

  # role-specific:telegraf
  - |-
    {{ ({'name': (telegraf_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'telegraf']} if telegraf_enabled else omit) }}
  # /role-specific:telegraf

  # role-specific:valkey
  - |-
    {{ ({'name': (valkey_identifier + '.service'), 'priority': 750, 'groups': ['mash', 'valkey']} if valkey_enabled else omit) }}
  # /role-specific:valkey

  # role-specific:vaultwarden
  - |-
    {{ ({'name': (vaultwarden_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'vaultwarden', 'vaultwarden-server']} if vaultwarden_enabled else omit) }}
  # /role-specific:vaultwarden

  # role-specific:uptime_kuma
  - |-
    {{ ({'name': (uptime_kuma_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'uptime-kuma']} if uptime_kuma_enabled else omit) }}
  # /role-specific:uptime_kuma

  # role-specific:wg_easy
  - |-
    {{ ({'name': (wg_easy_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'wg-easy']} if wg_easy_enabled else omit) }}
  # /role-specific:wg_easy

  # role-specific:wordpress
  - |-
    {{ ({'name': (wordpress_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'wordpress']} if wordpress_enabled else omit) }}
  # /role-specific:wordpress

  # role-specific:forgejo
  - |-
    {{ ({'name': (forgejo_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'forgejo', 'forgejo-server']} if forgejo_enabled else omit) }}
  # /role-specific:forgejo

  # role-specific:forgejo_runner
  - |-
    {{ ({'name': (forgejo_runner_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'forgejo-runner']} if forgejo_runner_enabled else omit) }}
  # /role-specific:forgejo_runner

  # role-specific:tsdproxy
  - |-
    {{ ({'name': (tsdproxy_identifier + '.service'), 'priority': 2000, 'groups': ['mash', 'tsdproxy']} if tsdproxy_enabled else omit) }}
  # /role-specific:tsdproxy

  # role-specific:writefreely
  - |-
    {{ ({'name': (writefreely_identifier  + '.service'), 'priority': 2000, 'groups': ['mash', 'writefreely']} if writefreely_enabled else omit) }}
  # /role-specific:writefreely


devture_systemd_service_manager_services_list_auto: "{{ mash_playbook_devture_systemd_service_manager_services_list_auto_itemized | reject('equalto', omit) }}"

########################################################################
#                                                                      #
# /systemd_service_manager                                             #
#                                                                      #
########################################################################
# /role-specific:systemd_service_manager


# role-specific:postgres
########################################################################
#                                                                      #
# postgres                                                             #
#                                                                      #
########################################################################

postgres_enabled: false

postgres_identifier: "{{ mash_playbook_service_identifier_prefix }}postgres"

postgres_architecture: "{{ mash_playbook_architecture }}"

postgres_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}postgres"

postgres_uid: "{{ mash_playbook_uid }}"
postgres_gid: "{{ mash_playbook_gid }}"

# This includes everything for maximum safety.
# It may not be optimal though, because some services may not be dependant on Postgres at all, etc.
postgres_systemd_services_to_stop_for_maintenance_list_auto: "{{ devture_systemd_service_manager_services_list_auto | map(attribute='name') | reject('equalto', (postgres_identifier + '.service')) }}"

mash_playbook_postgres_managed_databases_auto_itemized:
  # Dummy entry, which is not role-specific.
  # Ensures there's at least one entry defined in the list.
  - "{{ omit }}"

  # role-specific:authelia
  - |-
    {{
      ({
        'name': authelia_config_storage_postgres_database,
        'username': authelia_config_storage_postgres_username,
        'password': authelia_config_storage_postgres_password,
      } if authelia_enabled and authelia_config_storage_postgres_host == postgres_identifier else omit)
    }}
  # /role-specific:authelia

  # role-specific:authentik
  - |-
    {{
      ({
        'name': authentik_database_name,
        'username': authentik_database_username,
        'password': authentik_database_password,
      } if authentik_enabled and authentik_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:authentik

  # role-specific:etherpad
  - |-
    {{
      ({
        'name': etherpad_database_name,
        'username': etherpad_database_username,
        'password': etherpad_database_password,
      } if etherpad_enabled else omit)
    }}
  # /role-specific:etherpad

  # role-specific:focalboard
  - |-
    {{
      ({
        'name': focalboard_database_name,
        'username': focalboard_database_username,
        'password': focalboard_database_password,
      } if focalboard_enabled and focalboard_database_type == 'postgres' and focalboard_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:focalboard

  # role-specific:freshrss
  - |-
    {{
      ({
        'name': freshrss_database_name,
        'username': freshrss_database_username,
        'password': freshrss_database_password,
      } if freshrss_enabled and freshrss_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:freshrss

  # role-specific:funkwhale
  - |-
    {{
      ({
        'name': funkwhale_database_name,
        'username': funkwhale_database_username,
        'password': funkwhale_database_password,
      } if funkwhale_enabled and funkwhale_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:funkwhale

  # role-specific:gitea
  - |-
    {{
      ({
        'name': gitea_config_database_name,
        'username': gitea_config_database_username,
        'password': gitea_config_database_password,
      } if gitea_enabled else omit)
    }}
  # /role-specific:gitea

  # role-specific:healthchecks
  - |-
    {{
      ({
        'name': healthchecks_database_name,
        'username': healthchecks_database_username,
        'password': healthchecks_database_password,
      } if healthchecks_enabled and healthchecks_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:healthchecks

  # role-specific:woodpecker_ci_server
  - |-
    {{
      ({
        'name': woodpecker_ci_server_database_datasource_db_name,
        'username': woodpecker_ci_server_database_datasource_username,
        'password': woodpecker_ci_server_database_datasource_password,
      } if woodpecker_ci_server_enabled else omit)
    }}
  # /role-specific:woodpecker_ci_server

  # role-specific:gotosocial
  - |-
    {{
      ({
        'name': gotosocial_database_name,
        'username': gotosocial_database_username,
        'password': gotosocial_database_password,
      } if gotosocial_enabled else omit)
    }}
  # /role-specific:gotosocial

  # role-specific:ilmo
  - |-
    {{
      ({
        'name': ilmo_database_name,
        'username': ilmo_database_username,
        'password': ilmo_database_password,
      } if ilmo_enabled else omit)
    }}
  # /role-specific:ilmo

  # role-specific:keycloak
  - |-
    {{
      ({
        'name': keycloak_database_name,
        'username': keycloak_database_username,
        'password': keycloak_database_password,
      } if keycloak_enabled and keycloak_database_type == 'postgres' and keycloak_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:keycloak

  # role-specific:labelstudio
  - |-
    {{
      ({
        'name': labelstudio_database_name,
        'username': labelstudio_database_username,
        'password': labelstudio_database_password,
      } if labelstudio_enabled and labelstudio_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:labelstudio

  # role-specific:lago
  - |-
    {{
      ({
        'name': lago_database_name,
        'username': lago_database_username,
        'password': lago_database_password,
      } if lago_enabled and lago_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:lago

  # role-specific:linkding
  - |-
    {{
      ({
        'name': linkding_database_name,
        'username': linkding_database_username,
        'password': linkding_database_password,
      } if linkding_enabled and linkding_database_engine == 'postgres' else omit)
    }}
  # /role-specific:linkding

  # role-specific:freescout
  - |-
    {{
      ({
        'name': freescout_database_name,
        'username': freescout_database_username,
        'password': freescout_database_password,
      } if freescout_enabled else omit)
    }}
  # /role-specific:freescout

  # role-specific:miniflux
  - |-
    {{
      ({
        'name': miniflux_database_name,
        'username': miniflux_database_username,
        'password': miniflux_database_password,
      } if miniflux_enabled else omit)
    }}
  # /role-specific:miniflux

  # role-specific:redmine
  - |-
    {{
      ({
        'name': redmine_database_name,
        'username': redmine_database_username,
        'password': redmine_database_password,
      } if redmine_enabled else omit)
    }}
  # /role-specific:redmine

  # role-specific:n8n
  - |-
    {{
      ({
        'name': n8n_database_name,
        'username': n8n_database_username,
        'password': n8n_database_password,
      } if n8n_enabled else omit)
    }}
  # /role-specific:n8n

  # role-specific:netbox
  - |-
    {{
      ({
        'name': netbox_database_name,
        'username': netbox_database_username,
        'password': netbox_database_password,
      } if netbox_enabled else omit)
    }}
  # /role-specific:netbox

  # role-specific:nextcloud
  - |-
    {{
      ({
        'name': nextcloud_database_name,
        'username': nextcloud_database_username,
        'password': nextcloud_database_password,
      } if nextcloud_enabled else omit)
    }}
  # /role-specific:nextcloud

  # role-specific:notfellchen
  - |-
    {{
      ({
        'name': notfellchen_database_name,
        'username': notfellchen_database_username,
        'password': notfellchen_database_password,
      } if notfellchen_enabled else omit)
    }}
  # /role-specific:notfellchen

  # role-specific:outline
  - |-
    {{
      ({
        'name': outline_database_name,
        'username': outline_database_username,
        'password': outline_database_password,
      } if outline_enabled and outline_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:outline

  # role-specific:oxitraffic
  - |-
    {{
      ({
        'name': oxitraffic_database_name,
        'username': oxitraffic_database_username,
        'password': oxitraffic_database_password,
      } if oxitraffic_enabled and oxitraffic_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:oxitraffic


  # role-specific:paperless
  - |-
    {{
      ({
        'name': paperless_database_name,
        'username': paperless_database_username,
        'password': paperless_database_password,
      } if paperless_enabled and paperless_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:paperless

  # role-specific:peertube
  - |-
    {{
      ({
        'name': peertube_config_database_name,
        'username': peertube_config_database_username,
        'password': peertube_config_database_password,
      } if peertube_enabled else omit)
    }}
  # /role-specific:peertube

  # role-specific:plausible
  - |-
    {{
      ({
        'name': plausible_database_name,
        'username': plausible_database_username,
        'password': plausible_database_password,
      } if plausible_enabled else omit)
    }}
  # /role-specific:plausible

  # role-specific:prometheus_postgres_exporter
  - |-
    {{
      ({
        'name': prometheus_postgres_exporter_database_name,
        'username': prometheus_postgres_exporter_database_username,
        'password': prometheus_postgres_exporter_database_password,
      } if prometheus_postgres_exporter_enabled else omit)
    }}
  # /role-specific:prometheus_postgres_exporter

  # role-specific:firezone
  - |-
    {{
      ({
        'name': firezone_database_name,
        'username': firezone_database_username,
        'password': firezone_database_password,
      } if firezone_enabled else omit)
    }}
  # /role-specific:firezone

  # role-specific:vaultwarden
  - |-
    {{
      ({
        'name': vaultwarden_database_name,
        'username': vaultwarden_database_username,
        'password': vaultwarden_database_password,
      } if vaultwarden_enabled else omit)
    }}
  # /role-specific:vaultwarden

  # role-specific:forgejo
  - |-
    {{
      ({
        'name': forgejo_config_database_name,
        'username': forgejo_config_database_username,
        'password': forgejo_config_database_password,
      } if forgejo_enabled else omit)
    }}
  # /role-specific:forgejo

  # role-specific:roundcube
  - |-
    {{
      ({
        'name': roundcube_database_name,
        'username': roundcube_database_username,
        'password': roundcube_database_password,
      } if roundcube_enabled and roundcube_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:roundcube

  # role-specific:semaphore
  - |-
    {{
      ({
        'name': semaphore_database_name,
        'username': semaphore_database_username,
        'password': semaphore_database_password,
      } if semaphore_enabled and semaphore_database_host == postgres_identifier else omit)
    }}
  # /role-specific:semaphore

  # role-specific:tandoor
  - |-
    {{
      ({
        'name': tandoor_database_name,
        'username': tandoor_database_username,
        'password': tandoor_database_password,
      } if tandoor_enabled and tandoor_database_hostname == postgres_identifier else omit)
    }}
  # /role-specific:tandoor

postgres_managed_databases_auto: "{{ mash_playbook_postgres_managed_databases_auto_itemized | reject('equalto', omit) }}"

########################################################################
#                                                                      #
# /postgres                                                            #
#                                                                      #
########################################################################
# /role-specific:postgres


# role-specific:postgres_backup
########################################################################
#                                                                      #
# postgres_backup                                                      #
#                                                                      #
########################################################################

postgres_backup_enabled: false

postgres_backup_postgres_role_include_name: galaxy/postgres

postgres_backup_identifier: "{{ mash_playbook_service_identifier_prefix }}postgres-backup"

postgres_backup_architecture: "{{ mash_playbook_architecture }}"

postgres_backup_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}postgres-backup"

postgres_backup_uid: "{{ mash_playbook_uid }}"
postgres_backup_gid: "{{ mash_playbook_gid }}"

# role-specific:postgres
postgres_backup_systemd_required_services_list_auto: |
  {{
    ([(postgres_identifier + '.service')] if (postgres_enabled and postgres_backup_connection_hostname == postgres_connection_hostname) else [])
  }}

postgres_backup_container_network: "{{ (postgres_container_network if (postgres_enabled and postgres_backup_connection_hostname == postgres_connection_hostname) else postgres_backup_identifier) }}"

postgres_backup_container_additional_networks_auto: |-
  {{
      ([postgres_container_network] if (postgres_enabled and postgres_backup_connection_hostname == postgres_connection_hostname and postgres_backup_container_network != postgres_container_network) else [])
  }}

postgres_backup_connection_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
postgres_backup_connection_port: "{{ postgres_connection_port if postgres_enabled else 5432 }}"
postgres_backup_connection_username: "{{ postgres_connection_username if postgres_enabled else '' }}"
postgres_backup_connection_password: "{{ postgres_connection_password if postgres_enabled else '' }}"

postgres_backup_postgres_data_path: "{{ postgres_data_path if postgres_enabled else '' }}"

postgres_backup_databases_auto: "{{ postgres_managed_databases | map(attribute='name') if postgres_enabled else [] }}"
# /role-specific:postgres

########################################################################
#                                                                      #
# /postgres_backup                                                     #
#                                                                      #
########################################################################
# /role-specific:postgres_backup


# role-specific:playbook_state_preserver
########################################################################
#                                                                      #
# playbook_state_preserver                                             #
#                                                                      #
########################################################################

# To completely disable this feature, use `devture_playbook_state_preserver_enabled: false`.

devture_playbook_state_preserver_uid: "{{ mash_playbook_uid }}"
devture_playbook_state_preserver_gid: "{{ mash_playbook_gid }}"

devture_playbook_state_preserver_vars_preservation_dst: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}vars.yml"

devture_playbook_state_preserver_commit_hash_preservation_dst: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}git_hash.yml"

########################################################################
#                                                                      #
# /playbook_state_preserver                                            #
#                                                                      #
########################################################################
# /role-specific:playbook_state_preserver



########################################################################
#                                                                      #
# geerlingguy/ansible-role-docker                                      #
#                                                                      #
########################################################################

docker_daemon_options: "{{ mash_playbook_docker_installation_daemon_options }}"

########################################################################
#                                                                      #
# /geerlingguy/ansible-role-docker                                     #
#                                                                      #
########################################################################



# role-specific:container_socket_proxy
########################################################################
#                                                                      #
# container_socket_proxy                                               #
#                                                                      #
########################################################################

container_socket_proxy_enabled: "{{ traefik_enabled }}"

container_socket_proxy_identifier: "{{ mash_playbook_service_identifier_prefix }}container-socket-proxy"

container_socket_proxy_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}container-socket-proxy"

container_socket_proxy_uid: "{{ mash_playbook_uid }}"
container_socket_proxy_gid: "{{ mash_playbook_gid }}"

# Traefik requires read access to the containers APIs to do its job
container_socket_proxy_api_containers_enabled: true

########################################################################
#                                                                      #
# /container_socket_proxy                                              #
#                                                                      #
########################################################################
# /role-specific:container_socket_proxy



# role-specific:traefik
########################################################################
#                                                                      #
# traefik                                                              #
#                                                                      #
########################################################################

traefik_enabled: "{{ mash_playbook_reverse_proxy_type == 'playbook-managed-traefik' }}"

traefik_identifier: "{{ mash_playbook_service_identifier_prefix }}traefik"

traefik_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}traefik"

traefik_uid: "{{ mash_playbook_uid }}"
traefik_gid: "{{ mash_playbook_gid }}"

# role-specific:container_socket_proxy
traefik_config_providers_docker_endpoint: "{{ container_socket_proxy_endpoint if container_socket_proxy_enabled else 'unix:///var/run/docker.sock' }}"
# /role-specific:container_socket_proxy

traefik_container_additional_networks_auto: |
  {{
    ([container_socket_proxy_container_network] if container_socket_proxy_enabled | default(false) else [])
  }}

traefik_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([container_socket_proxy_identifier + '.service'] if container_socket_proxy_enabled | default(false) else [])
  }}

########################################################################
#                                                                      #
# /traefik                                                             #
#                                                                      #
########################################################################
# /role-specific:traefik



# role-specific:docker_sdk_for_python
########################################################################
#                                                                      #
# docker_sdk_for_python                                                #
#                                                                      #
########################################################################

devture_docker_sdk_for_python_installation_enabled: false

########################################################################
#                                                                      #
# /docker_sdk_for_python                                               #
#                                                                      #
########################################################################
# /role-specific:docker_sdk_for_python



# role-specific:timesync
########################################################################
#                                                                      #
# timesync                                                             #
#                                                                      #
########################################################################

# To completely disable installing systemd-timesyncd/ntpd, use `devture_timesync_installation_enabled: false`.

devture_timesync_installation_enabled: false

########################################################################
#                                                                      #
# /timesync                                                            #
#                                                                      #
########################################################################
# /role-specific:timesync



# role-specific:adguard_home
########################################################################
#                                                                      #
# adguard-home                                                         #
#                                                                      #
########################################################################

adguard_home_enabled: false

adguard_home_identifier: "{{ mash_playbook_service_identifier_prefix }}adguard-home"

adguard_home_uid: "{{ mash_playbook_uid }}"
adguard_home_gid: "{{ mash_playbook_gid }}"

adguard_home_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}adguard-home"

adguard_home_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

adguard_home_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
adguard_home_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
adguard_home_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
adguard_home_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /adguard-home                                                        #
#                                                                      #
########################################################################
# /role-specific:adguard_home



# role-specific:apisix_dashboard
########################################################################
#                                                                      #
# apisix_dashboard                                                       #
#                                                                      #
########################################################################

apisix_dashboard_enabled: false

apisix_dashboard_identifier: "{{ mash_playbook_service_identifier_prefix }}apisix-dashboard"

apisix_dashboard_uid: "{{ mash_playbook_uid }}"
apisix_dashboard_gid: "{{ mash_playbook_gid }}"

apisix_dashboard_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}apisix-dashboard"

apisix_dashboard_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if (mash_playbook_reverse_proxyable_services_additional_network and apisix_dashboard_container_labels_traefik_enabled) else [])
  }}

apisix_dashboard_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
apisix_dashboard_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
apisix_dashboard_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
apisix_dashboard_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

# role-specific:etcd
apisix_dashboard_config_conf_etcd_endpoints: |
  {{
    ([(etcd_identifier + ':2379')] if etcd_enabled else [])
  }}

apisix_dashboard_config_conf_etcd_username: "{{ ('root' if (etcd_enabled and not etcd_environment_variable_allow_none_authentication) else '') }}"
apisix_dashboard_config_conf_etcd_password: "{{ (etcd_environment_variable_etcd_root_password if (etcd_enabled and not etcd_environment_variable_allow_none_authentication) else '') }}"

apisix_dashboard_container_additional_networks_custom: |
  {{
    ([etcd_container_network] if etcd_enabled else [])
  }}

apisix_dashboard_systemd_required_systemd_services_list_auto: |
  {{
    ([(etcd_identifier + '.service')] if etcd_enabled else [])
  }}
# /role-specific:etcd

########################################################################
#                                                                      #
# /apisix_dashboard                                                      #
#                                                                      #
########################################################################
# /role-specific:apisix_dashboard



# role-specific:apisix_gateway
########################################################################
#                                                                      #
# apisix_gateway                                                       #
#                                                                      #
########################################################################

apisix_gateway_enabled: false

apisix_gateway_identifier: "{{ mash_playbook_service_identifier_prefix }}apisix-gateway"

apisix_gateway_uid: "{{ mash_playbook_uid }}"
apisix_gateway_gid: "{{ mash_playbook_gid }}"

apisix_gateway_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}apisix-gateway"

apisix_gateway_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if (mash_playbook_reverse_proxyable_services_additional_network and apisix_gateway_container_labels_traefik_enabled) else [])
  }}

apisix_gateway_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
apisix_gateway_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
apisix_gateway_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
apisix_gateway_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

apisix_gateway_container_labels_metrics_enabled: "{{ prometheus_enabled | default(false) or mash_playbook_metrics_exposure_enabled }}"
apisix_gateway_container_labels_metrics_hostname: "{{ mash_playbook_metrics_exposure_hostname }}"
apisix_gateway_container_labels_metrics_path_prefix: "{{ mash_playbook_metrics_exposure_path_prefix }}/{{ apisix_gateway_identifier }}"
apisix_gateway_container_labels_metrics_middleware_basic_auth_enabled: "{{ mash_playbook_metrics_exposure_http_basic_auth_enabled }}"
apisix_gateway_container_labels_metrics_middleware_basic_auth_users: "{{ mash_playbook_metrics_exposure_http_basic_auth_users }}"

# role-specific:etcd
apisix_gateway_config_deployment_etcd_host: |
  {{
    ([('http://' + etcd_identifier + ':2379')] if etcd_enabled else [])
  }}

apisix_gateway_config_deployment_etcd_user: "{{ ('root' if (etcd_enabled and not etcd_environment_variable_allow_none_authentication) else '') }}"
apisix_gateway_config_deployment_etcd_password: "{{ (etcd_environment_variable_etcd_root_password if (etcd_enabled and not etcd_environment_variable_allow_none_authentication) else '') }}"

apisix_gateway_container_additional_networks_custom: |
  {{
    ([etcd_container_network] if etcd_enabled else [])
  }}

apisix_gateway_systemd_required_systemd_services_list_auto: |
  {{
    ([(etcd_identifier + '.service')] if etcd_enabled else [])
  }}
# /role-specific:etcd

########################################################################
#                                                                      #
# /apisix_gateway                                                      #
#                                                                      #
########################################################################
# /role-specific:apisix_gateway



# role-specific:appsmith
########################################################################
#                                                                      #
# appsmith                                                             #
#                                                                      #
########################################################################

appsmith_enabled: false

appsmith_identifier: "{{ mash_playbook_service_identifier_prefix }}appsmith"

appsmith_uid: "{{ mash_playbook_uid }}"
appsmith_gid: "{{ mash_playbook_gid }}"

appsmith_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}appsmith"

appsmith_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

appsmith_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
appsmith_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
appsmith_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
appsmith_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /appsmith                                                            #
#                                                                      #
########################################################################
# /role-specific:appsmith



# role-specific:authentik
########################################################################
#                                                                      #
# authentik                                                            #
#                                                                      #
########################################################################

authentik_enabled: false

authentik_identifier: "{{ mash_playbook_service_identifier_prefix }}authentik"

authentik_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}authentik"

authentik_uid: "{{ mash_playbook_uid }}"
authentik_gid: "{{ mash_playbook_gid }}"

# role-specific:postgres
authentik_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
authentik_database_port: "{{ '5432' if postgres_enabled else '' }}"
authentik_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.authentik', rounds=655555) | to_uuid }}"
authentik_database_username: "{{ authentik_identifier }}"
# /role-specific:postgres

authentik_server_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and authentik_database_hostname == postgres_identifier else [])
  }}

authentik_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and authentik_database_hostname == postgres_identifier and authentik_container_network != postgres_container_network else [])
  }}

authentik_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
authentik_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
authentik_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
authentik_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /authentik                                                           #
#                                                                      #
########################################################################
# /role-specific:authentik



# role-specific:backup_borg
########################################################################
#                                                                      #
# backup-borg                                                          #
#                                                                      #
########################################################################

backup_borg_enabled: false

backup_borg_identifier: "{{ mash_playbook_service_identifier_prefix }}backup-borg"

backup_borg_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}borg-backup"

backup_borg_uid: "{{ mash_playbook_uid }}"
backup_borg_gid: "{{ mash_playbook_gid }}"

backup_borg_container_network: "{{ postgres_container_network if postgres_enabled else backup_borg_identifier }}"

backup_borg_retention_prefix: "{{ mash_playbook_service_identifier_prefix }}"
backup_borg_storage_archive_name_format: "{{ mash_playbook_service_identifier_prefix }}-{now:%Y-%m-%d-%H%M%S}"

backup_borg_container_image_self_build: "{{ mash_playbook_architecture not in ['amd64', 'arm32', 'arm64'] }}"

# role-specific:postgres
backup_borg_postgresql_enabled: "{{ postgres_enabled }}"
backup_borg_postgresql_databases_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
backup_borg_postgresql_databases_username: "{{ postgres_connection_username if postgres_enabled else '' }}"
backup_borg_postgresql_databases_password: "{{ postgres_connection_password if postgres_enabled else '' }}"
backup_borg_postgresql_databases_port: "{{ postgres_connection_port if postgres_enabled else 5432 }}"
backup_borg_postgresql_databases_auto: "{{ postgres_managed_databases | map(attribute='name') if postgres_enabled else [] }}"
# /role-specific:postgres

# role-specific:mariadb
backup_borg_mysql_enabled: "{{ mariadb_enabled }}"
backup_borg_mysql_databases_hostname: "{{ mariadb_identifier if mariadb_enabled else '' }}"
backup_borg_mysql_databases_username: "root"
backup_borg_mysql_databases_password: "{{ mariadb_root_password if mariadb_enabled else '' }}"
backup_borg_mysql_databases_port: 3306
backup_borg_mysql_databases: "{{ mariadb_managed_databases | map(attribute='name') if mariadb_enabled else [] }}"
# /role-specific:mariadb

backup_borg_location_source_directories:
  - "{{ mash_playbook_base_path }}"

backup_borg_location_exclude_patterns: |
  {{
    ([postgres_data_path] if postgres_enabled | default(false) else [])
    +
    ([mariadb_data_path] if mariadb_enabled | default(false) else [])
  }}

backup_borg_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled | default(false) else [])
    +
    ([mariadb_identifier ~ '.service'] if mariadb_enabled | default(false) else [])
  }}

########################################################################
#                                                                      #
# /backup-borg                                                         #
#                                                                      #
########################################################################
# /role-specific:backup_borg



# role-specific:changedetection
########################################################################
#                                                                      #
# Changedetection.io                                                   #
#                                                                      #
########################################################################

changedetection_enabled: false

changedetection_identifier: "{{ mash_playbook_service_identifier_prefix }}changedetection"

changedetection_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}changedetection"

changedetection_uid: "{{ mash_playbook_uid }}"
changedetection_gid: "{{ mash_playbook_gid }}"

changedetection_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

changedetection_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
changedetection_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
changedetection_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
changedetection_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /Changedetection.io                                                  #
#                                                                      #
########################################################################
# /role-specific:changedetection



# role-specific:wetty
########################################################################
#                                                                      #
# wetty                                                                #
#                                                                      #
########################################################################

wetty_enabled: false

wetty_identifier: "{{ mash_playbook_service_identifier_prefix }}wetty"

wetty_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}wetty"

wetty_uid: "{{ mash_playbook_uid }}"
wetty_gid: "{{ mash_playbook_gid }}"

wetty_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

wetty_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
wetty_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
wetty_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
wetty_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /wetty                                                               #
#                                                                      #
########################################################################
# /role-specific:wetty



# role-specific:calibre-web
########################################################################
#                                                                      #
# calibre-web                                                            #
#                                                                      #
########################################################################

calibre_web_enabled: false

calibre_web_identifier: "{{ mash_playbook_service_identifier_prefix }}calibre-web"

calibre_web_uid: "{{ mash_playbook_uid }}"
calibre_web_gid: "{{ mash_playbook_gid }}"

calibre_web_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}calibre-web"

calibre_web_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

calibre_web_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
calibre_web_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
calibre_web_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
calibre_web_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /calibre-web                                                           #
#                                                                      #
########################################################################
# /role-specific:calibre-web



# role-specific:readeck
########################################################################
#                                                                      #
# readeck                                                            #
#                                                                      #
########################################################################

readeck_enabled: false

readeck_identifier: "{{ mash_playbook_service_identifier_prefix }}readeck"

readeck_uid: "{{ mash_playbook_uid }}"
readeck_gid: "{{ mash_playbook_gid }}"

readeck_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}readeck"

readeck_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

readeck_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
readeck_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
readeck_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
readeck_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /readeck                                                           #
#                                                                      #
########################################################################
# /role-specific:readeck



# role-specific:clickhouse
########################################################################
#                                                                      #
# clickhouse                                                           #
#                                                                      #
########################################################################

clickhouse_enabled: false

clickhouse_identifier: "{{ mash_playbook_service_identifier_prefix }}clickhouse"

clickhouse_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}clickhouse"

clickhouse_uid: "{{ mash_playbook_uid }}"
clickhouse_gid: "{{ mash_playbook_gid }}"

clickhouse_managed_databases_auto_itemized:
  # Dummy entry, which is not role-specific.
  # Ensures there's at least one entry defined in the list.
  - "{{ omit }}"

  # role-specific:plausible
  - |-
    {{
      ({
        'name': plausible_clickhouse_database_name,
        'username': plausible_clickhouse_database_username,
        'password': plausible_clickhouse_database_password,
        'additional_sql': ('GRANT SELECT ON system.replicas TO ' + plausible_clickhouse_database_username + '; GRANT SELECT ON system.parts TO ' + plausible_clickhouse_database_username + ';')
      } if plausible_enabled and plausible_clickhouse_database_hostname == clickhouse_identifier else omit)
    }}
  # /role-specific:plausible

clickhouse_managed_databases_auto: "{{ clickhouse_managed_databases_auto_itemized | reject('equalto', omit) }}"

########################################################################
#                                                                      #
# /clickhouse                                                          #
#                                                                      #
########################################################################
# /role-specific:clickhouse



# role-specific:collabora_online
########################################################################
#                                                                      #
# collabora-online                                                     #
#                                                                      #
########################################################################

collabora_online_enabled: false

collabora_online_identifier: "{{ mash_playbook_service_identifier_prefix }}collabora-online"

collabora_online_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}collabora-online"

collabora_online_uid: "{{ mash_playbook_uid }}"
collabora_online_gid: "{{ mash_playbook_gid }}"

collabora_online_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

collabora_online_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
collabora_online_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
collabora_online_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
collabora_online_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /collabora-online                                                    #
#                                                                      #
########################################################################
# /role-specific:collabora_online



# role-specific:couchdb
########################################################################
#                                                                      #
# couchdb                                                              #
#                                                                      #
########################################################################
couchdb_enabled: false

couchdb_identifier: "{{ mash_playbook_service_identifier_prefix }}couchdb"

couchdb_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}couchdb"

couchdb_uid: "5984"
couchdb_gid: "5984"

couchdb_admin_user: "{{ mash_playbook_service_identifier_prefix }}admin"
couchdb_admin_passwd: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.couchdb', rounds=655555) | to_uuid }}"
couchdb_config_uuid: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'uuid.couchdb', rounds=655555) | to_uuid }}"

couchdb_config_couch_chttpd_auth_secret: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'secret.couchdb', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /couchdb                                                             #
#                                                                      #
########################################################################
# /role-specific:couchdb



# role-specific:docker_registry
########################################################################
#                                                                      #
# docker-registry                                                      #
#                                                                      #
########################################################################

docker_registry_enabled: false

docker_registry_identifier: "{{ mash_playbook_service_identifier_prefix }}docker-registry"

docker_registry_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}docker-registry"

docker_registry_uid: "{{ mash_playbook_uid }}"
docker_registry_gid: "{{ mash_playbook_gid }}"

docker_registry_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

docker_registry_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
docker_registry_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
docker_registry_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
docker_registry_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /docker-registry                                                     #
#                                                                      #
########################################################################
# /role-specific:docker_registry



# role-specific:docker_registry_proxy
########################################################################
#                                                                      #
# docker-registry-proxy                                              #
#                                                                      #
########################################################################

docker_registry_proxy_enabled: false

docker_registry_proxy_identifier: "{{ mash_playbook_service_identifier_prefix }}docker-registry-proxy"

docker_registry_proxy_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}docker-registry-proxy"

docker_registry_proxy_uid: "{{ mash_playbook_uid }}"
docker_registry_proxy_gid: "{{ mash_playbook_gid }}"

docker_registry_proxy_target_scheme: "{{ 'http' if docker_registry_enabled else '' }}"
docker_registry_proxy_target_host: "{{ docker_registry_identifier+':5000' if docker_registry_enabled else '' }}"

docker_registry_proxy_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

docker_registry_proxy_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
docker_registry_proxy_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
docker_registry_proxy_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
docker_registry_proxy_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /docker-registry-proxy                                             #
#                                                                      #
########################################################################
# /role-specific:docker_registry_proxy



# role-specific:docker_registry_browser
########################################################################
#                                                                      #
# docker-registry-browser                                              #
#                                                                      #
########################################################################

docker_registry_browser_enabled: false

docker_registry_browser_identifier: "{{ mash_playbook_service_identifier_prefix }}docker-registry-browser"

docker_registry_browser_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}docker-registry-browser"

docker_registry_browser_uid: "{{ mash_playbook_uid }}"
docker_registry_browser_gid: "{{ mash_playbook_gid }}"

docker_registry_browser_secret_key_base: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'browser', rounds=655555) }}"

docker_registry_browser_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

docker_registry_browser_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
docker_registry_browser_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
docker_registry_browser_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
docker_registry_browser_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /docker-registry-browser                                             #
#                                                                      #
########################################################################
# /role-specific:docker_registry_browser


# role-specific:docker_registry_purger
########################################################################
#                                                                      #
# docker-registry-purger                                               #
#                                                                      #
########################################################################

docker_registry_purger_enabled: false

docker_registry_purger_identifier: "{{ mash_playbook_service_identifier_prefix }}docker-registry-purger"

docker_registry_purger_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}docker-registry-purger"

docker_registry_purger_uid: "{{ mash_playbook_uid }}"
docker_registry_purger_gid: "{{ mash_playbook_gid }}"

########################################################################
#                                                                      #
# /docker-registry-purger                                              #
#                                                                      #
########################################################################
# /role-specific:docker_registry_purger



# role-specific:dokuwiki
########################################################################
#                                                                      #
# dokuwiki                                                             #
#                                                                      #
########################################################################

dokuwiki_enabled: false

dokuwiki_identifier: "{{ mash_playbook_service_identifier_prefix }}dokuwiki"

dokuwiki_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}dokuwiki"

dokuwiki_uid: "{{ mash_playbook_uid }}"
dokuwiki_gid: "{{ mash_playbook_gid }}"

dokuwiki_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
  }}

dokuwiki_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

dokuwiki_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
dokuwiki_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
dokuwiki_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
dokuwiki_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /dokuwiki                                                            #
#                                                                      #
########################################################################
# /role-specific:dokuwiki



# role-specific:echoip
########################################################################
#                                                                      #
# echoip                                                               #
#                                                                      #
########################################################################

echoip_enabled: false

echoip_identifier: "{{ mash_playbook_service_identifier_prefix }}echoip"

echoip_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}echoip"

echoip_uid: "{{ mash_playbook_uid }}"
echoip_gid: "{{ mash_playbook_gid }}"

echoip_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
  }}

echoip_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

echoip_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
echoip_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
echoip_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
echoip_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /echoip                                                              #
#                                                                      #
########################################################################
# /role-specific:echoip


# role-specific:endlessh
########################################################################
#                                                                      #
# endlessh                                                             #
#                                                                      #
########################################################################

endlessh_enabled: false
endlessh_hostname: "{{ mash_playbook_metrics_exposure_hostname }}"
endlessh_path_prefix: "{{ mash_playbook_metrics_exposure_path_prefix }}/{{ endlessh_identifier }}"
endlessh_identifier: "{{ mash_playbook_service_identifier_prefix }}endlessh"
endlessh_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}endlessh"
endlessh_uid: "{{ mash_playbook_uid }}"
endlessh_gid: "{{ mash_playbook_gid }}"

endlessh_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}


# Only enable Traefik labels if a hostname is set (indicating that this will be exposed publicly)
endlessh_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled and endlessh_hostname }}"
endlessh_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
endlessh_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
endlessh_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

endlessh_container_labels_metrics_middleware_basic_auth_enabled: "{{ mash_playbook_metrics_exposure_http_basic_auth_enabled }}"
endlessh_container_labels_metrics_middleware_basic_auth_users: "{{ mash_playbook_metrics_exposure_http_basic_auth_users }}"

########################################################################
#                                                                      #
# /endlessh                                                            #
#                                                                      #
########################################################################
# /role-specific:endlessh

# role-specific:etcd
########################################################################
#                                                                      #
# etcd                                                                 #
#                                                                      #
########################################################################

etcd_enabled: false

etcd_identifier: "{{ mash_playbook_service_identifier_prefix }}etcd"

etcd_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}etcd"

etcd_uid: "{{ mash_playbook_uid }}"
etcd_gid: "{{ mash_playbook_gid }}"

etcd_environment_variable_etcd_root_password: "{{ '' if etcd_environment_variable_allow_none_authentication else ('%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'etcd', rounds=655555) | to_uuid) }}"

########################################################################
#                                                                      #
# /etcd                                                                #
#                                                                      #
########################################################################
# /role-specific:etcd

# role-specific:etherpad
########################################################################
#                                                                      #
# etherpad                                                             #
#                                                                      #
########################################################################

etherpad_enabled: false

etherpad_identifier: "{{ mash_playbook_service_identifier_prefix }}etherpad"

etherpad_uid: "{{ mash_playbook_uid }}"
etherpad_gid: "{{ mash_playbook_gid }}"

etherpad_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}etherpad"

etherpad_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled | default(false) and etherpad_database_hostname == postgres_identifier else [])
  }}

etherpad_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled | default(false) and etherpad_database_hostname == postgres_identifier and etherpad_container_network != postgres_container_network else [])
  }}

etherpad_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
etherpad_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:postgres
etherpad_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
etherpad_database_port: "{{ '5432' if postgres_enabled else '' }}"
etherpad_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.etherpad', rounds=655555) | to_uuid }}"
etherpad_database_username: "{{ etherpad_identifier }}"
# /role-specific:postgres

# role-specific:traefik
etherpad_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
etherpad_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /etherpad                                                            #
#                                                                      #
########################################################################
# /role-specific:etherpad

# role-specific:exim_relay
########################################################################
#                                                                      #
# exim_relay                                                           #
#                                                                      #
########################################################################

exim_relay_enabled: false

exim_relay_identifier: "{{ mash_playbook_service_identifier_prefix }}exim-relay"

exim_relay_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}exim-relay"

exim_relay_uid: "{{ mash_playbook_uid }}"
exim_relay_gid: "{{ mash_playbook_gid }}"

########################################################################
#                                                                      #
# /exim_relay                                                          #
#                                                                      #
########################################################################
# /role-specific:exim_relay


# role-specific:firezone
########################################################################
#                                                                      #
# firezone                                                             #
#                                                                      #
########################################################################

firezone_enabled: false

firezone_identifier: "{{ mash_playbook_service_identifier_prefix }}firezone"

firezone_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}firezone"

firezone_uid: "{{ mash_playbook_uid }}"
firezone_gid: "{{ mash_playbook_gid }}"
firezone_generic_secret: "{{ mash_playbook_generic_secret_key }}"

# role-specific:postgres
firezone_database_host: "{{ postgres_identifier if postgres_enabled else '' }}"
firezone_database_port: "{{ '5432' if postgres_enabled else '' }}"
firezone_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'fz.db.user', rounds=655555) | to_uuid }}"
firezone_database_username: "{{ firezone_identifier }}"
# /role-specific:postgres

firezone_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled | default(false) and firezone_database_host == postgres_identifier else [])
  }}

firezone_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled | default(false) and firezone_database_host == postgres_identifier and firezone_container_network != postgres_container_network else [])
  }}

firezone_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
firezone_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
firezone_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
firezone_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /firezone                                                            #
#                                                                      #
########################################################################
# /role-specific:firezone



# role-specific:focalboard
########################################################################
#                                                                      #
# focalboard                                                           #
#                                                                      #
########################################################################

focalboard_enabled: false

focalboard_identifier: "{{ mash_playbook_service_identifier_prefix }}focalboard"

focalboard_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}focalboard"

focalboard_uid: "{{ mash_playbook_uid }}"
focalboard_gid: "{{ mash_playbook_gid }}"

focalboard_systemd_required_systemd_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and focalboard_database_hostname == postgres_identifier else [])
  }}

# role-specific:postgres
focalboard_database_type: "{{ 'postgres' if postgres_enabled else '' }}"
focalboard_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
focalboard_database_port: "{{ '5432' if postgres_enabled else '' }}"
focalboard_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.focalboard', rounds=655555) | to_uuid }}"
# /role-specific:postgres

focalboard_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled | default(false) and focalboard_database_hostname == postgres_identifier else [])
  }}

focalboard_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
focalboard_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
focalboard_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
focalboard_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /focalboard                                                          #
#                                                                      #
########################################################################
# /role-specific:focalboard



# role-specific:freshrss
########################################################################
#                                                                      #
# freshrss                                                             #
#                                                                      #
########################################################################

freshrss_enabled: false

freshrss_identifier: "{{ mash_playbook_service_identifier_prefix }}freshrss"

freshrss_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}freshrss"

# freshrss_uid and freshrss_gid are intentionally not being set here.
# FreshRSS can only work with a specific user and group, as hardcoded in the role defaults.
freshrss_uid: "0"
freshrss_gid: "33"

freshrss_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and freshrss_database_hostname == postgres_identifier else [])
  }}

freshrss_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and freshrss_database_hostname == postgres_identifier and freshrss_container_network != postgres_container_network else [])
  }}

freshrss_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
freshrss_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
freshrss_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
freshrss_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

freshrss_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"

# Intentionally not auto-generating freshrss_database_password.
# It's meant to be explicitly defined, so that it can be used in the setup wizard after installation.

########################################################################
#                                                                      #
# /freshrss                                                            #
#                                                                      #
########################################################################
# /role-specific:freshrss


# role-specific:funkwhale
########################################################################
#                                                                      #
# funkwhale                                                            #
#                                                                      #
########################################################################

funkwhale_enabled: false

funkwhale_identifier: "{{ mash_playbook_service_identifier_prefix }}funkwhale"

funkwhale_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}funkwhale"

funkwhale_uid: "{{ mash_playbook_uid }}"
funkwhale_gid: "{{ mash_playbook_gid }}"

funkwhale_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
funkwhale_database_port: "{{ '5432' if postgres_enabled else '' }}"
funkwhale_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.funkwhale', rounds=655555) | to_uuid }}"
funkwhale_database_username: "{{ funkwhale_identifier }}"

funkwhale_api_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and funkwhale_database_hostname == postgres_identifier else [])
  }}

funkwhale_frontend_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and funkwhale_database_hostname == postgres_identifier else [])
  }}

funkwhale_api_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and funkwhale_database_hostname == postgres_identifier and funkwhale_api_container_network != postgres_container_network else [])
  }}

funkwhale_frontend_container_additional_networks_auto: |
  {{
    ([postgres_container_network] if postgres_enabled and funkwhale_database_hostname == postgres_identifier and funkwhale_frontend_container_network != postgres_container_network else [])
    +
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

funkwhale_api_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
funkwhale_api_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
funkwhale_api_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
funkwhale_api_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

funkwhale_frontend_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
funkwhale_frontend_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
funkwhale_frontend_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
funkwhale_frontend_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /funkwhale                                                           #
#                                                                      #
########################################################################
# /role-specific:funkwhale


# role-specific:gitea
########################################################################
#                                                                      #
# gitea                                                                #
#                                                                      #
########################################################################

gitea_enabled: false

gitea_identifier: "{{ mash_playbook_service_identifier_prefix }}gitea"

gitea_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}gitea"

gitea_uid: "{{ mash_playbook_uid }}"
gitea_gid: "{{ mash_playbook_gid }}"

gitea_systemd_required_systemd_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and gitea_config_database_hostname == postgres_identifier else [])
  }}

gitea_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and gitea_config_database_hostname == postgres_identifier and gitea_container_network != postgres_container_network else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and gitea_config_mailer_smtp_addr == exim_relay_identifier | default('mash-exim-relay') and gitea_container_network != exim_relay_container_network) else [])
  }}

gitea_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
gitea_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
gitea_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
gitea_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

gitea_container_labels_traefik_compression_middleware_enabled: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_enabled }}"
gitea_container_labels_traefik_compression_middleware_name: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_name if mash_playbook_reverse_proxy_traefik_middleware_compession_enabled else '' }}"

gitea_config_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
gitea_config_database_port: "{{ '5432' if postgres_enabled else '' }}"
gitea_config_database_username: "gitea"
gitea_config_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.gitea', rounds=655555) | to_uuid }}"

# role-specific:exim_relay
gitea_config_mailer_enabled: "{{ 'true' if exim_relay_enabled else '' }}"
gitea_config_mailer_smtp_addr: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
gitea_config_mailer_smtp_port: 8025
gitea_config_mailer_from: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
gitea_config_mailer_protocol: "{{ 'smtp' if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /gitea                                                               #
#                                                                      #
########################################################################
# /role-specific:gitea



# role-specific:gotosocial
########################################################################
#                                                                      #
# gotosocial                                                           #
#                                                                      #
########################################################################

gotosocial_enabled: false

gotosocial_identifier: "{{ mash_playbook_service_identifier_prefix }}gotosocial"

gotosocial_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}gotosocial"

gotosocial_uid: "{{ mash_playbook_uid }}"
gotosocial_gid: "{{ mash_playbook_gid }}"

gotosocial_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and gotosocial_database_host == postgres_identifier else [])
  }}

gotosocial_systemd_wanted_services_list_auto: |
  {{
    ([(exim_relay_identifier | default('mash-exim-relay')) ~ '.service'] if (exim_relay_enabled | default(false) and gotosocial_smtp_host == exim_relay_identifier | default('mash-exim-relay')) else [])
  }}

gotosocial_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and gotosocial_database_host == postgres_identifier and gotosocial_container_network != postgres_container_network else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and gotosocial_smtp_host == exim_relay_identifier | default('mash-exim-relay') and gotosocial_container_network != exim_relay_container_network) else [])
  }}

gotosocial_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
gotosocial_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
gotosocial_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
gotosocial_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

gotosocial_database_host: "{{ postgres_identifier if postgres_enabled else '' }}"
gotosocial_database_port: "{{ '5432' if postgres_enabled else '' }}"
gotosocial_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.gotosocial', rounds=655555) | to_uuid }}"
gotosocial_database_username: "{{ gotosocial_identifier }}"

# role-specific:exim_relay
gotosocial_smtp_host: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
gotosocial_smtp_port: 8025
gotosocial_smtp_from: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /gotosocial                                                          #
#                                                                      #
########################################################################
# /role-specific:gotosocial



# role-specific:grafana
########################################################################
#                                                                      #
# grafana                                                              #
#                                                                      #
########################################################################

grafana_enabled: false

grafana_identifier: "{{ mash_playbook_service_identifier_prefix }}grafana"

grafana_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}grafana"

grafana_uid: "{{ mash_playbook_uid }}"
grafana_gid: "{{ mash_playbook_gid }}"


# role-specific:exim_relay
grafana_smtp_enabled: "{{ exim_relay_enabled | default(false) }}"
grafana_smtp_host: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
grafana_smtp_port: 8025
grafana_smtp_from_address: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

grafana_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if (grafana_container_labels_traefik_enabled and mash_playbook_reverse_proxyable_services_additional_network) else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and grafana_smtp_host == exim_relay_identifier | default('mash-exim-relay') and grafana_container_network != exim_relay_container_network) else [])
  }}

grafana_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
grafana_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
grafana_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
grafana_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /grafana                                                             #
#                                                                      #
########################################################################
# /role-specific:grafana



# role-specific:headscale
########################################################################
#                                                                      #
# headscale                                                            #
#                                                                      #
########################################################################

headscale_enabled: false

headscale_identifier: "{{ mash_playbook_service_identifier_prefix }}headscale"

headscale_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}headscale"

headscale_uid: "{{ mash_playbook_uid }}"
headscale_gid: "{{ mash_playbook_gid }}"

headscale_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

headscale_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
headscale_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
headscale_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
headscale_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /headscale                                                           #
#                                                                      #
########################################################################
# /role-specific:headscale



# role-specific:healthchecks
########################################################################
#                                                                      #
# healthchecks                                                         #
#                                                                      #
########################################################################

healthchecks_enabled: false

healthchecks_identifier: "{{ mash_playbook_service_identifier_prefix }}healthchecks"

healthchecks_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}healthchecks"

healthchecks_uid: "{{ mash_playbook_uid }}"
healthchecks_gid: "{{ mash_playbook_gid }}"

healthchecks_systemd_required_systemd_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and healthchecks_database_hostname == postgres_identifier else [])
  }}

healthchecks_systemd_wanted_systemd_services_list_auto: |
  {{
    ([(exim_relay_identifier | default('mash-exim-relay')) ~ '.service'] if (exim_relay_enabled | default(false) and healthchecks_environment_variable_email_host == exim_relay_identifier | default('mash-exim-relay')) else [])
  }}

healthchecks_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and healthchecks_database_hostname == postgres_identifier and healthchecks_container_network != postgres_container_network else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and healthchecks_environment_variable_email_host == exim_relay_identifier | default('mash-exim-relay') and healthchecks_container_network != exim_relay_container_network) else [])
  }}

healthchecks_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
healthchecks_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
healthchecks_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
healthchecks_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

healthchecks_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
healthchecks_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'healthchecks.db', rounds=655555) | to_uuid }}"

healthchecks_environment_variable_secret_key: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'healthchecks', rounds=655555) | to_uuid }}"

# role-specific:exim_relay
healthchecks_environment_variable_default_from_email: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
healthchecks_environment_variable_email_host: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
healthchecks_environment_variable_email_port: "{{ 8025 if exim_relay_enabled else '587' }}"
healthchecks_environment_variable_email_use_tls: "{{ false if exim_relay_enabled else true }}"
healthchecks_environment_variable_email_use_verification: "{{ false if exim_relay_enabled else true }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /healthchecks                                                        #
#                                                                      #
########################################################################
# /role-specific:healthchecks



# role-specific:hubsite
########################################################################
#                                                                      #
# hubsite                                                              #
#                                                                      #
########################################################################

hubsite_enabled: false

hubsite_identifier: "{{ mash_playbook_service_identifier_prefix }}hubsite"

hubsite_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}hubsite"

hubsite_uid: "{{ mash_playbook_uid }}"
hubsite_gid: "{{ mash_playbook_gid }}"

hubsite_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

hubsite_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
hubsite_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
hubsite_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
hubsite_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

# Services
##########

# role-specific:adguard_home
# Adguard home
hubsite_service_adguard_home_enabled: "{{ adguard_home_enabled }}"
hubsite_service_adguard_home_name: Adguard Home
hubsite_service_adguard_home_url: "https://{{ adguard_home_hostname }}{{ adguard_home_path_prefix }}"
hubsite_service_adguard_home_logo_location: "{{ role_path }}/assets/shield.png"
hubsite_service_adguard_home_description: "A network-wide DNS software for blocking ads & tracking"
hubsite_service_adguard_home_priority: 1000
# /role-specific:adguard_home

# role-specific:authentik
# authentik
hubsite_service_authentik_enabled: "{{ authentik_enabled }}"
hubsite_service_authentik_name: Authentik
hubsite_service_authentik_url: "https://{{ authentik_hostname }}"
hubsite_service_authentik_logo_location: "{{ role_path }}/assets/authentik.png"
hubsite_service_authentik_description: "An open source identity provider"
hubsite_service_authentik_priority: 1000
# /role-specific:authentik

# role-specific:appsmith
# Appsmith
hubsite_service_appsmith_enabled: "{{ appsmith_enabled }}"
hubsite_service_appsmith_name: Appsmith
hubsite_service_appsmith_url: "https://{{ appsmith_hostname }}{{ appsmith_path_prefix }}"
hubsite_service_appsmith_logo_location: "{{ role_path }}/assets/appsmith.png"
hubsite_service_appsmith_description: "Platform for building and deploying custom internal tools and applications without writing code"
hubsite_service_appsmith_priority: 1000
# /role-specific:appsmith

# role-specific:docker_registry_browser
# Docker Registry Browser
hubsite_service_docker_registry_browser_enabled: "{{ docker_registry_browser_enabled }}"
hubsite_service_docker_registry_browser_name: Docker Registry Browser
hubsite_service_docker_registry_browser_url: "https://{{ docker_registry_browser_hostname }}{{ docker_registry_browser_path_prefix }}"
hubsite_service_docker_registry_browser_logo_location: "{{ role_path }}/assets/docker.png"
hubsite_service_docker_registry_browser_description: "Browse docker images"
hubsite_service_docker_registry_browser_priority: 1000
# /role-specific:docker_registry_browser

# role-specific:etherpad
# etherpad
hubsite_service_etherpad_enabled: "{{ etherpad_enabled }}"
hubsite_service_etherpad_name: Etherpad
hubsite_service_etherpad_url: "https://{{ etherpad_hostname }}{{ etherpad_path_prefix }}"
hubsite_service_etherpad_logo_location: "{{ role_path }}/assets/etherpad.png"
hubsite_service_etherpad_description: "Open source collaborative text editor"
hubsite_service_etherpad_priority: 1000
# /role-specific:etherpad

# role-specific:firezone
# Firezone
hubsite_service_firezone_enabled: "{{ firezone_enabled }}"
hubsite_service_firezone_name: Firezone
hubsite_service_firezone_url: "https://{{ firezone_hostname }}"
hubsite_service_firezone_logo_location: "{{ role_path }}/assets/firezone.png"
hubsite_service_firezone_description: "A self-hosted VPN server, based on Wireguard"
hubsite_service_firezone_priority: 1000
# /role-specific:firezone

# role-specific:focalboard
# Focalboard
hubsite_service_focalboard_enabled: "{{ focalboard_enabled }}"
hubsite_service_focalboard_name: Focalboard
hubsite_service_focalboard_url: "https://{{ focalboard_hostname }}{{ focalboard_path_prefix }}"
hubsite_service_focalboard_logo_location: "{{ role_path }}/assets/focalboard.png"
hubsite_service_focalboard_description: "An open source, self-hosted alternative to Trello, Notion, and Asana."
hubsite_service_focalboard_priority: 1000
# /role-specific:focalboard

# role-specific:freshrss
# FreshRSS
hubsite_service_freshrss_enabled: "{{ freshrss_enabled }}"
hubsite_service_freshrss_name: FreshRSS
hubsite_service_freshrss_url: "https://{{ freshrss_hostname }}{{ freshrss_path_prefix }}"
hubsite_service_freshrss_logo_location: "{{ role_path }}/assets/freshrss.png"
hubsite_service_freshrss_description: "RSS and Atom feed aggregator."
hubsite_service_freshrss_priority: 1000
# /role-specific:freshrss

# role-specific:funkwhale
# Funkwhale
hubsite_service_funkwhale_enabled: "{{ funkwhale_enabled }}"
hubsite_service_funkwhale_name: Funkwhale
hubsite_service_funkwhale_url: "https://{{ funkwhale_hostname }}"
hubsite_service_funkwhale_logo_location: "{{ role_path }}/assets/funkwhale.png"
hubsite_service_funkwhale_description: "Listen and share music with a selfhosted streaming server"
hubsite_service_funkwhale_priority: 1000
# /role-specific:funkwhale

# role-specific:gitea
# Gitea
hubsite_service_gitea_enabled: "{{ gitea_enabled }}"
hubsite_service_gitea_name: Gitea
hubsite_service_gitea_url: "https://{{ gitea_hostname }}{{ gitea_path_prefix }}"
hubsite_service_gitea_logo_location: "{{ role_path }}/assets/gitea.png"
hubsite_service_gitea_description: "A git service"
hubsite_service_gitea_priority: 1000
# /role-specific:gitea

# role-specific:gotosocial
# GoToSocial
hubsite_service_gotosocial_enabled: "{{ gotosocial_enabled }}"
hubsite_service_gotosocial_name: GoToSocial
hubsite_service_gotosocial_url: "https://{{ gotosocial_hostname }}"
hubsite_service_gotosocial_logo_location: "{{ role_path }}/assets/gotosocial.png"
hubsite_service_gotosocial_description: "A fediverse server"
hubsite_service_gotosocial_priority: 1000
# /role-specific:gotosocial

# role-specific:grafana
# Grafana
hubsite_service_grafana_enabled: "{{ grafana_enabled }}"
hubsite_service_grafana_name: Grafana
hubsite_service_grafana_url: "https://{{ grafana_hostname }}{{ grafana_path_prefix }}"
hubsite_service_grafana_logo_location: "{{ role_path }}/assets/grafana.png"
hubsite_service_grafana_description: "Check how your server is doing"
hubsite_service_grafana_priority: 1000
# /role-specific:grafana

# role-specific:healthchecks
# Healthchecks
hubsite_service_healthchecks_enabled: "{{ healthchecks_enabled }}"
hubsite_service_healthchecks_name: Healthchecks
hubsite_service_healthchecks_url: "https://{{ healthchecks_hostname }}{{ healthchecks_path_prefix }}"
hubsite_service_healthchecks_logo_location: "{{ role_path }}/assets/healthchecks.png"
hubsite_service_healthchecks_description: "A simple and Effective Cron Job Monitoring solution"
hubsite_service_healthchecks_priority: 1000
# /role-specific:healthchecks

# role-specific:keycloak
# Keycloak
hubsite_service_keycloak_enabled: "{{ keycloak_enabled }}"
hubsite_service_keycloak_name: Keycloak
hubsite_service_keycloak_url: "https://{{ keycloak_hostname }}{{ keycloak_path_prefix }}"
hubsite_service_keycloak_logo_location: "{{ role_path }}/assets/keycloak.png"
hubsite_service_keycloak_description: "An open source identity and access management solution."
hubsite_service_keycloak_priority: 1000
# /role-specific:keycloak

# role-specific:labelstudio
# Label Studio
hubsite_service_labelstudio_enabled: "{{ labelstudio_enabled }}"
hubsite_service_labelstudio_name: "Label Studio"
hubsite_service_labelstudio_url: "https://{{ labelstudio_hostname }}{{ labelstudio_path_prefix }}"
hubsite_service_labelstudio_logo_location: "{{ role_path }}/assets/labelstudio.png"
hubsite_service_labelstudio_description: "The most flexible data labeling platform to fine-tune LLMs, prepare training data or validate AI models"
hubsite_service_labelstudio_priority: 1000
# /role-specific:labelstudio

# role-specific:languagetool
# LanguageTool
hubsite_service_languagetool_enabled: "{{ languagetool_enabled }}"
hubsite_service_languagetool_name: LanguageTool
hubsite_service_languagetool_url: "https://{{ languagetool_hostname }}{{ languagetool_path_prefix }}"
hubsite_service_languagetool_logo_location: "{{ role_path }}/assets/languagetool.png"
hubsite_service_languagetool_description: "An open source online grammar, style and spell checker"
hubsite_service_languagetool_priority: 1000
# /role-specific:languagetool

# role-specific:miniflux
# Miniflux
hubsite_service_miniflux_enabled: "{{ miniflux_enabled }}"
hubsite_service_miniflux_name: Miniflux
hubsite_service_miniflux_url: "https://{{ miniflux_hostname }}{{ miniflux_path_prefix }}"
hubsite_service_miniflux_logo_location: "{{ role_path }}/assets/miniflux.png"
hubsite_service_miniflux_description: "An opinionated feed reader"
hubsite_service_miniflux_priority: 1000
# /role-specific:miniflux

# role-specific:navidrome
# Navidrome
hubsite_service_navidrome_enabled: "{{ navidrome_enabled }}"
hubsite_service_navidrome_name: Navidrome
hubsite_service_navidrome_url: "https://{{ navidrome_hostname }}{{ navidrome_path_prefix }}"
hubsite_service_navidrome_logo_location: "{{ role_path }}/assets/navidrome.png"
hubsite_service_navidrome_description: "A Subsonic-API compatible music server"
hubsite_service_navidrome_priority: 1000
# /role-specific:navidrome

# role-specific:n8n
# n8n
hubsite_service_n8n_enabled: "{{ n8n_enabled }}"
hubsite_service_n8n_name: n8n
hubsite_service_n8n_url: "https://{{ n8n_hostname }}{{ n8n_path_prefix }}"
hubsite_service_n8n_logo_location: "{{ role_path }}/assets/n8n.png"
hubsite_service_n8n_description: "Workflow automation for technical people."
hubsite_service_n8n_priority: 1000
# /role-specific:n8n

# role-specific:linkding
# Linkding
hubsite_service_linkding_enabled: "{{ linkding_enabled }}"
hubsite_service_linkding_name: Linkding
hubsite_service_linkding_url: "https://{{ linkding_hostname }}{{ linkding_path_prefix }}"
hubsite_service_linkding_logo_location: "{{ role_path }}/assets/linkding.png"
hubsite_service_linkding_description: "Bookmark manager that is designed be to be minimal and fast."
hubsite_service_linkding_priority: 1000
# /role-specific:linkding

# role-specific:nextcloud
# Nextcloud
hubsite_service_nextcloud_enabled: "{{ nextcloud_enabled }}"
hubsite_service_nextcloud_name: Nextcloud
hubsite_service_nextcloud_url: "https://{{ nextcloud_hostname }}{{ nextcloud_path_prefix }}"
hubsite_service_nextcloud_logo_location: "{{ role_path }}/assets/nextcloud.png"
hubsite_service_nextcloud_description: "Sync your files & much more"
hubsite_service_nextcloud_priority: 1000
# /role-specific:nextcloud

# role-specific:ntfy
# ntfy
hubsite_service_ntfy_enabled: "{{ ntfy_enabled }}"
hubsite_service_ntfy_name: ntfy
hubsite_service_ntfy_url: "https://{{ ntfy_hostname }}{{ ntfy_path_prefix }}"
hubsite_service_ntfy_logo_location: "{{ role_path }}/assets/ntfy.png"
hubsite_service_ntfy_description: "Send push notifications from any computer"
hubsite_service_ntfy_priority: 1000
# /role-specific:ntfy

# role-specific:owncast
# Owncast
hubsite_service_owncast_enabled: "{{ owncast_enabled }}"
hubsite_service_owncast_name: Owncast
hubsite_service_owncast_url: "https://{{ owncast_hostname }}"
hubsite_service_owncast_logo_location: "{{ role_path }}/assets/owncast.png"
hubsite_service_owncast_description: "Livestream & Chat"
hubsite_service_owncast_priority: 1000
# /role-specific:owncast

# role-specific:peertube
# Peertube
hubsite_service_peertube_enabled: "{{ peertube_enabled }}"
hubsite_service_peertube_name: Peertube
hubsite_service_peertube_url: "https://{{ peertube_hostname }}{{ peertube_path_prefix }}"
hubsite_service_peertube_logo_location: "{{ role_path }}/assets/peertube.png"
hubsite_service_peertube_description: "Watch and upload videos"
hubsite_service_peertube_priority: 1000
# /role-specific:peertube

# role-specific:radicale
# Radicale
hubsite_service_radicale_enabled: "{{ radicale_enabled }}"
hubsite_service_radicale_name: Radicale
hubsite_service_radicale_url: "https://{{ radicale_hostname }}{{ radicale_path_prefix }}"
hubsite_service_radicale_logo_location: "{{ role_path }}/assets/radicale.png"
hubsite_service_radicale_description: "Sync contacts and calendars"
hubsite_service_radicale_priority: 1000
# /role-specific:radicale

# role-specific:syncthing
# Syncthing
hubsite_service_syncthing_enabled: "{{ syncthing_enabled }}"
hubsite_service_syncthing_name: Syncthing
hubsite_service_syncthing_url: "https://{{ syncthing_hostname }}{{ syncthing_path_prefix }}"
hubsite_service_syncthing_logo_location: "{{ role_path }}/assets/syncthing.png"
hubsite_service_syncthing_description: "Sync your files"
hubsite_service_syncthing_priority: 1000
# /role-specific:syncthing

# role-specific:tandoor
# tandoor
hubsite_service_tandoor_enabled: "{{ tandoor_enabled }}"
hubsite_service_tandoor_name: tandoor
hubsite_service_tandoor_url: "https://{{ tandoor_hostname }}{{ tandoor_path_prefix }}"
hubsite_service_tandoor_logo_location: "{{ role_path }}/assets/tandoor.png"
hubsite_service_tandoor_description: "The recipe manager that allows you to manage your ever growing collection of digital recipes."
hubsite_service_tandoor_priority: 1000
# /role-specific:tandoor

# role-specific:uptime_kuma
# Uptime Kuma
hubsite_service_uptime_kuma_enabled: "{{ uptime_kuma_enabled }}"
hubsite_service_uptime_kuma_name: Uptime Kuma
hubsite_service_uptime_kuma_url: "https://{{ uptime_kuma_hostname }}{{ uptime_kuma_path_prefix }}"
hubsite_service_uptime_kuma_logo_location: "{{ role_path }}/assets/uptime-kuma.png"
hubsite_service_uptime_kuma_description: "Check the status of the services"
hubsite_service_uptime_kuma_priority: 1000
# /role-specific:uptime_kuma

# role-specific:vaultwarden
# Vaultwarden
# The vaultwarden service link is deactivated by default for security reasons, see: https://github.com/dani-garcia/vaultwarden/wiki/Hardening-Guide#hiding-under-a-subdir
hubsite_service_vaultwarden_enabled: false
hubsite_service_vaultwarden_name: Vaultwarden
hubsite_service_vaultwarden_url: "https://{{ vaultwarden_hostname }}{{ vaultwarden_path_prefix }}"
hubsite_service_vaultwarden_logo_location: "{{ role_path }}/assets/vaultwarden.png"
hubsite_service_vaultwarden_description: "Securely access your passwords"
hubsite_service_vaultwarden_priority: 1000
# /role-specific:vaultwarden

# role-specific:woodpecker_ci_server
# Woodpecker CI
hubsite_service_woodpecker_ci_enabled: "{{ woodpecker_ci_server_enabled }}"
hubsite_service_woodpecker_ci_name: Woodpecker CI
hubsite_service_woodpecker_ci_url: "https://{{ woodpecker_ci_server_hostname }}"
hubsite_service_woodpecker_ci_logo_location: "{{ role_path }}/assets/woodpecker.png"
hubsite_service_woodpecker_ci_description: "Check you CI"
hubsite_service_woodpecker_ci_priority: 1000
# /role-specific:woodpecker_ci_server

# role-specific:writefreely
# writefreely
hubsite_service_writefreely_enabled: "{{ writefreely_enabled }}"
hubsite_service_writefreely_name: writefreely
hubsite_service_writefreely_url: "https://{{ writefreely_hostname }}"
hubsite_service_writefreely_logo_location: "{{ role_path }}/assets/writefreely.png"
hubsite_service_writefreely_description: "A minimalist web blog"
hubsite_service_writefreely_priority: 1000
# /role-specific:writefreely

# role-specific:forgejo
# Forgejo
hubsite_service_forgejo_enabled: "{{ forgejo_enabled }}"
hubsite_service_forgejo_name: Forgejo
hubsite_service_forgejo_url: "https://{{ forgejo_hostname }}{{ forgejo_path_prefix }}"
hubsite_service_forgejo_logo_location: "{{ role_path }}/assets/forgejo.png"
hubsite_service_forgejo_description: "Another git service"
hubsite_service_forgejo_priority: 1000
# /role-specific:forgejo

mash_playbook_hubsite_service_list_auto_itemized:
  # Dummy entry, which is not role-specific.
  # Ensures there's at least one entry defined in the list.
  - "{{ omit }}"

  # role-specific:adguard_home
  - |-
    {{
      ({
        'name': hubsite_service_adguard_home_name,
        'url': hubsite_service_adguard_home_url,
        'logo_location': hubsite_service_adguard_home_logo_location,
        'description': hubsite_service_adguard_home_description,
        'priority': hubsite_service_adguard_home_priority,
      } if hubsite_service_adguard_home_enabled else omit)
    }}
  # /role-specific:adguard_home

  # role-specific:authentik
  - |-
    {{
      ({
        'name': hubsite_service_authentik_name,
        'url': hubsite_service_authentik_url,
        'logo_location': hubsite_service_authentik_logo_location,
        'description': hubsite_service_authentik_description,
        'priority': hubsite_service_adguard_home_priority,
      } if hubsite_service_authentik_enabled else omit)
    }}
  # /role-specific:authentik

  # role-specific:appsmith
  - |-
    {{
      ({
        'name': hubsite_service_appsmith_name,
        'url': hubsite_service_appsmith_url,
        'logo_location': hubsite_service_appsmith_logo_location,
        'description': hubsite_service_appsmith_description,
        'priority': hubsite_service_appsmith_priority,
      } if hubsite_service_appsmith_enabled else omit)
    }}
  # /role-specific:appsmith

  # role-specific:docker_registry_browser
  - |-
    {{
      ({
        'name': hubsite_service_docker_registry_browser_name,
        'url': hubsite_service_docker_registry_browser_url,
        'logo_location': hubsite_service_docker_registry_browser_logo_location,
        'description': hubsite_service_docker_registry_browser_description,
        'priority': hubsite_service_docker_registry_browser_priority,
      } if hubsite_service_docker_registry_browser_enabled else omit)
    }}
  # /role-specific:docker_registry_browser

  # role-specific:etherpad
  - |-
    {{
      ({
        'name': hubsite_service_etherpad_name,
        'url': hubsite_service_etherpad_url,
        'logo_location': hubsite_service_etherpad_logo_location,
        'description': hubsite_service_etherpad_description,
        'priority': hubsite_service_etherpad_priority,
      } if hubsite_service_etherpad_enabled else omit)
    }}
  # /role-specific:etherpad

  # role-specific:firezone
  - |-
    {{
      ({
        'name': hubsite_service_firezone_name,
        'url': hubsite_service_firezone_url,
        'logo_location': hubsite_service_firezone_logo_location,
        'description': hubsite_service_firezone_description,
        'priority': hubsite_service_firezone_priority,
      } if hubsite_service_firezone_enabled else omit)
    }}
  # /role-specific:firezone

  # role-specific:focalboard
  - |-
    {{
      ({
        'name': hubsite_service_focalboard_name,
        'url': hubsite_service_focalboard_url,
        'logo_location': hubsite_service_focalboard_logo_location,
        'description': hubsite_service_focalboard_description,
        'priority': hubsite_service_focalboard_priority,
      } if hubsite_service_focalboard_enabled else omit)
    }}
  # /role-specific:focalboard

  # role-specific:freshrss
  - |-
    {{
      ({
        'name': hubsite_service_freshrss_name,
        'url': hubsite_service_freshrss_url,
        'logo_location': hubsite_service_freshrss_logo_location,
        'description': hubsite_service_freshrss_description,
        'priority': hubsite_service_freshrss_priority,
      } if hubsite_service_freshrss_enabled else omit)
    }}
  # /role-specific:freshrss

  # role-specific:funkwhale
  - |-
    {{
      ({
        'name': hubsite_service_funkwhale_name,
        'url': hubsite_service_funkwhale_url,
        'logo_location': hubsite_service_funkwhale_logo_location,
        'description': hubsite_service_funkwhale_description,
        'priority': hubsite_service_funkwhale_priority,
      } if hubsite_service_funkwhale_enabled else omit)
    }}
  # /role-specific:funkwhale

  # role-specific:gitea
  - |-
    {{
      ({
        'name': hubsite_service_gitea_name,
        'url': hubsite_service_gitea_url,
        'logo_location': hubsite_service_gitea_logo_location,
        'description': hubsite_service_gitea_description,
        'priority': hubsite_service_gitea_priority,
      } if hubsite_service_gitea_enabled else omit)
    }}
  # /role-specific:gitea

  # role-specific:gotosocial
  - |-
    {{
      ({
        'name': hubsite_service_gotosocial_name,
        'url': hubsite_service_gotosocial_url,
        'logo_location': hubsite_service_gotosocial_logo_location,
        'description': hubsite_service_gotosocial_description,
        'priority': hubsite_service_gotosocial_priority,
      } if hubsite_service_gotosocial_enabled else omit)
    }}
  # /role-specific:gotosocial

  # role-specific:grafana
  - |-
    {{
      ({
        'name': hubsite_service_grafana_name,
        'url': hubsite_service_grafana_url,
        'logo_location': hubsite_service_grafana_logo_location,
        'description': hubsite_service_grafana_description,
        'priority': hubsite_service_grafana_priority,
      } if hubsite_service_grafana_enabled else omit)
    }}
  # /role-specific:grafana

  # role-specific:healthchecks
  - |-
    {{
      ({
        'name': hubsite_service_healthchecks_name,
        'url': hubsite_service_healthchecks_url,
        'logo_location': hubsite_service_healthchecks_logo_location,
        'description': hubsite_service_healthchecks_description,
        'priority': hubsite_service_healthchecks_priority,
      } if hubsite_service_healthchecks_enabled else omit)
    }}
  # /role-specific:healthchecks

  # role-specific:keycloak
  - |-
    {{
      ({
        'name': hubsite_service_keycloak_name,
        'url': hubsite_service_keycloak_url,
        'logo_location': hubsite_service_keycloak_logo_location,
        'description': hubsite_service_keycloak_description,
        'priority': hubsite_service_keycloak_priority,
      } if hubsite_service_keycloak_enabled else omit)
    }}
  # /role-specific:keycloak

  # role-specific:labelstudio
  - |-
    {{
      ({
        'name': hubsite_service_labelstudio_name,
        'url': hubsite_service_labelstudio_url,
        'logo_location': hubsite_service_labelstudio_logo_location,
        'description': hubsite_service_labelstudio_description,
        'priority': hubsite_service_labelstudio_priority,
      } if hubsite_service_labelstudio_enabled else omit)
    }}
  # /role-specific:labelstudio

 # role-specific:languagetool
  - |-
    {{
      ({
        'name': hubsite_service_languagetool_name,
        'url': hubsite_service_languagetool_url,
        'logo_location': hubsite_service_languagetool_logo_location,
        'description': hubsite_service_languagetool_description,
        'priority': hubsite_service_languagetool_priority,
      } if hubsite_service_languagetool_enabled else omit)
    }}
  # /role-specific:languagetool

  # role-specific:linkding
  - |-
    {{
      ({
        'name': hubsite_service_linkding_name,
        'url': hubsite_service_linkding_url,
        'logo_location': hubsite_service_linkding_logo_location,
        'description': hubsite_service_linkding_description,
        'priority': hubsite_service_linkding_priority,
      } if hubsite_service_linkding_enabled else omit)
    }}
  # /role-specific:linkding

  # role-specific:miniflux
  - |-
    {{
      ({
        'name': hubsite_service_miniflux_name,
        'url': hubsite_service_miniflux_url,
        'logo_location': hubsite_service_miniflux_logo_location,
        'description': hubsite_service_miniflux_description,
        'priority': hubsite_service_miniflux_priority,
      } if hubsite_service_miniflux_enabled else omit)
    }}
  # /role-specific:miniflux

  # role-specific:navidrome
  - |-
    {{
      ({
        'name': hubsite_service_navidrome_name,
        'url': hubsite_service_navidrome_url,
        'logo_location': hubsite_service_navidrome_logo_location,
        'description': hubsite_service_navidrome_description,
        'priority': hubsite_service_navidrome_priority,
      } if hubsite_service_navidrome_enabled else omit)
    }}
  # /role-specific:navidrome

  # role-specific:n8n
  - |-
    {{
      ({
        'name': hubsite_service_n8n_name,
        'url': hubsite_service_n8n_url,
        'logo_location': hubsite_service_n8n_logo_location,
        'description': hubsite_service_n8n_description,
        'priority': hubsite_service_n8n_priority,
      } if hubsite_service_n8n_enabled else omit)
    }}
  # /role-specific:n8n

  # role-specific:nextcloud
  - |-
    {{
      ({
        'name': hubsite_service_nextcloud_name,
        'url': hubsite_service_nextcloud_url,
        'logo_location': hubsite_service_nextcloud_logo_location,
        'description': hubsite_service_nextcloud_description,
        'priority': hubsite_service_nextcloud_priority,
      } if hubsite_service_nextcloud_enabled else omit)
    }}
  # /role-specific:nextcloud

  # role-specific:ntfy
  - |-
    {{
      ({
        'name': hubsite_service_ntfy_name,
        'url': hubsite_service_ntfy_url,
        'logo_location': hubsite_service_ntfy_logo_location,
        'description': hubsite_service_ntfy_description,
        'priority': hubsite_service_ntfy_priority,
      } if hubsite_service_ntfy_enabled else omit)
    }}
  # /role-specific:ntfy

  # role-specific:owncast
  - |-
    {{
      ({
        'name': hubsite_service_owncast_name,
        'url': hubsite_service_owncast_url,
        'logo_location': hubsite_service_owncast_logo_location,
        'description': hubsite_service_owncast_description,
        'priority': hubsite_service_owncast_priority,
      } if hubsite_service_owncast_enabled else omit)
    }}
  # /role-specific:owncast

  # role-specific:peertube
  - |-
    {{
      ({
        'name': hubsite_service_peertube_name,
        'url': hubsite_service_peertube_url,
        'logo_location': hubsite_service_peertube_logo_location,
        'description': hubsite_service_peertube_description,
        'priority': hubsite_service_peertube_priority,
      } if hubsite_service_peertube_enabled else omit)
    }}
  # /role-specific:peertube

  # role-specific:radicale
  - |-
    {{
      ({
        'name': hubsite_service_radicale_name,
        'url': hubsite_service_radicale_url,
        'logo_location': hubsite_service_radicale_logo_location,
        'description': hubsite_service_radicale_description,
        'priority': hubsite_service_radicale_priority,
      } if hubsite_service_radicale_enabled else omit)
    }}
  # /role-specific:radicale

  # role-specific:uptime_kuma
  - |-
    {{
      ({
        'name': hubsite_service_uptime_kuma_name,
        'url': hubsite_service_uptime_kuma_url,
        'logo_location': hubsite_service_uptime_kuma_logo_location,
        'description': hubsite_service_uptime_kuma_description,
        'priority': hubsite_service_uptime_kuma_priority,
      } if hubsite_service_uptime_kuma_enabled else omit)
    }}
  # /role-specific:uptime_kuma

  # role-specific:syncthing
  - |-
    {{
      ({
        'name': hubsite_service_syncthing_name,
        'url': hubsite_service_syncthing_url,
        'logo_location': hubsite_service_syncthing_logo_location,
        'description': hubsite_service_syncthing_description,
        'priority': hubsite_service_syncthing_priority
      } if hubsite_service_syncthing_enabled else omit)
    }}
  # /role-specific:syncthing

  # role-specific:tandoor
  - |-
    {{
      ({
        'name': hubsite_service_tandoor_name,
        'url': hubsite_service_tandoor_url,
        'logo_location': hubsite_service_tandoor_logo_location,
        'description': hubsite_service_tandoor_description,
        'priority': hubsite_service_tandoor_priority,
      } if hubsite_service_tandoor_enabled else omit)
    }}
  # /role-specific:tandoor

  # role-specific:vaultwarden
  - |-
    {{
      ({
        'name': hubsite_service_vaultwarden_name,
        'url': hubsite_service_vaultwarden_url,
        'logo_location': hubsite_service_vaultwarden_logo_location,
        'description': hubsite_service_vaultwarden_description,
        'priority': hubsite_service_vaultwarden_priority
      } if hubsite_service_vaultwarden_enabled else omit)
    }}
  # /role-specific:vaultwarden

  # role-specific:woodpecker_ci_server
  - |-
    {{
      ({
        'name': hubsite_service_woodpecker_ci_name,
        'url': hubsite_service_woodpecker_ci_url,
        'logo_location': hubsite_service_woodpecker_ci_logo_location,
        'description': hubsite_service_woodpecker_ci_description,
        'priority': hubsite_service_woodpecker_ci_priority
      } if hubsite_service_woodpecker_ci_enabled else omit)
    }}
  # /role-specific:woodpecker_ci_server

  # role-specific:writefreely
  - |-
    {{
      ({
        'name': hubsite_service_writefreely_name,
        'url': hubsite_service_writefreely_url,
        'logo_location': hubsite_service_writefreely_logo_location,
        'description': hubsite_service_writefreely_description,
        'priority': hubsite_service_writefreely_priority,
      } if hubsite_service_writefreely_enabled else omit)
    }}
  # /role-specific:writefreely

hubsite_service_list_auto: "{{ mash_playbook_hubsite_service_list_auto_itemized | reject('equalto', omit) }}"

########################################################################
#                                                                      #
# /hubsite                                                             #
#                                                                      #
########################################################################
# /role-specific:hubsite



# role-specific:ilmo
########################################################################
#                                                                      #
# ilmo                                                                 #
#                                                                      #
########################################################################

ilmo_enabled: false

ilmo_identifier: "{{ mash_playbook_service_identifier_prefix }}ilmo"

ilmo_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}ilmo"

ilmo_uid: "{{ mash_playbook_uid }}"
ilmo_gid: "{{ mash_playbook_gid }}"

ilmo_secret: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'secret.ilmo', rounds=655555) | to_uuid }}"

ilmo_database_host: "{{ postgres_identifier if postgres_enabled else '' }}"
ilmo_database_port: "{{ '5432' if postgres_enabled else '' }}"
ilmo_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.ilmo', rounds=655555) | to_uuid }}"
ilmo_database_username: "ilmo"

ilmo_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and ilmo_database_host == postgres_identifier else [])
  }}

ilmo_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and ilmo_database_host == postgres_identifier and ilmo_container_network != postgres_container_network else [])
  }}

ilmo_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
ilmo_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
ilmo_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
ilmo_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /ilmo                                                                #
#                                                                      #
########################################################################
# /role-specific:ilmo



# role-specific:infisical
########################################################################
#                                                                      #
# infisical                                                            #
#                                                                      #
########################################################################

infisical_enabled: false

infisical_identifier: "{{ mash_playbook_service_identifier_prefix }}infisical"

infisical_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}infisical"

infisical_uid: "{{ mash_playbook_uid }}"
infisical_gid: "{{ mash_playbook_gid }}"

infisical_backend_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([mongodb_identifier ~ '.service'] if mongodb_enabled and infisical_mongodb_hostname == mongodb_identifier else [])
  }}

infisical_backend_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([mongodb_container_network] if mongodb_enabled and infisical_mongodb_hostname == mongodb_identifier and infisical_backend_container_network != mongodb_container_network else [])
  }}

infisical_backend_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
infisical_backend_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
infisical_backend_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
infisical_backend_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

# Intentionally not auto-generating infisical_backend_environment_variable_encryption_key here.
# We prefer it to be explicit as it seems important that it remains stable.

infisical_backend_environment_variable_jwt_signup_secret: "{{ ('%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'inf.jwt.signup', rounds=655555) | to_uuid | replace('-', ''))[0:32] }}"
infisical_backend_environment_variable_jwt_refresh_secret: "{{ ('%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'inf.jwt.r', rounds=655555) | to_uuid | replace('-', ''))[0:32] }}"
infisical_backend_environment_variable_jwt_auth_secret: "{{ ('%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'inf.jwt.a', rounds=655555) | to_uuid | replace('-', ''))[0:32] }}"
infisical_backend_environment_variable_jwt_service_secret: "{{ ('%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'inf.jwt.svc', rounds=655555) | to_uuid | replace('-', ''))[0:32] }}"

infisical_frontend_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

infisical_frontend_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
infisical_frontend_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
infisical_frontend_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
infisical_frontend_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

infisical_mongodb_hostname: "{{ mongodb_identifier if mongodb_enabled else '' }}"
infisical_mongodb_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'infisical.db', rounds=655555) | to_uuid }}"
infisical_mongodb_auth_source: "{{ infisical_mongodb_db_name }}"

########################################################################
#                                                                      #
# /infisical                                                           #
#                                                                      #
########################################################################
# /role-specific:infisical



# role-specific:influxdb
########################################################################
#                                                                      #
# influxdb                                                             #
#                                                                      #
########################################################################

influxdb_enabled: false

influxdb_identifier: "{{ mash_playbook_service_identifier_prefix }}influxdb"

influxdb_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}influxdb"

influxdb_uid: "{{ mash_playbook_uid }}"
influxdb_gid: "{{ mash_playbook_gid }}"

influxdb_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if (influxdb_container_labels_traefik_enabled and mash_playbook_reverse_proxyable_services_additional_network) else [])
  }}

influxdb_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
influxdb_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
influxdb_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
influxdb_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /influxdb                                                            #
#                                                                      #
########################################################################
# /role-specific:influxdb



# role-specific:jackett
########################################################################
#                                                                      #
# jackett                                                              #
#                                                                      #
########################################################################

jackett_enabled: false

jackett_identifier: "{{ mash_playbook_service_identifier_prefix }}jackett"

jackett_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}jackett"

jackett_uid: "{{ mash_playbook_uid }}"
jackett_gid: "{{ mash_playbook_gid }}"

jackett_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

jackett_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
jackett_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
jackett_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
jackett_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /jackett                                                             #
#                                                                      #
########################################################################
# /role-specific:jackett



# role-specific:jitsi
########################################################################
#                                                                      #
# jitsi                                                                #
#                                                                      #
########################################################################

jitsi_enabled: false

jitsi_architecture: "{{ mash_playbook_architecture }}"

jitsi_identifier: "{{ mash_playbook_service_identifier_prefix }}jitsi"

jitsi_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}jitsi"

jitsi_uid: "{{ mash_playbook_uid }}"
jitsi_gid: "{{ mash_playbook_gid }}"

jitsi_user_username: "{{ mash_playbook_user_username }}"

jitsi_web_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

jitsi_prosody_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

jitsi_jvb_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

jitsi_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
jitsi_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
jitsi_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
jitsi_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

jitsi_jibri_xmpp_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'jibri', rounds=655555) | to_uuid }}"
jitsi_jicofo_auth_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'jicofo', rounds=655555) | to_uuid }}"
jitsi_jvb_auth_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'jvb', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /jitsi                                                               #
#                                                                      #
########################################################################
# /role-specific:jitsi



# role-specific:keycloak
########################################################################
#                                                                      #
# keycloak                                                             #
#                                                                      #
########################################################################

keycloak_enabled: false

keycloak_identifier: "{{ mash_playbook_service_identifier_prefix }}keycloak"

keycloak_uid: "{{ mash_playbook_uid }}"
keycloak_gid: "{{ mash_playbook_gid }}"

keycloak_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}keycloak"

keycloak_systemd_required_systemd_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and keycloak_database_hostname == postgres_identifier else [])
  }}

keycloak_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and keycloak_database_hostname == postgres_identifier and keycloak_container_network != postgres_container_network else [])
  }}

keycloak_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
keycloak_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
keycloak_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
keycloak_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

keycloak_container_labels_metrics_hostname: "{{ mash_playbook_metrics_exposure_hostname }}"
keycloak_container_labels_metrics_path_prefix: "{{ mash_playbook_metrics_exposure_path_prefix }}/{{ keycloak_identifier }}"
keycloak_container_labels_metrics_middleware_basic_auth_enabled: "{{ mash_playbook_metrics_exposure_http_basic_auth_enabled }}"
keycloak_container_labels_metrics_middleware_basic_auth_users: "{{ mash_playbook_metrics_exposure_http_basic_auth_users }}"

keycloak_environment_variable_kc_metrics_enabled: "{{ prometheus_enabled | default(false) or mash_playbook_metrics_exposure_enabled }}"

keycloak_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
keycloak_database_port: "{{ '5432' if postgres_enabled else '' }}"
keycloak_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.keycloak', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /keycloak                                                            #
#                                                                      #
########################################################################
# /role-specific:keycloak



# role-specific:labelstudio
########################################################################
#                                                                      #
# labelstudio                                                          #
#                                                                      #
########################################################################

labelstudio_enabled: false

labelstudio_identifier: "{{ mash_playbook_service_identifier_prefix }}labelstudio"

labelstudio_uid: "{{ mash_playbook_uid }}"
labelstudio_gid: "{{ mash_playbook_gid }}"

labelstudio_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}labelstudio"

labelstudio_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and labelstudio_database_hostname == postgres_identifier and labelstudio_container_network != postgres_container_network else [])
  }}

labelstudio_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
labelstudio_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
labelstudio_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
labelstudio_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

labelstudio_container_labels_traefik_compression_middleware_enabled: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_enabled }}"
labelstudio_container_labels_traefik_compression_middleware_name: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_name if mash_playbook_reverse_proxy_traefik_middleware_compession_enabled else '' }}"

labelstudio_postgres_backend_enabled: "{{ postgres_enabled }}"
labelstudio_database_name: "labelstudio"
labelstudio_database_username: "labelstudio"
labelstudio_database_port: "{{ '5432' if postgres_enabled else '' }}"
labelstudio_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
labelstudio_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.labelstudio', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /labelstudio                                                         #
#                                                                      #
########################################################################
# /role-specific:labelstudio



# role-specific:lago
########################################################################
#                                                                      #
# lago                                                                 #
#                                                                      #
########################################################################

lago_enabled: false

lago_architecture: "{{ mash_playbook_architecture }}"

lago_identifier: "{{ mash_playbook_service_identifier_prefix }}lago"

lago_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}lago"

lago_uid: "{{ mash_playbook_uid }}"
lago_gid: "{{ mash_playbook_gid }}"

lago_api_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and lago_database_hostname == postgres_identifier and lago_api_container_network != postgres_container_network else [])
  }}

lago_front_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

lago_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
lago_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
lago_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
lago_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

lago_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
lago_database_port: "{{ '5432' if postgres_enabled else '' }}"
lago_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'lago.db', rounds=655555) | to_uuid }}"

lago_api_environment_variable_secret_key_base: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'lago.sec.key', rounds=655555) | to_uuid }}"
lago_api_environment_variable_encryption_primary_key: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'lago.enc.primary', rounds=655555) | to_uuid }}"
lago_api_environment_variable_encryption_deterministic_key: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'lago.deter.key', rounds=655555) | to_uuid }}"
lago_api_environment_variable_encryption_key_derivation_salt: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'lago.deriv.salt', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /lago                                                                #
#                                                                      #
########################################################################
# /role-specific:lago



# role-specific:languagetool
########################################################################
#                                                                      #
# languagetool                                                         #
#                                                                      #
########################################################################

languagetool_enabled: false

languagetool_identifier: "{{ mash_playbook_service_identifier_prefix }}languagetool"

languagetool_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}languagetool"

languagetool_uid: "{{ mash_playbook_uid }}"
languagetool_gid: "{{ mash_playbook_gid }}"

languagetool_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
languagetool_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
languagetool_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
languagetool_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

languagetool_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

########################################################################
#                                                                      #
# /languagetool                                                        #
#                                                                      #
########################################################################
# /role-specific:languagetool



# role-specific:loki
########################################################################
#                                                                      #
# loki                                                                 #
#                                                                      #
########################################################################

loki_enabled: false

loki_identifier: "{{ mash_playbook_service_identifier_prefix }}loki"

loki_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}loki"

loki_uid: "{{ mash_playbook_uid }}"
loki_gid: "{{ mash_playbook_gid }}"

# Only enable Traefik labels if a hostname is set (indicating that this will be exposed publicly)
loki_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled and loki_hostname | length > 0 }}"
loki_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
loki_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
loki_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

loki_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

########################################################################
#                                                                      #
# /loki                                                                #
#                                                                      #
########################################################################
# /role-specific:loki



# role-specific:linkding
########################################################################
#                                                                      #
# linkding                                                             #
#                                                                      #
########################################################################

linkding_enabled: false

linkding_identifier: "{{ mash_playbook_service_identifier_prefix }}linkding"

linkding_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}linkding"

linkding_uid: "{{ mash_playbook_uid }}"
linkding_gid: "{{ mash_playbook_gid }}"

linkding_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and linkding_database_hostname == postgres_identifier else [])
  }}

linkding_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and linkding_database_hostname == postgres_identifier and linkding_container_network != postgres_container_network else [])
  }}

linkding_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
linkding_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
linkding_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
linkding_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

linkding_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
linkding_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'linkding.db', rounds=655555) | to_uuid }}"
linkding_database_engine: "{{ 'postgres' if postgres_enabled and linkding_database_hostname == postgres_connection_hostname else 'sqlite' }}"

########################################################################
#                                                                      #
# /linkding                                                            #
#                                                                      #
########################################################################
# /role-specific:linkding



# role-specific:freescout
########################################################################
#                                                                      #
# freescout                                                            #
#                                                                      #
########################################################################

freescout_enabled: false

freescout_identifier: "{{ mash_playbook_service_identifier_prefix }}freescout"

freescout_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}freescout"

freescout_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and freescout_database_hostname == postgres_identifier else [])
  }}

freescout_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and freescout_database_hostname == postgres_identifier and freescout_container_network != postgres_container_network else [])
  }}

freescout_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
freescout_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
freescout_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
freescout_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

freescout_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
freescout_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'freescout.db', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /freescout                                                           #
#                                                                      #
########################################################################
# /role-specific:freescout



# role-specific:miniflux
########################################################################
#                                                                      #
# miniflux                                                             #
#                                                                      #
########################################################################

miniflux_enabled: false

miniflux_identifier: "{{ mash_playbook_service_identifier_prefix }}miniflux"

miniflux_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}miniflux"

miniflux_uid: "{{ mash_playbook_uid }}"
miniflux_gid: "{{ mash_playbook_gid }}"

miniflux_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and miniflux_database_hostname == postgres_identifier else [])
  }}

miniflux_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and miniflux_database_hostname == postgres_identifier and miniflux_container_network != postgres_container_network else [])
  }}

miniflux_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
miniflux_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
miniflux_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
miniflux_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

miniflux_container_labels_traefik_compression_middleware_enabled: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_enabled }}"
miniflux_container_labels_traefik_compression_middleware_name: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_name if mash_playbook_reverse_proxy_traefik_middleware_compession_enabled else '' }}"

miniflux_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
miniflux_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'miniflux.db', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /miniflux                                                            #
#                                                                      #
########################################################################
# /role-specific:miniflux



# role-specific:mobilizon
########################################################################
#                                                                      #
# mobilizon                                                            #
#                                                                      #
########################################################################

mobilizon_enabled: false

mobilizon_identifier: "{{ mash_playbook_service_identifier_prefix }}mobilizon"

mobilizon_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}mobilizon"

mobilizon_uid: "{{ mash_playbook_uid }}"
mobilizon_gid: "{{ mash_playbook_gid }}"

mobilizon_secret_key: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'sk.mobilizon', rounds=655555) | to_uuid }}"
mobilizon_secret_key_base: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'skb.mobilizon', rounds=655555) | to_uuid }}"

mobilizon_database_hostname: "{{ postgis_identifier if postgis_enabled else '' }}"
mobilizon_database_name: "mobilizon"
mobilizon_database_port: "{{ '5432' if postgis_enabled else '' }}"
mobilizon_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.mobilizon', rounds=655555) | to_uuid }}"
mobilizon_database_username: "mobilizon"

mobilizon_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgis_identifier ~ '.service'] if postgis_enabled and mobilizon_database_hostname == postgis_identifier else [])
  }}

mobilizon_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgis_container_network] if postgis_enabled and mobilizon_database_hostname == postgis_identifier and mobilizon_container_network != postgis_container_network else [])
  }}

mobilizon_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
mobilizon_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
mobilizon_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
mobilizon_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /mobilizon                                                           #
#                                                                      #
########################################################################
# /role-specific:mobilizon



# role-specific:mongodb
########################################################################
#                                                                      #
# mongodb                                                              #
#                                                                      #
########################################################################

mongodb_enabled: false

mongodb_identifier: "{{ mash_playbook_service_identifier_prefix }}mongodb"

mongodb_uid: "{{ mash_playbook_uid }}"
mongodb_gid: "{{ mash_playbook_gid }}"

mongodb_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}mongodb"

mongodb_managed_databases_auto: |
  {{
    ([{
      'name': infisical_mongodb_db_name,
      'username': infisical_mongodb_username,
      'password': infisical_mongodb_password,
    }] if infisical_enabled and infisical_mongodb_hostname == mongodb_identifier else [])
  }}

########################################################################
#                                                                      #
# /mongodb                                                             #
#                                                                      #
########################################################################
# /role-specific:mongodb


# role-specific:mosquitto
########################################################################
#                                                                      #
# mosquitto                                                            #
#                                                                      #
########################################################################

mosquitto_enabled: false

mosquitto_identifier: "{{ mash_playbook_service_identifier_prefix }}mosquitto"

mosquitto_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}mosquitto"

mosquitto_uid: "{{ mash_playbook_uid }}"
mosquitto_gid: "{{ mash_playbook_gid }}"

########################################################################
#                                                                      #
# /mosquitto                                                           #
#                                                                      #
########################################################################
# /role-specific:mosquitto


# role-specific:mrs
########################################################################
#                                                                      #
# mrs                                                                  #
#                                                                      #
########################################################################

mrs_enabled: false

mrs_identifier: "{{ mash_playbook_service_identifier_prefix }}mrs"

mrs_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}mrs"

mrs_uid: "{{ mash_playbook_uid }}"
mrs_gid: "{{ mash_playbook_gid }}"

mrs_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

mrs_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
mrs_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
mrs_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
mrs_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /mrs                                                                 #
#                                                                      #
########################################################################
# /role-specific:mrs



# role-specific:n8n
########################################################################
#                                                                      #
# n8n                                                                  #
#                                                                      #
########################################################################

n8n_enabled: false

n8n_identifier: "{{ mash_playbook_service_identifier_prefix }}n8n"

n8n_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}n8n"

# Please see the note attached to this comment on why we can't use mash's playbook uid and gid
# https://github.com/kinduff/ansible-docker-n8n/blob/v1.4.2/defaults/main.yml
n8n_uid: "1000"
n8n_gid: "1000"

n8n_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and n8n_database_hostname == postgres_identifier else [])
  }}

n8n_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and n8n_database_hostname == postgres_identifier and n8n_container_network != postgres_container_network else [])
  }}

n8n_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
n8n_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
n8n_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
n8n_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

n8n_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
n8n_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'n8n.db', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /n8n                                                                 #
#                                                                      #
########################################################################
# /role-specific:n8n



# role-specific:navidrome
########################################################################
#                                                                      #
# navidrome                                                            #
#                                                                      #
########################################################################

navidrome_enabled: false

navidrome_identifier: "{{ mash_playbook_service_identifier_prefix }}navidrome"

navidrome_uid: "{{ mash_playbook_uid }}"
navidrome_gid: "{{ mash_playbook_gid }}"

navidrome_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}navidrome"

navidrome_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

navidrome_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
navidrome_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
navidrome_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
navidrome_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

navidrome_container_labels_traefik_compression_middleware_enabled: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_enabled }}"
navidrome_container_labels_traefik_compression_middleware_name: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_name if mash_playbook_reverse_proxy_traefik_middleware_compession_enabled else '' }}"

########################################################################
#                                                                      #
# /navidrome                                                           #
#                                                                      #
########################################################################
# /role-specific:navidrome


# role-specific:neko
########################################################################
#                                                                      #
# neko                                                                 #
#                                                                      #
########################################################################

neko_enabled: false

neko_identifier: "{{ mash_playbook_service_identifier_prefix }}neko"

neko_uid: "{{ mash_playbook_uid }}"
neko_gid: "{{ mash_playbook_gid }}"

neko_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}neko"

neko_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

neko_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
neko_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
neko_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
neko_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /neko                                                                #
#                                                                      #
########################################################################
# /role-specific:neko



# role-specific:nextcloud
########################################################################
#                                                                      #
# nextcloud                                                            #
#                                                                      #
########################################################################

nextcloud_enabled: false

nextcloud_identifier: "{{ mash_playbook_service_identifier_prefix }}nextcloud"

nextcloud_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}nextcloud"

nextcloud_uid: "{{ mash_playbook_uid }}"
nextcloud_gid: "{{ mash_playbook_gid }}"

nextcloud_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and nextcloud_database_hostname == postgres_identifier else [])
  }}

nextcloud_systemd_wanted_services_list_auto: |
  {{
    ([(exim_relay_identifier | default('mash-exim-relay')) ~ '.service'] if (exim_relay_enabled | default(false) and nextcloud_config_parameter_mail_smtphost == exim_relay_identifier | default('mash-exim-relay')) else [])
  }}

nextcloud_container_additional_networks_auto: |
  {{
    (
      ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
      +
      ([postgres_container_network] if postgres_enabled and nextcloud_database_hostname == postgres_identifier and nextcloud_container_network != postgres_container_network else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and nextcloud_config_parameter_mail_smtphost == exim_relay_identifier | default('mash-exim-relay') and nextcloud_container_network != exim_relay_container_network) else [])
    ) | unique
  }}

nextcloud_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
nextcloud_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
nextcloud_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
nextcloud_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

nextcloud_container_labels_traefik_compression_middleware_enabled: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_enabled }}"
nextcloud_container_labels_traefik_compression_middleware_name: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_name if mash_playbook_reverse_proxy_traefik_middleware_compession_enabled else '' }}"

nextcloud_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
nextcloud_database_port: "{{ '5432' if postgres_enabled else '' }}"
nextcloud_database_username: "nextcloud"
nextcloud_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.nextcloud', rounds=655555) | to_uuid }}"

# role-specific:exim_relay
nextcloud_config_parameter_mail_smtphost: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
nextcloud_config_parameter_mail_smtpport: "{{ 8025 if exim_relay_enabled else '' }}"
nextcloud_config_parameter_mail_smtpsecure: ''
nextcloud_config_parameter_mail_smtpauth: false
nextcloud_config_parameter_mail_from_address: "{{ (exim_relay_sender_address | split('@'))[0] if exim_relay_enabled else '' }}"
nextcloud_config_parameter_mail_domain: "{{ (exim_relay_sender_address | split('@'))[1] if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /nextcloud                                                           #
#                                                                      #
########################################################################
# /role-specific:nextcloud



# role-specific:netbox
########################################################################
#                                                                      #
# netbox                                                               #
#                                                                      #
########################################################################

netbox_enabled: false

netbox_identifier: "{{ mash_playbook_service_identifier_prefix }}netbox"

netbox_uid: "{{ mash_playbook_uid }}"
netbox_gid: "{{ mash_playbook_gid }}"

netbox_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}netbox"

netbox_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and netbox_database_hostname == postgres_identifier else [])
  }}

netbox_container_additional_networks_auto: |
  {{
    (
      ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
      +
      ([postgres_container_network] if postgres_enabled and netbox_database_hostname == postgres_identifier and netbox_container_network != postgres_container_network else [])
    ) | unique
  }}

netbox_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
netbox_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
netbox_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
netbox_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

netbox_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
netbox_database_port: "{{ '5432' if postgres_enabled else '' }}"
netbox_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.netbox', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /netbox                                                              #
#                                                                      #
########################################################################
# /role-specific:netbox



# role-specific:mariadb
########################################################################
#                                                                      #
# mariadb                                                              #
#                                                                      #
########################################################################

mariadb_enabled: false

mariadb_identifier: "{{ mash_playbook_service_identifier_prefix }}mariadb"

mariadb_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}mariadb"

mariadb_uid: "{{ mash_playbook_uid }}"
mariadb_gid: "{{ mash_playbook_gid }}"

mash_playbook_mariadb_managed_databases_auto_itemized:
  # Dummy entry, which is not role-specific.
  # Ensures there's at least one entry defined in the list.
  - "{{ omit }}"

  # role-specific:authelia
  - |-
    {{
      ({
        'name': authelia_config_storage_mysql_database,
        'username': authelia_config_storage_mysql_username,
        'password': authelia_config_storage_mysql_password,
      } if authelia_enabled and authelia_config_storage_mysql_host == mariadb_identifier else omit)
    }}
  # /role-specific:authelia

  # role-specific:wordpress
  - |-
    {{
      ({
        'name': wordpress_database_name,
        'username': wordpress_database_username,
        'password': wordpress_database_password,
      } if wordpress_enabled and wordpress_database_hostname == mariadb_identifier | default('mash-mariadb') else omit)
    }}
  # /role-specific:wordpress

mariadb_managed_databases_auto: "{{ mash_playbook_mariadb_managed_databases_auto_itemized | reject('equalto', omit) }}"

########################################################################
#                                                                      #
# /mariadb                                                             #
#                                                                      #
########################################################################
# /role-specific:mariadb



# role-specific:notfellchen
########################################################################
#                                                                      #
# notfellchen                                                          #
#                                                                      #
########################################################################

notfellchen_enabled: false

notfellchen_identifier: "{{ mash_playbook_service_identifier_prefix }}notfellchen"

notfellchen_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}notfellchen"

notfellchen_uid: "{{ mash_playbook_uid }}"
notfellchen_gid: "{{ mash_playbook_gid }}"

notfellchen_secret: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'secret.nf', rounds=655555) | to_uuid }}"

notfellchen_database_host: "{{ postgres_identifier if postgres_enabled else '' }}"
notfellchen_database_port: "{{ '5432' if postgres_enabled else '' }}"
notfellchen_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.nf', rounds=655555) | to_uuid }}"
notfellchen_database_username: "notfellchen"

notfellchen_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and notfellchen_database_host == postgres_identifier else [])
  }}

notfellchen_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and notfellchen_database_host == postgres_identifier and notfellchen_container_network != postgres_container_network else [])
  }}

notfellchen_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
notfellchen_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
notfellchen_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
notfellchen_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

notfellchen_sws_container_labels_traefik_enabled: "{{ notfellchen_container_labels_traefik_enabled }}"
notfellchen_sws_container_labels_traefik_docker_network: "{{ notfellchen_container_labels_traefik_docker_network }}"
notfellchen_sws_container_labels_traefik_entrypoints: "{{ notfellchen_container_labels_traefik_entrypoints }}"
notfellchen_sws_container_labels_traefik_tls_certResolver: "{{ notfellchen_container_labels_traefik_tls_certResolver }}"


########################################################################
#                                                                      #
# /notfellchen                                                         #
#                                                                      #
########################################################################
# /role-specific:notfellchen


# role-specific:ntfy
########################################################################
#                                                                      #
# ntfy                                                                 #
#                                                                      #
########################################################################

ntfy_enabled: false

ntfy_identifier: "{{ mash_playbook_service_identifier_prefix }}ntfy"

ntfy_uid: "{{ mash_playbook_uid }}"
ntfy_gid: "{{ mash_playbook_gid }}"

ntfy_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}ntfy"

ntfy_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and ntfy_smtp_sender_addr_host == exim_relay_identifier | default('mash-exim-relay') and ntfy_container_network != exim_relay_container_network) else [])
  }}

ntfy_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
ntfy_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

ntfy_visitor_request_limit_exempt_hosts_hostnames_auto: |
  {{
    [ntfy_hostname]
  }}

# role-specific:traefik
ntfy_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
ntfy_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

# role-specific:exim_relay
ntfy_smtp_sender_enabled: "{{ 'true' if exim_relay_enabled else '' }}"
ntfy_smtp_sender_addr_host: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
ntfy_smtp_sender_addr_port: 8025
ntfy_smtp_sender_from: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /ntfy                                                                #
#                                                                      #
########################################################################
# /role-specific:ntfy


# role-specific:outline
########################################################################
#                                                                      #
# outline                                                              #
#                                                                      #
########################################################################

outline_enabled: false

outline_identifier: "{{ mash_playbook_service_identifier_prefix }}outline"

outline_uid: "{{ mash_playbook_uid }}"
outline_gid: "{{ mash_playbook_gid }}"

outline_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}outline"

outline_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and outline_database_hostname == postgres_identifier else [])
  }}

outline_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and outline_database_hostname == postgres_identifier and outline_container_network != postgres_container_network else [])
  }}

outline_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
outline_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
outline_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
outline_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

outline_environment_variable_utils_secret: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'utils.out', rounds=655555) | to_uuid }}"

outline_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
outline_database_port: "{{ '5432' if postgres_enabled else '' }}"
outline_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.out', rounds=655555) | to_uuid }}"
outline_database_sslmode: "{{ 'disable' if postgres_enabled and outline_database_hostname == postgres_identifier else 'prefer' }}"

########################################################################
#                                                                      #
# /outline                                                             #
#                                                                      #
########################################################################
# /role-specific:outline



# role-specific:oauth2_proxy
########################################################################
#                                                                      #
# oauth2_proxy                                                         #
#                                                                      #
########################################################################

oauth2_proxy_enabled: false

oauth2_proxy_identifier: "{{ mash_playbook_service_identifier_prefix }}oauth2-proxy"

oauth2_proxy_uid: "{{ mash_playbook_uid }}"
oauth2_proxy_gid: "{{ mash_playbook_gid }}"

oauth2_proxy_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}oauth2-proxy"

oauth2_proxy_container_network: "{{ (mash_playbook_reverse_proxyable_services_additional_network if mash_playbook_traefik_labels_enabled else '') | default(oauth2_proxy_identifier) }}"

oauth2_proxy_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
oauth2_proxy_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
oauth2_proxy_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
oauth2_proxy_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /oauth2_proxy                                                        #
#                                                                      #
########################################################################
# /role-specific:oauth2_proxy



# role-specific:overseerr
########################################################################
#                                                                      #
# overseerr                                                            #
#                                                                      #
########################################################################

overseerr_enabled: false

overseerr_identifier: "{{ mash_playbook_service_identifier_prefix }}overseerr"

overseerr_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}overseerr"

overseerr_uid: "{{ mash_playbook_uid }}"
overseerr_gid: "{{ mash_playbook_gid }}"

overseerr_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

overseerr_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
overseerr_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
overseerr_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
overseerr_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /overseerr                                                           #
#                                                                      #
########################################################################
# /role-specific:overseerr



# role-specific:owncast
########################################################################
#                                                                      #
# owncast                                                              #
#                                                                      #
########################################################################

owncast_enabled: false

owncast_identifier: "{{ mash_playbook_service_identifier_prefix }}owncast"

owncast_uid: "{{ mash_playbook_uid }}"
owncast_gid: "{{ mash_playbook_gid }}"

owncast_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}owncast"

owncast_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

owncast_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
owncast_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
owncast_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
owncast_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /owncast                                                             #
#                                                                      #
########################################################################
# /role-specific:owncast



# role-specific:oxitraffic
########################################################################
#                                                                      #
# oxitraffic                                                           #
#                                                                      #
########################################################################

oxitraffic_enabled: false

oxitraffic_identifier: "{{ mash_playbook_service_identifier_prefix }}oxitraffic"

oxitraffic_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}oxitraffic"

oxitraffic_uid: "{{ mash_playbook_uid }}"
oxitraffic_gid: "{{ mash_playbook_gid }}"

oxitraffic_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
oxitraffic_database_port: "{{ '5432' if postgres_enabled else '' }}"
oxitraffic_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.oxitraffic', rounds=655555) | to_uuid }}"

oxitraffic_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and oxitraffic_database_hostname == postgres_identifier else [])
  }}

oxitraffic_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and oxitraffic_database_hostname == postgres_identifier and oxitraffic_container_network != postgres_container_network else [])
  }}

oxitraffic_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
oxitraffic_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
oxitraffic_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
oxitraffic_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /oxitraffic                                                          #
#                                                                      #
########################################################################
# /role-specific:oxitraffic


# role-specific:paperless
########################################################################
#                                                                      #
# paperless                                                            #
#                                                                      #
########################################################################

paperless_enabled: false

paperless_identifier: "{{ mash_playbook_service_identifier_prefix }}paperless"

paperless_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}paperless"

paperless_uid: "{{ mash_playbook_uid }}"
paperless_gid: "{{ mash_playbook_gid }}"

paperless_secret_key: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'paperless.secret', rounds=655555) | to_uuid }}"

paperless_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
paperless_database_username: "paperless"
paperless_database_port: "{{ '5432' if postgres_enabled else '' }}"
paperless_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.paperless', rounds=655555) | to_uuid }}"

paperless_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and paperless_database_hostname == postgres_identifier else [])
  }}

paperless_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and paperless_database_hostname == postgres_identifier and paperless_container_network != postgres_container_network else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and paperless_email_host == exim_relay_identifier | default('mash-exim-relay') and paperless_container_network != exim_relay_container_network) else [])
  }}

paperless_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
paperless_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
paperless_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
paperless_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

# role-specific:exim_relay
paperless_email_host: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
paperless_email_port: "{{ 8025 if exim_relay_enabled else '' }}"
paperless_email_from: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /paperless                                                           #
#                                                                      #
########################################################################
# /role-specific:paperless



# role-specific:peertube
########################################################################
#                                                                      #
# peertube                                                             #
#                                                                      #
########################################################################

peertube_enabled: false

peertube_identifier: "{{ mash_playbook_service_identifier_prefix }}peertube"

peertube_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}peertube"

peertube_uid: "{{ mash_playbook_uid }}"
peertube_gid: "{{ mash_playbook_gid }}"

peertube_container_additional_networks_auto: |
  {{
    (
      ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
      +
      ([postgres_container_network] if postgres_enabled and peertube_config_database_hostname == postgres_identifier and peertube_container_network != postgres_container_network else [])
      +
      ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and peertube_config_smtp_hostname == exim_relay_identifier | default('mash-exim-relay') and peertube_container_network != exim_relay_container_network) else [])
    ) | unique
  }}

peertube_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
peertube_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
peertube_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
peertube_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

peertube_config_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
peertube_config_database_port: "{{ '5432' if postgres_enabled else '' }}"
peertube_config_database_username: peertube
peertube_config_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.peertube', rounds=655555) | to_uuid }}"

peertube_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and peertube_config_database_hostname == postgres_identifier else [])
  }}

peertube_systemd_wanted_services_list_auto: |
  {{
    ([(exim_relay_identifier | default('mash-exim-relay')) ~ '.service'] if (exim_relay_enabled | default(false) and peertube_config_smtp_hostname == exim_relay_identifier | default('mash-exim-relay')) else [])
  }}

# role-specific:exim_relay
peertube_config_smtp_hostname: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
peertube_config_smtp_port: "{{ 8025 if exim_relay_enabled else '' }}"
peertube_config_smtp_from: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /peertube                                                            #
#                                                                      #
########################################################################
# /role-specific:peertube


# role-specific:plausible
########################################################################
#                                                                      #
# plausible                                                            #
#                                                                      #
########################################################################

plausible_enabled: false

plausible_identifier: "{{ mash_playbook_service_identifier_prefix }}plausible"

plausible_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}plausible"

plausible_uid: "{{ mash_playbook_uid }}"
plausible_gid: "{{ mash_playbook_gid }}"

plausible_systemd_required_systemd_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and plausible_database_hostname == postgres_identifier else [])
    +
    ([clickhouse_identifier ~ '.service'] if clickhouse_enabled | default(false) and plausible_clickhouse_database_hostname == clickhouse_identifier| default('mash-clickhouse') else [])
  }}

plausible_systemd_wanted_systemd_services_list_auto: |
  {{
    ([(exim_relay_identifier | default('mash-exim-relay')) ~ '.service'] if (exim_relay_enabled | default(false) and plausible_environment_variable_smtp_host_addr == exim_relay_identifier | default('mash-exim-relay')) else [])
  }}

plausible_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and plausible_database_hostname == postgres_identifier and plausible_container_network != postgres_container_network else [])
    +
    ([clickhouse_container_network | default('mash-clickhouse')] if (clickhouse_enabled | default(false) and plausible_clickhouse_database_hostname == clickhouse_identifier | default('mash-clickhouse') and plausible_container_network != clickhouse_container_network | default('mash-clickhouse')) else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and plausible_environment_variable_smtp_host_addr == exim_relay_identifier | default('mash-exim-relay') and plausible_container_network != exim_relay_container_network) else [])
  }}

plausible_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
plausible_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
plausible_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
plausible_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

plausible_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
plausible_database_password: "{{ ('%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'plausible.db', rounds=655555) | to_uuid) if postgres_enabled else '' }}"

# role-specific:clickhouse
plausible_clickhouse_database_hostname: "{{ clickhouse_identifier if clickhouse_enabled else '' }}"
plausible_clickhouse_database_password: "{{ ('%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'plaus.clk.db', rounds=655555) | to_uuid) if clickhouse_enabled else '' }}"
# /role-specific:clickhouse

# role-specific:exim_relay
# As of 2024-06-28, only `Bamboo.SMTPAdapter` behaves well when no SMTP username/password AUTH is required (as is the case for exim-relay).
# The Bamboo.Mua SMTP adapter is more modern, but always sends authentication, even when the SMTP user is empty.
plausible_environment_variable_mailer_adapter: "{{ 'Bamboo.SMTPAdapter' if exim_relay_enabled else 'Bamboo.LocalAdapter' }}"
plausible_environment_variable_mailer_email: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
plausible_environment_variable_smtp_host_addr: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
plausible_environment_variable_smtp_host_port: "{{ 8025 if exim_relay_enabled else '587' }}"
plausible_environment_variable_smtp_host_ssl_enabled: false
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /plausible                                                           #
#                                                                      #
########################################################################
# /role-specific:plausible



# role-specific:postgis
########################################################################
#                                                                      #
# postgis                                                              #
#                                                                      #
########################################################################

postgis_enabled: false

postgis_identifier: "{{ mash_playbook_service_identifier_prefix }}postgis"

postgis_architecture: "{{ mash_playbook_architecture }}"

postgis_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}postgis"

postgis_uid: "{{ mash_playbook_uid }}"
postgis_gid: "{{ mash_playbook_gid }}"

postgis_systemd_services_to_stop_for_maintenance_list: |
  {{
    ([(mobilizon_identifier + '.service')] if mobilizon_enabled else [])
  }}

postgis_managed_databases_auto: |
  {{
    ([{
      'name': mobilizon_database_name,
      'username': mobilizon_database_username,
      'password': mobilizon_database_password,
    }] if mobilizon_enabled and mobilizon_database_type == 'postgis' and mobilizon_database_hostname == postgis_identifier else [])
  }}

########################################################################
#                                                                      #
# /postgis                                                             #
#                                                                      #
########################################################################
# /role-specific:postgis




# role-specific:prometheus_postgres_exporter
########################################################################
#                                                                      #
# prometheus_postgres_exporter                                         #
#                                                                      #
########################################################################

prometheus_postgres_exporter_enabled: false

prometheus_postgres_exporter_identifier: "{{ mash_playbook_service_identifier_prefix }}prometheus-postgres-exporter"

prometheus_postgres_exporter_hostname: "{{ mash_playbook_metrics_exposure_hostname }}"
prometheus_postgres_exporter_path_prefix: "{{ mash_playbook_metrics_exposure_path_prefix }}/{{ prometheus_postgres_exporter_identifier }}"

prometheus_postgres_exporter_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}prometheus-postgres-exporter"

prometheus_postgres_exporter_uid: "{{ mash_playbook_uid }}"
prometheus_postgres_exporter_gid: "{{ mash_playbook_gid }}"

prometheus_postgres_exporter_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and prometheus_postgres_exporter_database_hostname == postgres_identifier and prometheus_postgres_exporter_container_network != postgres_container_network else [])
  }}

# Only enable Traefik labels if a hostname is set (indicating that this will be exposed publicly)
prometheus_postgres_exporter_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled and prometheus_postgres_exporter_hostname | length > 0 }}"
prometheus_postgres_exporter_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
prometheus_postgres_exporter_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
prometheus_postgres_exporter_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

prometheus_postgres_exporter_container_labels_metrics_middleware_basic_auth_enabled: "{{ mash_playbook_metrics_exposure_http_basic_auth_enabled }}"
prometheus_postgres_exporter_container_labels_metrics_middleware_basic_auth_users: "{{ mash_playbook_metrics_exposure_http_basic_auth_users }}"

prometheus_postgres_exporter_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
prometheus_postgres_exporter_database_username: prometheus_postgres_exporter
prometheus_postgres_exporter_database_password: "{{ postgres_connection_password if postgres_enabled else '' }}"
prometheus_postgres_exporter_database_port: "{{ postgres_connection_port if postgres_enabled else 5432 }}"
prometheus_postgres_exporter_database_ssl: false

prometheus_postgres_exporter_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled else [])
  }}

########################################################################
#                                                                      #
# /prometheus_postgres_exporter                                        #
#                                                                      #
########################################################################
# /role-specific:prometheus_postgres_exporter



# role-specific:prometheus
########################################################################
#                                                                      #
# prometheus                                                           #
#                                                                      #
########################################################################

prometheus_enabled: false

prometheus_identifier: "{{ mash_playbook_service_identifier_prefix }}prometheus"

prometheus_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}prometheus"

prometheus_uid: "{{ mash_playbook_uid }}"
prometheus_gid: "{{ mash_playbook_gid }}"

# Only enable Traefik labels if a hostname is set (indicating that this will be exposed publicly)
prometheus_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled and prometheus_hostname | length > 0 }}"
prometheus_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
prometheus_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
prometheus_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

prometheus_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}


########################################################################
#                                                                      #
# /prometheus                                                          #
#                                                                      #
########################################################################
# /role-specific:prometheus



# role-specific:prometheus_blackbox_exporter
########################################################################
#                                                                      #
# prometheus_blackbox_exporter                                         #
#                                                                      #
########################################################################

prometheus_blackbox_exporter_enabled: false

prometheus_blackbox_exporter_identifier: "{{ mash_playbook_service_identifier_prefix }}prometheus-blackbox-exporter"

prometheus_blackbox_exporter_hostname: "{{ mash_playbook_metrics_exposure_hostname }}"
prometheus_blackbox_exporter_path_prefix: "{{ mash_playbook_metrics_exposure_path_prefix }}/{{ prometheus_blackbox_exporter_identifier }}"

prometheus_blackbox_exporter_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}prometheus-blackbox-exporter"

prometheus_blackbox_exporter_uid: "{{ mash_playbook_uid }}"
prometheus_blackbox_exporter_gid: "{{ mash_playbook_gid }}"

prometheus_blackbox_exporter_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

# Only enable Traefik labels if a hostname is set (indicating that this will be exposed publicly)
prometheus_blackbox_exporter_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled and prometheus_blackbox_exporter_hostname }}"
prometheus_blackbox_exporter_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
prometheus_blackbox_exporter_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
prometheus_blackbox_exporter_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

prometheus_blackbox_exporter_container_labels_metrics_middleware_basic_auth_enabled: "{{ mash_playbook_metrics_exposure_http_basic_auth_enabled }}"
prometheus_blackbox_exporter_container_labels_metrics_middleware_basic_auth_users: "{{ mash_playbook_metrics_exposure_http_basic_auth_users }}"

########################################################################
#                                                                      #
# /prometheus_blackbox_exporter                                        #
#                                                                      #
########################################################################
# /role-specific:prometheus_blackbox_exporter



# role-specific:prometheus_ssh_exporter
########################################################################
#                                                                      #
# prometheus_ssh_exporter                                              #
#                                                                      #
########################################################################

prometheus_ssh_exporter_enabled: false

prometheus_ssh_exporter_identifier: "{{ mash_playbook_service_identifier_prefix }}prometheus-ssh-exporter"

prometheus_ssh_exporter_hostname: "{{ mash_playbook_metrics_exposure_hostname }}"
prometheus_ssh_exporter_path_prefix: "{{ mash_playbook_metrics_exposure_path_prefix }}/{{ prometheus_ssh_exporter_identifier }}"

prometheus_ssh_exporter_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}prometheus-ssh-exporter"

prometheus_ssh_exporter_uid: "{{ mash_playbook_uid }}"
prometheus_ssh_exporter_gid: "{{ mash_playbook_gid }}"

prometheus_ssh_exporter_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

# Only enable Traefik labels if a hostname is set (indicating that this will be exposed publicly)
prometheus_ssh_exporter_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled and prometheus_ssh_exporter_hostname }}"
prometheus_ssh_exporter_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
prometheus_ssh_exporter_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
prometheus_ssh_exporter_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

prometheus_ssh_exporter_container_labels_metrics_middleware_basic_auth_enabled: "{{ mash_playbook_metrics_exposure_http_basic_auth_enabled }}"
prometheus_ssh_exporter_container_labels_metrics_middleware_basic_auth_users: "{{ mash_playbook_metrics_exposure_http_basic_auth_users }}"

########################################################################
#                                                                      #
# /prometheus_ssh_exporter                                             #
#                                                                      #
########################################################################
# /role-specific:prometheus_ssh_exporter



# role-specific:prometheus_node_exporter
########################################################################
#                                                                      #
# prometheus_node_exporter                                             #
#                                                                      #
########################################################################

prometheus_node_exporter_enabled: false

prometheus_node_exporter_identifier: "{{ mash_playbook_service_identifier_prefix }}prometheus-node-exporter"

prometheus_node_exporter_hostname: "{{ mash_playbook_metrics_exposure_hostname }}"
prometheus_node_exporter_path_prefix: "{{ mash_playbook_metrics_exposure_path_prefix }}/{{ prometheus_node_exporter_identifier }}"

prometheus_node_exporter_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}prometheus-node-exporter"

prometheus_node_exporter_uid: "{{ mash_playbook_uid }}"
prometheus_node_exporter_gid: "{{ mash_playbook_gid }}"

prometheus_node_exporter_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

# Only enable Traefik labels if a hostname is set (indicating that this will be exposed publicly)
prometheus_node_exporter_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled and prometheus_node_exporter_hostname }}"
prometheus_node_exporter_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
prometheus_node_exporter_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
prometheus_node_exporter_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

prometheus_node_exporter_container_labels_metrics_middleware_basic_auth_enabled: "{{ mash_playbook_metrics_exposure_http_basic_auth_enabled }}"
prometheus_node_exporter_container_labels_metrics_middleware_basic_auth_users: "{{ mash_playbook_metrics_exposure_http_basic_auth_users }}"

prometheus_node_exporter_process_extra_arguments:
  - "--collector.disable-defaults"
  - "--collector.cpu"
  - "--collector.filesystem"
  - "--collector.meminfo"
  - "--collector.systemd"
  - "--collector.uname"

prometheus_node_exporter_container_extra_arguments:
  - "--security-opt apparmor=unconfined"
  - "--mount type=bind,src=/var/run/dbus/system_bus_socket,dst=/var/run/dbus/system_bus_socket,ro,bind-propagation=rslave"

########################################################################
#                                                                      #
# /prometheus_node_exporter                                            #
#                                                                      #
########################################################################
# /role-specific:prometheus_node_exporter



# role-specific:promtail
########################################################################
#                                                                      #
# promtail                                                             #
#                                                                      #
########################################################################

promtail_enabled: false

promtail_identifier: "{{ mash_playbook_service_identifier_prefix }}promtail"

promtail_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}promtail"

promtail_uid: "{{ mash_playbook_uid }}"
promtail_gid: "{{ mash_playbook_gid }}"

# role-specific:loki
promtail_config_clients_auto: |
  {{
    ([{
      'url': ('http://' + loki_identifier + ':' + (loki_server_http_listen_port | string) + '/loki/api/v1/push'),
      'tenant_id': 'mash',
    }] if loki_enabled else [])
  }}
# /role-specific:loki

# Only enable Traefik labels if a hostname is set (indicating that this will be exposed publicly)
promtail_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
promtail_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
promtail_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
promtail_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

promtail_container_labels_metrics_enabled: "{{ prometheus_enabled | default(false) or mash_playbook_metrics_exposure_enabled }}"
promtail_container_labels_metrics_hostname: "{{ mash_playbook_metrics_exposure_hostname }}"
promtail_container_labels_metrics_path_prefix: "{{ mash_playbook_metrics_exposure_path_prefix }}/{{ promtail_identifier }}"
promtail_container_labels_metrics_traefik_middleware_basic_auth_enabled: "{{ mash_playbook_metrics_exposure_http_basic_auth_enabled }}"
promtail_container_labels_metrics_traefik_middleware_basic_auth_users: "{{ mash_playbook_metrics_exposure_http_basic_auth_users }}"

promtail_container_additional_networks_auto: |
  {{
    (
      ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
      +
      ([loki_container_network] if (loki_enabled | default(false) and loki_container_network | default('') != promtail_container_network) else [])
    ) | unique
  }}

########################################################################
#                                                                      #
# /promtail                                                            #
#                                                                      #
########################################################################
# /role-specific:promtail



# role-specific:qbittorrent
########################################################################
#                                                                      #
# qbittorrent                                                          #
#                                                                      #
########################################################################

qbittorrent_enabled: false

qbittorrent_identifier: "{{ mash_playbook_service_identifier_prefix }}qbittorrent"

qbittorrent_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}qbittorrent"

qbittorrent_uid: "{{ mash_playbook_uid }}"
qbittorrent_gid: "{{ mash_playbook_gid }}"

qbittorrent_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

qbittorrent_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
qbittorrent_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
qbittorrent_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
qbittorrent_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /qbittorrent                                                         #
#                                                                      #
########################################################################
# /role-specific:qbittorrent



# role-specific:radarr
########################################################################
#                                                                      #
# radarr                                                               #
#                                                                      #
########################################################################

radarr_enabled: false

radarr_identifier: "{{ mash_playbook_service_identifier_prefix }}radarr"

radarr_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}radarr"

radarr_uid: "{{ mash_playbook_uid }}"
radarr_gid: "{{ mash_playbook_gid }}"

radarr_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

radarr_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
radarr_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
radarr_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
radarr_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /radarr                                                              #
#                                                                      #
########################################################################
# /role-specific:radarr



# role-specific:radicale
########################################################################
#                                                                      #
# radicale                                                             #
#                                                                      #
########################################################################

radicale_enabled: false

radicale_identifier: "{{ mash_playbook_service_identifier_prefix }}radicale"

radicale_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}radicale"

radicale_uid: "{{ mash_playbook_uid }}"
radicale_gid: "{{ mash_playbook_gid }}"

radicale_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

radicale_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
radicale_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
radicale_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
radicale_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /radicale                                                            #
#                                                                      #
########################################################################
# /role-specific:radicale



# role-specific:redmine
########################################################################
#                                                                      #
# redmine                                                              #
#                                                                      #
########################################################################

redmine_enabled: false

redmine_identifier: "{{ mash_playbook_service_identifier_prefix }}redmine"

redmine_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}redmine"

redmine_uid: "{{ mash_playbook_uid }}"
redmine_gid: "{{ mash_playbook_gid }}"

redmine_secret_key_base: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'secret.base', rounds=655555) | to_uuid }}"
redmine_secret_token: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'secret.token', rounds=655555) | to_uuid }}"
redmine_database_cipher_key: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.cipherkey', rounds=655555) | to_uuid }}"

redmine_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and redmine_database_hostname == postgres_identifier else [])
  }}

redmine_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and redmine_database_hostname == postgres_identifier and redmine_container_network != postgres_container_network else [])
  }}

redmine_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
redmine_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
redmine_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
redmine_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

redmine_database_type: "{{ 'postgresql' if postgres_enabled else 'sqlite3' }}"
redmine_database_hostname: "{{ postgres_connection_hostname if postgres_enabled else '' }}"
redmine_database_username: "redmine"
redmine_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'redmine.db', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /redmine                                                             #
#                                                                      #
########################################################################
# /role-specific:redmine



# role-specific:redis
########################################################################
#                                                                      #
# redis                                                                #
#                                                                      #
########################################################################

redis_enabled: false

redis_identifier: "{{ mash_playbook_service_identifier_prefix }}redis"

redis_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}redis"

redis_uid: "{{ mash_playbook_uid }}"
redis_gid: "{{ mash_playbook_gid }}"

########################################################################
#                                                                      #
# /redis                                                               #
#                                                                      #
########################################################################
# /role-specific:redis


# role-specific:keydb
########################################################################
#                                                                      #
# keydb                                                                #
#                                                                      #
########################################################################

keydb_enabled: false

keydb_identifier: "{{ mash_playbook_service_identifier_prefix }}keydb"

keydb_uid: "{{ mash_playbook_uid }}"
keydb_gid: "{{ mash_playbook_gid }}"

keydb_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}keydb"

keydb_arch: |-
  {{
    ({
      'amd64': 'x86_64',
      'arm32': 'arm32',
      'arm64': 'arm64',
    })[mash_playbook_architecture]
  }}

########################################################################
#                                                                      #
# /keydb                                                               #
#                                                                      #
########################################################################
# /role-specific:keydb


# role-specific:roundcube
########################################################################
#                                                                      #
# roundcube                                                            #
#                                                                      #
########################################################################

roundcube_enabled: false

roundcube_identifier: "{{ mash_playbook_service_identifier_prefix }}roundcube"

roundcube_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}roundcube"

roundcube_uid: "0"
roundcube_gid: "0"

roundcube_database_type: "{{ 'postgresql' if postgres_enabled else 'sqlite' }}"
roundcube_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
roundcube_database_port: "{{ '5432' if postgres_enabled else '' }}"
roundcube_database_name: "{{ 'roundcube' if postgres_enabled else '' }}"
roundcube_database_username: "{{ 'roundcube' if postgres_enabled else '' }}"
roundcube_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.roundcube', rounds=655555) | to_uuid }}"

roundcube_systemd_required_systemd_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and roundcube_database_hostname == postgres_identifier else [])
  }}

roundcube_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
roundcube_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
roundcube_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
roundcube_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

roundcube_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and roundcube_database_hostname == postgres_identifier and roundcube_container_network != postgres_container_network else [])
  }}

########################################################################
#                                                                      #
# /roundcube                                                           #
#                                                                      #
########################################################################
# /role-specific:roundcube



# role-specific:rumqttd
########################################################################
#                                                                      #
# rumqttd                                                              #
#                                                                      #
########################################################################

rumqttd_enabled: false

rumqttd_identifier: "{{ mash_playbook_service_identifier_prefix }}rumqttd"

rumqttd_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}rumqttd"

rumqttd_uid: "{{ mash_playbook_uid }}"
rumqttd_gid: "{{ mash_playbook_gid }}"

########################################################################
#                                                                      #
# /rumqttd                                                             #
#                                                                      #
########################################################################
# /role-specific:rumqttd



# role-specific:searxng
########################################################################
#                                                                      #
# searxng                                                              #
#                                                                      #
########################################################################

searxng_enabled: false

searxng_identifier: "{{ mash_playbook_service_identifier_prefix }}searxng"

searxng_uid: "{{ mash_playbook_uid }}"
searxng_gid: "{{ mash_playbook_gid }}"

searxng_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}searxng"

searxng_systemd_required_systemd_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
  }}

searxng_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
searxng_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
searxng_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
searxng_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

searxng_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

########################################################################
#                                                                      #
# /searxng                                                             #
#                                                                      #
########################################################################
# /role-specific:searxng



# role-specific:semaphore
########################################################################
#                                                                      #
# semaphore                                                            #
#                                                                      #
########################################################################

semaphore_enabled: false

semaphore_identifier: "{{ mash_playbook_service_identifier_prefix }}semaphore"

semaphore_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}semaphore"

semaphore_uid: "{{ mash_playbook_uid }}"
semaphore_gid: "{{ mash_playbook_gid }}"

semaphore_database_host: "{{ postgres_identifier if postgres_enabled else '' }}"
semaphore_database_port: "{{ '5432' if postgres_enabled else '' }}"
semaphore_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.semaphore', rounds=655555) | to_uuid }}"
semaphore_database_username: "{{ semaphore_identifier }}"

semaphore_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and semaphore_database_host == postgres_identifier else [])
  }}

semaphore_container_additional_networks: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and semaphore_database_host == postgres_identifier and semaphore_container_network != postgres_container_network else [])
  }}

semaphore_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
semaphore_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
semaphore_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
semaphore_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /semaphore                                                           #
#                                                                      #
########################################################################
# /role-specific:semaphore



# role-specific:soft_serve
########################################################################
#                                                                      #
# soft-serve                                                           #
#                                                                      #
########################################################################

soft_serve_enabled: false

soft_serve_identifier: "{{ mash_playbook_service_identifier_prefix }}soft-serve"

soft_serve_uid: "{{ mash_playbook_uid }}"
soft_serve_gid: "{{ mash_playbook_gid }}"

soft_serve_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}soft-serve"

########################################################################
#                                                                      #
# /soft-serve                                                          #
#                                                                      #
########################################################################
# /role-specific:soft_serve



# role-specific:sonarr
########################################################################
#                                                                      #
# sonarr                                                               #
#                                                                      #
########################################################################

sonarr_enabled: false

sonarr_identifier: "{{ mash_playbook_service_identifier_prefix }}sonarr"

sonarr_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}sonarr"

sonarr_uid: "{{ mash_playbook_uid }}"
sonarr_gid: "{{ mash_playbook_gid }}"

sonarr_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

sonarr_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
sonarr_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
sonarr_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
sonarr_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /sonarr                                                              #
#                                                                      #
########################################################################
# /role-specific:sonarr



# role-specific:stirling_pdf
########################################################################
#                                                                      #
# stirling-pdf                                                         #
#                                                                      #
########################################################################

stirling_pdf_enabled: false

stirling_pdf_identifier: "{{ mash_playbook_service_identifier_prefix }}stirling-pdf"

stirling_pdf_uid: "{{ mash_playbook_uid }}"
stirling_pdf_gid: "{{ mash_playbook_gid }}"

stirling_pdf_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}stirling-pdf"

stirling_pdf_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

stirling_pdf_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
stirling_pdf_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
stirling_pdf_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
stirling_pdf_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /stirling-pdf                                                        #
#                                                                      #
########################################################################
# /role-specific:stirling_pdf



# role-specific:syncthing
########################################################################
#                                                                      #
# syncthing                                                            #
#                                                                      #
########################################################################

syncthing_enabled: false

syncthing_identifier: "{{ mash_playbook_service_identifier_prefix }}syncthing"

syncthing_uid: "{{ mash_playbook_uid }}"
syncthing_gid: "{{ mash_playbook_gid }}"

syncthing_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}syncthing"

syncthing_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

syncthing_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
syncthing_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
syncthing_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
syncthing_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /syncthing                                                           #
#                                                                      #
########################################################################
# /role-specific:syncthing



# role-specific:tandoor
########################################################################
#                                                                      #
# tandoor                                                              #
#                                                                      #
########################################################################

tandoor_enabled: false

tandoor_identifier: "{{ mash_playbook_service_identifier_prefix }}tandoor"

tandoor_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}tandoor"

tandoor_uid: "{{ mash_playbook_uid }}"
tandoor_gid: "{{ mash_playbook_gid }}"

tandoor_systemd_required_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and tandoor_database_hostname == postgres_identifier else [])
  }}

tandoor_systemd_wanted_services_list_auto: |
  {{
    ([(exim_relay_identifier | default('mash-exim-relay')) ~ '.service'] if (exim_relay_enabled | default(false) and tandoor_email_host == exim_relay_identifier | default('mash-exim-relay')) else [])
  }}

tandoor_api_container_additional_networks_auto: |
  {{
    (
      ([postgres_container_network] if postgres_enabled and tandoor_database_hostname == postgres_identifier and tandoor_container_network != postgres_container_network else [])
      +
      ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and tandoor_email_host == exim_relay_identifier | default('mash-exim-relay') and tandoor_container_network != exim_relay_container_network) else [])
    ) | unique
  }}

tandoor_frontend_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

tandoor_frontend_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
tandoor_frontend_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
tandoor_frontend_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
tandoor_frontend_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

tandoor_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
tandoor_database_port: "{{ '5432' if postgres_enabled else '' }}"
tandoor_database_username: "tandoor"
tandoor_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.tandoor', rounds=655555) | to_uuid }}"

tandoor_environment_variable_secret: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'secret.tandoor', rounds=655555) | to_uuid }}"

# role-specific:exim_relay
tandoor_email_host: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
tandoor_email_port: "{{ 8025 if exim_relay_enabled else '' }}"
tandoor_default_from_email: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /tandoor                                                             #
#                                                                      #
########################################################################
# /role-specific:tandoor



# role-specific:telegraf
########################################################################
#                                                                      #
# telegraf                                                             #
#                                                                      #
########################################################################

telegraf_enabled: false

telegraf_identifier: "{{ mash_playbook_service_identifier_prefix }}telegraf"

telegraf_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}telegraf"

telegraf_uid: "{{ mash_playbook_uid }}"
telegraf_gid: "{{ mash_playbook_gid }}"

telegraf_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([(influxdb_identifier + '.service')] if influxdb_enabled else [])
  }}


########################################################################
#                                                                      #
# /telegraf                                                            #
#                                                                      #
########################################################################
# /role-specific:telegraf



# role-specific:tsdproxy
########################################################################
#                                                                      #
# tsdproxy                                                             #
#                                                                      #
########################################################################

tsdproxy_enabled: false

tsdproxy_identifier: "{{ mash_playbook_service_identifier_prefix }}tsdproxy"

tsdproxy_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}tsdproxy"

tsdproxy_uid: "{{ mash_playbook_uid }}"
tsdproxy_gid: "{{ mash_playbook_gid }}"

# role-specific:container_socket_proxy
tsdproxy_docker_endpoint_is_unix_socket: "{{ false if container_socket_proxy_enabled else true }}"
tsdproxy_docker_endpoint: "{{ container_socket_proxy_endpoint if container_socket_proxy_enabled == true and tsdproxy_enabled == true else 'unix:///var/run/docker.sock' }}"
# TSDProxy needs access to the Docker Networks and images to work
container_socket_proxy_api_network_enabled: "{{ true if tsdproxy_docker_endpoint == container_socket_proxy_endpoint else false }}"
container_socket_proxy_api_images_enabled: "{{ true if tsdproxy_docker_endpoint == container_socket_proxy_endpoint else false }}"
# /role-specific:container_socket_proxy

tsdproxy_container_additional_networks_auto: |
  {{
    ([container_socket_proxy_container_network] if container_socket_proxy_enabled | default(false) else [])
  }}

tsdproxy_systemd_required_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([container_socket_proxy_identifier + '.service'] if container_socket_proxy_enabled | default(false) else [])
  }}

########################################################################
#                                                                      #
# /tsdproxy                                                            #
#                                                                      #
########################################################################
# /role-specific:tsdproxy



# role-specific:valkey
########################################################################
#                                                                      #
# valkey                                                               #
#                                                                      #
########################################################################

valkey_enabled: false

valkey_identifier: "{{ mash_playbook_service_identifier_prefix }}valkey"

valkey_uid: "{{ mash_playbook_uid }}"
valkey_gid: "{{ mash_playbook_gid }}"

valkey_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}valkey"

valkey_arch: "{{ mash_playbook_architecture }}"

########################################################################
#                                                                      #
# /valkey                                                              #
#                                                                      #
########################################################################
# /role-specific:valkey



# role-specific:vaultwarden
########################################################################
#                                                                      #
# vaultwarden                                                          #
#                                                                      #
########################################################################

vaultwarden_enabled: false

vaultwarden_identifier: "{{ mash_playbook_service_identifier_prefix }}vaultwarden"

vaultwarden_uid: "{{ mash_playbook_uid }}"
vaultwarden_gid: "{{ mash_playbook_gid }}"

vaultwarden_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}vaultwarden"

vaultwarden_systemd_required_systemd_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and vaultwarden_database_hostname == postgres_identifier else [])
  }}

vaultwarden_systemd_wanted_systemd_services_list_auto: |
  {{
    ([(exim_relay_identifier | default('mash-exim-relay')) ~ '.service'] if (exim_relay_enabled | default(false) and vaultwarden_config_smtp_host == exim_relay_identifier | default('mash-exim-relay')) else [])
  }}

vaultwarden_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and vaultwarden_database_hostname == postgres_identifier and vaultwarden_container_network != postgres_container_network else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and vaultwarden_config_smtp_host == exim_relay_identifier | default('mash-exim-relay') and vaultwarden_container_network != exim_relay_container_network) else [])
  }}

vaultwarden_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
vaultwarden_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
vaultwarden_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
vaultwarden_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

vaultwarden_container_labels_traefik_compression_middleware_enabled: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_enabled }}"
vaultwarden_container_labels_traefik_compression_middleware_name: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_name if mash_playbook_reverse_proxy_traefik_middleware_compession_enabled else '' }}"

vaultwarden_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
vaultwarden_database_port: "{{ '5432' if postgres_enabled else '' }}"
vaultwarden_database_username: "vaultwarden"
vaultwarden_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.vaultwarden', rounds=655555) | to_uuid }}"

# role-specific:exim_relay
vaultwarden_config_smtp_from: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
vaultwarden_config_smtp_host: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
vaultwarden_config_smtp_port: "{{ 8025 if exim_relay_enabled else '587' }}"
vaultwarden_config_smtp_security: "{{ 'off' if exim_relay_enabled else 'starttls' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /vaultwarden                                                         #
#                                                                      #
########################################################################
# /role-specific:vaultwarden



# role-specific:uptime_kuma
########################################################################
#                                                                      #
# uptime_kuma                                                          #
#                                                                      #
########################################################################

uptime_kuma_enabled: false

uptime_kuma_identifier: "{{ mash_playbook_service_identifier_prefix }}uptime-kuma"

uptime_kuma_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}uptime-kuma"

uptime_kuma_uid: "{{ mash_playbook_uid }}"
uptime_kuma_gid: "{{ mash_playbook_gid }}"

uptime_kuma_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

uptime_kuma_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
uptime_kuma_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
uptime_kuma_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
uptime_kuma_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /uptime_kuma                                                         #
#                                                                      #
########################################################################
# /role-specific:uptime_kuma



# role-specific:versatiles
########################################################################
#                                                                      #
# versatiles                                                          #
#                                                                      #
########################################################################

versatiles_enabled: false

versatiles_identifier: "{{ mash_playbook_service_identifier_prefix }}versatiles"

versatiles_uid: "{{ mash_playbook_uid }}"
versatiles_gid: "{{ mash_playbook_gid }}"

versatiles_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}versatiles"

versatiles_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

versatiles_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
versatiles_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
versatiles_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
versatiles_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /versatiles                                                         #
#                                                                      #
########################################################################
# /role-specific:versatiles



# role-specific:wg_easy
########################################################################
#                                                                      #
# wg-easy                                                              #
#                                                                      #
########################################################################

wg_easy_enabled: false

wg_easy_identifier: "{{ mash_playbook_service_identifier_prefix }}wg-easy"

wg_easy_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}wg-easy"

wg_easy_uid: "{{ mash_playbook_uid }}"
wg_easy_gid: "{{ mash_playbook_gid }}"

wg_easy_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

wg_easy_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
wg_easy_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
wg_easy_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
wg_easy_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

########################################################################
#                                                                      #
# /wg-easy                                                             #
#                                                                      #
########################################################################
# /role-specific:wg_easy



# role-specific:forgejo
########################################################################
#                                                                      #
# forgejo                                                              #
#                                                                      #
########################################################################

forgejo_enabled: false

forgejo_identifier: "{{ mash_playbook_service_identifier_prefix }}forgejo"

forgejo_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}forgejo"

forgejo_uid: "{{ mash_playbook_uid }}"
forgejo_gid: "{{ mash_playbook_gid }}"

forgejo_systemd_required_systemd_services_list_auto: |
  {{
    ([postgres_identifier ~ '.service'] if postgres_enabled and forgejo_config_database_hostname == postgres_identifier else [])
  }}

forgejo_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([postgres_container_network] if postgres_enabled and forgejo_config_database_hostname == postgres_identifier and forgejo_container_network != postgres_container_network else [])
    +
    ([exim_relay_container_network | default('mash-exim-relay')] if (exim_relay_enabled | default(false) and forgejo_config_mailer_smtp_addr == exim_relay_identifier | default('mash-exim-relay') and forgejo_container_network != exim_relay_container_network) else [])
  }}

forgejo_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
forgejo_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
forgejo_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
forgejo_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"

forgejo_container_labels_traefik_compression_middleware_enabled: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_enabled }}"
forgejo_container_labels_traefik_compression_middleware_name: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_name if mash_playbook_reverse_proxy_traefik_middleware_compession_enabled else '' }}"

forgejo_config_database_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
forgejo_config_database_port: "{{ '5432' if postgres_enabled else '' }}"
forgejo_config_database_username: "forgejo"
forgejo_config_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.forgejo', rounds=655555) | to_uuid }}"

# role-specific:exim_relay
forgejo_config_mailer_enabled: "{{ 'true' if exim_relay_enabled else '' }}"
forgejo_config_mailer_smtp_addr: "{{ exim_relay_identifier if exim_relay_enabled else '' }}"
forgejo_config_mailer_smtp_port: 8025
forgejo_config_mailer_from: "{{ exim_relay_sender_address if exim_relay_enabled else '' }}"
forgejo_config_mailer_protocol: "{{ 'smtp' if exim_relay_enabled else '' }}"
# /role-specific:exim_relay

########################################################################
#                                                                      #
# /forgejo                                                             #
#                                                                      #
########################################################################
# /role-specific:forgejo



# role-specific:forgejo_runner
########################################################################
#                                                                      #
# forgejo_runner                                                       #
#                                                                      #
########################################################################

forgejo_runner_enabled: false

forgejo_runner_uid: "0"
forgejo_runner_gid: "0"

forgejo_runner_identifier: "{{ mash_playbook_service_identifier_prefix }}forgejo-runner"

forgejo_runner_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}forgejo-runner"

forgejo_runner_systemd_required_systemd_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
  }}

########################################################################
#                                                                      #
# /forgejo_runner                                                      #
#                                                                      #
########################################################################
# /role-specific:forgejo_runner



# role-specific:woodpecker_ci_server
########################################################################
#                                                                      #
# woodpecker-ci-server                                                 #
#                                                                      #
########################################################################

woodpecker_ci_server_enabled: false

woodpecker_ci_server_identifier: "{{ mash_playbook_service_identifier_prefix }}woodpecker-ci-server"

woodpecker_ci_server_uid: "{{ mash_playbook_uid }}"
woodpecker_ci_server_gid: "{{ mash_playbook_gid }}"

woodpecker_ci_server_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}woodpecker-ci/server"

woodpecker_ci_server_systemd_required_systemd_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([postgres_identifier ~ '.service'] if postgres_enabled and woodpecker_ci_server_database_datasource_hostname == postgres_identifier else [])
  }}

woodpecker_ci_server_container_additional_networks: |
  {{
    (
      ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
      +
      ([postgres_container_network] if postgres_enabled and woodpecker_ci_server_database_datasource_hostname == postgres_identifier and woodpecker_ci_server_container_network != postgres_container_network else [])
    ) | unique
  }}

woodpecker_ci_server_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
woodpecker_ci_server_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

woodpecker_ci_server_container_labels_traefik_compression_middleware_enabled: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_enabled }}"
woodpecker_ci_server_container_labels_traefik_compression_middleware_name: "{{ mash_playbook_reverse_proxy_traefik_middleware_compession_name if mash_playbook_reverse_proxy_traefik_middleware_compession_enabled else '' }}"

woodpecker_ci_server_database_driver: postgres
woodpecker_ci_server_database_datasource: "postgres://{{ woodpecker_ci_server_database_datasource_username }}:{{ woodpecker_ci_server_database_datasource_password }}@{{ woodpecker_ci_server_database_datasource_hostname }}:{{ woodpecker_ci_server_database_datasource_port }}/{{ woodpecker_ci_server_database_datasource_db_name }}?sslmode=disable"

woodpecker_ci_server_database_datasource_hostname: "{{ postgres_identifier if postgres_enabled else '' }}"
woodpecker_ci_server_database_datasource_port: "{{ '5432' if postgres_enabled else '' }}"
woodpecker_ci_server_database_datasource_username: woodpecker_ci_server
woodpecker_ci_server_database_datasource_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'woodpecker.ci', rounds=655555) | to_uuid }}"
woodpecker_ci_server_database_datasource_db_name: woodpecker_ci_server

########################################################################
#                                                                      #
# /woodpecker-ci-server                                                #
#                                                                      #
########################################################################
# /role-specific:woodpecker_ci_server



# role-specific:woodpecker_ci_agent
########################################################################
#                                                                      #
# woodpecker-ci-agent                                                  #
#                                                                      #
########################################################################

woodpecker_ci_agent_enabled: false

woodpecker_ci_agent_identifier: "{{ mash_playbook_service_identifier_prefix }}woodpecker-ci-agent"

woodpecker_ci_agent_uid: "{{ mash_playbook_uid }}"
woodpecker_ci_agent_gid: "{{ mash_playbook_gid }}"

woodpecker_ci_agent_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}woodpecker-ci/agent"

woodpecker_ci_agent_systemd_required_systemd_services_list: |
  {{
    ([devture_systemd_docker_base_docker_service_name] if devture_systemd_docker_base_docker_service_name else [])
    +
    ([woodpecker_ci_server_identifier ~ '.service'] if woodpecker_ci_server_enabled else [])
  }}

woodpecker_ci_agent_container_additional_networks: |
  {{
    (
      ([woodpecker_ci_server_container_network] if woodpecker_ci_server_enabled and woodpecker_ci_server_container_network != woodpecker_ci_agent_container_network else [])
    ) | unique
  }}

woodpecker_ci_agent_config_server: "{{ (woodpecker_ci_server_identifier + ':' + woodpecker_ci_server_config_grpc_addr_port | string) if woodpecker_ci_agent_enabled else '' }}"

woodpecker_ci_agent_config_agent_secret: "{{ woodpecker_ci_server_config_agent_secret if woodpecker_ci_agent_enabled else '' }}"

########################################################################
#                                                                      #
# /woodpecker-ci-agent                                                 #
#                                                                      #
########################################################################
# /role-specific:woodpecker_ci_agent



# role-specific:wordpress
########################################################################
#                                                                      #
# wordpress                                                            #
#                                                                      #
########################################################################

wordpress_enabled: false

wordpress_identifier: "{{ mash_playbook_service_identifier_prefix }}wordpress"

wordpress_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}wordpress"

wordpress_uid: "{{ mash_playbook_uid }}"
wordpress_gid: "{{ mash_playbook_gid }}"

wordpress_systemd_required_systemd_services_list_auto: |
  {{
    ([(mariadb_identifier | default('mash-mariadb')) ~ '.service'] if mariadb_enabled | default(false) and wordpress_database_hostname == mariadb_identifier | default('mash-mariadb') else [])
  }}


wordpress_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
    +
    ([mariadb_identifier | default('mash-mariadb')] if mariadb_enabled | default(false) and wordpress_database_hostname == mariadb_identifier | default('mash-mariadb') and wordpress_container_network != mariadb_identifier | default('mash-mariadb') else [])
  }}

wordpress_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
wordpress_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"
wordpress_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
wordpress_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"


wordpress_database_hostname: "{{ mariadb_identifier if mariadb_enabled | default(false) else '' }}"
wordpress_mysql_port: "{{ '3306' if mariadb_enabled | default(false) else '' }}"
wordpress_database_password: "{{ '%s' | format(mash_playbook_generic_secret_key) | password_hash('sha512', 'db.mariadb', rounds=655555) | to_uuid }}"

########################################################################
#                                                                      #
# /wordpress                                                           #
#                                                                      #
########################################################################
# /role-specific:wordpress



# role-specific:writefreely
########################################################################
#                                                                      #
# writefreely                                                          #
#                                                                      #
########################################################################

writefreely_enabled: false

writefreely_identifier: "{{ mash_playbook_service_identifier_prefix }}writefreely"

writefreely_uid: "{{ mash_playbook_uid }}"
writefreely_gid: "{{ mash_playbook_gid }}"

writefreely_base_path: "{{ mash_playbook_base_path }}/{{ mash_playbook_service_base_directory_name_prefix }}writefreely"

writefreely_container_additional_networks_auto: |
  {{
    ([mash_playbook_reverse_proxyable_services_additional_network] if mash_playbook_reverse_proxyable_services_additional_network else [])
  }}

writefreely_container_labels_traefik_enabled: "{{ mash_playbook_traefik_labels_enabled }}"
writefreely_container_labels_traefik_docker_network: "{{ mash_playbook_reverse_proxyable_services_additional_network }}"

# role-specific:traefik
writefreely_container_labels_traefik_entrypoints: "{{ traefik_entrypoint_primary }}"
writefreely_container_labels_traefik_tls_certResolver: "{{ traefik_certResolver_primary }}"
# /role-specific:traefik

########################################################################
#                                                                      #
# /writefreely                                                         #
#                                                                      #
########################################################################
# /role-specific:writefreely
````

## File: templates/requirements.yml
````yaml
---

- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-adguard-home.git
  version: v0.107.57-2
  name: adguard_home
  activation_prefix: adguard_home_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-apisix-dashboard.git
  version: v3.0.1-0
  name: apisix_dashboard
  activation_prefix: apisix_dashboard_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-apisix-gateway.git
  version: v3.8.0-2
  name: apisix_gateway
  activation_prefix: apisix_gateway_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-appsmith.git
  version: v1.9.50-0
  name: appsmith
  activation_prefix: appsmith_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-authelia.git
  version: v4.37.5-1
  name: authelia
  activation_prefix: authelia_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-authentik.git
  version: v2025.2.1-1
  name: authentik
  activation_prefix: authentik_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-aux.git
  version: v1.0.0-5
  name: auxiliary
  activation_prefix: aux_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-backup_borg.git
  version: v1.4.0-1.9.13-0
  name: backup_borg
  activation_prefix: backup_borg_
- src: git+https://github.com/lingawakad/ansible-role-calibre-web.git
  version: v0.6.24-0
  name: calibre-web
  activation_prefix: calibre-web_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-changedetection.git
  version: v0.48.04-0
  name: changedetection
  activation_prefix: changedetection_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-cleanup.git
  version: main
  name: cleanup
  activation_prefix: system_cleanup_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-clickhouse.git
  version: v24.8.4.13-0
  name: clickhouse
  activation_prefix: clickhouse_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-collabora-online.git
  version: v24.04.3.1.1-0
  name: collabora_online
  activation_prefix: collabora_online_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-container-socket-proxy.git
  version: v0.3.0-4
  name: container_socket_proxy
  activation_prefix: mash_
- src: git+https://github.com/Bergruebe/ansible-role-couchdb.git
  version: v3.4.3-0
  name: couchdb
  activation_prefix: couchdb_
- src: git+https://github.com/geerlingguy/ansible-role-docker
  version: 7.4.5
  name: docker
  activation_prefix: mash_playbook_docker_installation_enabled
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-docker-registry.git
  version: v2.8.3-4
  name: docker_registry
  activation_prefix: docker_registry_enabled
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-docker-registry-browser.git
  version: v1.7.2-0
  name: docker_registry_browser
  activation_prefix: docker_registry_browser_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-docker-registry-proxy.git
  version: v1.2.4-0
  name: docker_registry_proxy
  activation_prefix: docker_registry_proxy_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-docker-registry-purger.git
  version: v1.0.0-0
  name: docker_registry_purger
  activation_prefix: docker_registry_purger_
- src: git+https://github.com/devture/com.devture.ansible.role.docker_sdk_for_python.git
  version: 129c8590e106b83e6f4c259649a613c6279e937a
  name: docker_sdk_for_python
  activation_prefix: devture_docker_sdk_for_python_installation_enabled
- src: git+https://github.com/shukon/ansible-role-dokuwiki.git
  version: v2024-02-06b-2
  name: dokuwiki
  activation_prefix: dokuwiki_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-echoip.git
  version: v0.0.0-0
  name: echoip
  activation_prefix: echoip_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-endlessh.git
  version: v2024.0119.1-0
  name: endlessh
  activation_prefix: endlessh_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-etcd.git
  version: v3.5.11-0
  name: etcd
  activation_prefix: etcd_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-etherpad.git
  version: v2.2.7-4
  name: etherpad
  activation_prefix: etherpad_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-exim-relay.git
  version: v4.98.1-r0-2-0
  name: exim_relay
  activation_prefix: exim_relay_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-fail2ban.git
  version: e25d0d9b5282dbb5a1ec4cd23d26c16998e4334f
  name: fail2ban
  activation_prefix: system_security_fail2ban_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-firezone.git
  version: v0.7.36-3
  name: firezone
  activation_prefix: firezone_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-focalboard.git
  version: v7.10.4-0
  name: focalboard
  activation_prefix: focalboard_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-forgejo.git
  version: v10.0.1-2
  name: forgejo
  activation_prefix: forgejo_
- src: git+https://git.sergiodj.net/sergiodj/ansible-role-forgejo-runner.git
  version: v6.2.0-0
  name: forgejo_runner
  activation_prefix: forgejo_runner_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-freescout.git
  version: v1.17.110-0
  name: freescout
  activation_prefix: freescout_
- src: git+https://github.com/kinduff/ansible-docker-freshrss.git
  version: v2.5.0
  name: freshrss
  activation_prefix: freshrss_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-funkwhale.git
  version: v1.4.0-9
  name: funkwhale
  activation_prefix: funkwhale_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-gitea.git
  version: v1.23.5-0
  name: gitea
  activation_prefix: gitea_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-gotosocial.git
  version: v0.17.4-2
  name: gotosocial
  activation_prefix: gotosocial_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-grafana.git
  version: v11.5.2-2
  name: grafana
  activation_prefix: grafana_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-headscale.git
  version: v0.25.1-3
  name: headscale
  activation_prefix: headscale_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-healthchecks.git
  version: v3.9-1
  name: healthchecks
  activation_prefix: healthchecks_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-hubsite.git
  version: v1.27.4-1
  name: hubsite
  activation_prefix: hubsite_
- src: git+https://github.com/moan0s/ansible-role-ilmo.git
  version: v1.0.4-0
  name: ilmo
  activation_prefix: ilmo_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-infisical.git
  version: v0.43.19-0
  name: infisical
  activation_prefix: infisical_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-influxdb.git
  version: v2.7.6-0
  name: influxdb
  activation_prefix: influxdb_
- src: git+https://github.com/spatterIight/ansible-role-jackett.git
  version: v0.22.1377-0
  name: jackett
  activation_prefix: jackett_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-jitsi.git
  version: v10078-1-0
  name: jitsi
  activation_prefix: jitsi_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-keycloak.git
  version: v26.1.3-0
  name: keycloak
  activation_prefix: keycloak_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-keydb.git
  version: v6.3.4-3
  name: keydb
  activation_prefix: keydb_
- src: git+https://gitlab.com/horvathg.1988/ansible-role-labelstudio.git
  version: v1.0.0-3
  name: labelstudio
  activation_prefix: labelstudio_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-lago.git
  version: v0.50.0-0
  name: lago
  activation_prefix: lago_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-languagetool.git
  version: v6.5-3
  name: languagetool
  activation_prefix: languagetool_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-linkding.git
  version: v1.39.1-0
  name: linkding
  activation_prefix: linkding_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-loki.git
  version: v2.9.4-3
  name: loki
  activation_prefix: loki_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-mariadb.git
  version: v11.4.4-2
  name: mariadb
  activation_prefix: mariadb_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-matterbridge.git
  version: v1.26.0-0
  name: matterbridge
  activation_prefix: matterbridge_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-miniflux.git
  version: v2.2.4-3
  name: miniflux
  activation_prefix: miniflux_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-mobilizon.git
  version: v4.1.0-0
  name: mobilizon
  activation_prefix: mobilizon_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-mongodb.git
  version: v7.0.4-0
  name: mongodb
  activation_prefix: mongodb_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-mosquitto.git
  version: v2.0.15-1
  name: mosquitto
  activation_prefix: mosquitto_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-mrs.git
  version: v0.1.0-1
  name: mrs
  activation_prefix: mrs_
- src: git+https://github.com/kinduff/ansible-docker-n8n.git
  version: v1.4.2
  name: n8n
  activation_prefix: n8n_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-navidrome.git
  version: v0.55.0-0
  name: navidrome
  activation_prefix: navidrome_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-neko.git
  version: v2.8.12-0
  name: neko
  activation_prefix: neko_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-netbox.git
  version: v3.7.0-2.8.0-0
  name: netbox
  activation_prefix: netbox_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-nextcloud.git
  version: v30.0.6-1
  name: nextcloud
  activation_prefix: nextcloud_
- src: git+https://codeberg.org/moanos/ansible-role-notfellchen.git
  version: v0.4.0-0
  name: notfellchen
  activation_prefix: notfellchen_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-ntfy.git
  version: v2.11.0-4
  name: ntfy
  activation_prefix: ntfy_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-oauth2-proxy.git
  version: v7.6.0-1
  name: oauth2_proxy
  activation_prefix: oauth2_proxy_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-outline.git
  version: v0.82.0-0
  name: outline
  activation_prefix: outline_
- src: git+https://github.com/spatterIight/ansible-role-overseerr.git
  version: v1.33.2-0
  name: overseerr
  activation_prefix: overseerr_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-owncast.git
  version: v0.2.1-2
  name: owncast
  activation_prefix: owncast_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-oxitraffic.git
  version: v0.10.1-0
  name: oxitraffic
  activation_prefix: oxitraffic_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-paperless.git
  version: v2.13.2-0
  name: paperless
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-peertube.git
  version: v7.0.1-2
  name: peertube
  activation_prefix: peertube_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-plausible.git
  version: v2.1.5-0
  name: plausible
  activation_prefix: plausible_
- src: git+https://github.com/devture/com.devture.ansible.role.playbook_help.git
  version: 201c939eed363de269a83ba29784fc3244846048
  name: playbook_help
  activation_prefix: ""
- src: git+https://github.com/devture/com.devture.ansible.role.playbook_runtime_messages.git
  version: 9b4b088c62b528b73a9a7c93d3109b091dd42ec6
  name: playbook_runtime_messages
  activation_prefix: ""
- src: git+https://github.com/devture/com.devture.ansible.role.playbook_state_preserver.git
  version: ff2fd42e1c1a9e28e3312bbd725395f9c2fc7f16
  name: playbook_state_preserver
  activation_prefix: devture_playbook_state_preserver_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-postgis.git
  version: v15-3.3-0
  name: postgis
  activation_prefix: postgis_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-postgres.git
  version: v17.4-0
  name: postgres
  activation_prefix: postgres_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-postgres-backup.git
  version: v17-3
  name: postgres_backup
  activation_prefix: postgres_backup_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-prometheus.git
  version: v2.55.1-3
  name: prometheus
  activation_prefix: prometheus_enabled
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-prometheus-blackbox-exporter.git
  version: v0.25.0-1
  name: prometheus_blackbox_exporter
  activation_prefix: prometheus_blackbox_exporter_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-prometheus-node-exporter.git
  version: v1.8.2-5
  name: prometheus_node_exporter
  activation_prefix: prometheus_node_exporter_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-prometheus-postgres-exporter.git
  version: v0.14.0-9
  name: prometheus_postgres_exporter
  activation_prefix: prometheus_postgres_exporter_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-prometheus-ssh-exporter.git
  version: v1.5.0-2
  name: prometheus_ssh_exporter
  activation_prefix: prometheus_ssh_exporter_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-promtail.git
  version: v2.9.5-0
  name: promtail
  activation_prefix: promtail_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-qbittorrent.git
  version: v5.0.4-0
  name: qbittorrent
  activation_prefix: qbittorrent_
- src: git+https://github.com/spatterIight/ansible-role-radarr.git
  version: v5.18.4-0
  name: radarr
  activation_prefix: radarr_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-radicale.git
  version: v3.4.1.1-0
  name: radicale
  activation_prefix: radicale_
- src: git+https://github.com/lingawakad/ansible-role-readeck.git
  version: v0.17.1-0
  name: readeck
  activation_prefix: readeck_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-redis.git
  version: v7.4.2-0
  name: redis
  activation_prefix: redis_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-redmine.git
  version: v6.0.4-0
  name: redmine
  activation_prefix: redmine_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-roundcube.git
  version: v1.6.10-1
  name: roundcube
  activation_prefix: roundcube_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-rumqttd.git
  version: v0.21.0-0
  name: rumqttd
  activation_prefix: rumqttd_
- src: git+https://git.sergiodj.net/sergiodj/ansible-role-searxng.git
  version: v1.0-0
  name: searxng
  activation_prefix: searxng_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-semaphore.git
  version: v2.9.56-0
  name: semaphore
  activation_prefix: semaphore_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-soft_serve.git
  version: v0.4.7-0
  name: soft_serve
  activation_prefix: soft_serve_
- src: git+https://github.com/spatterIight/ansible-role-sonarr.git
  version: v4.0.12-0
  name: sonarr
  activation_prefix: sonarr_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-ssh.git
  version: 13440992374e0fef8f6360a0856b78b37a1fd831
  name: ssh
  activation_prefix: system_security_ssh_
- src: git+https://github.com/Bergruebe/ansible-role-stirling-pdf.git
  version: v0.42.0-0
  name: stirling_pdf
  activation_prefix: stirling_pdf_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-swap.git
  version: abfb18b6862108bbf24347500446203170324d7f
  name: swap
  activation_prefix: system_swap_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-syncthing.git
  version: v1.29.3-0
  name: syncthing
  activation_prefix: syncthing_
- src: git+https://github.com/devture/com.devture.ansible.role.systemd_docker_base.git
  version: v1.4.0-0
  name: systemd_docker_base
  activation_prefix: ""
- src: git+https://github.com/devture/com.devture.ansible.role.systemd_service_manager.git
  version: v1.0.0-4
  name: systemd_service_manager
  activation_prefix: ""
- src: git+https://github.com/IUCCA/ansible-role-tandoor.git
  version: v1.5.31-0
  name: tandoor
  activation_prefix: tandoor_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-telegraf.git
  version: v1.30.2-0
  name: telegraf
  activation_prefix: telegraf_
- src: git+https://github.com/devture/com.devture.ansible.role.timesync.git
  version: v1.0.0-0
  name: timesync
  activation_prefix: devture_timesync_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-traefik.git
  version: v3.3.4-0
  name: traefik
  activation_prefix: mash_
- src: git+https://github.com/Bergruebe/ansible-role-tsdproxy.git
  version: v1.4.4-1
  name: tsdproxy
  activation_prefix: tsdproxy_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-uptime_kuma.git
  version: v1.23.16-2
  name: uptime_kuma
  activation_prefix: uptime_kuma_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-valkey.git
  version: v8.0.1-3
  name: valkey
  activation_prefix: valkey_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-vaultwarden.git
  version: v1.33.2-3
  name: vaultwarden
  activation_prefix: vaultwarden_
- src: git+https://github.com/moan0s/ansible-role-versatiles.git
  version: v0.12.6-2
  name: versatiles
  activation_prefix: versatiles_
- src: git+https://github.com/spatterIight/ansible-role-wetty.git
  version: v2.5-0
  name: wetty
  activation_prefix: wetty_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-wg-easy.git
  version: v15.0.0-beta.7-1
  name: wg_easy
  activation_prefix: wg_easy_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-woodpecker-ci-agent.git
  version: v3.1.0-0
  name: woodpecker_ci_agent
  activation_prefix: woodpecker_ci_agent_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-woodpecker-ci-server.git
  version: v3.1.0-0
  name: woodpecker_ci_server
  activation_prefix: woodpecker_ci_server_
- src: git+https://github.com/mother-of-all-self-hosting/ansible-role-wordpress
  version: v6.6.2-0
  name: wordpress
  activation_prefix: wordpress_
- src: git+https://github.com/ZenKyma/ansible-role-writefreely.git
  version: v0.15.0-1
  name: writefreely
  activation_prefix: writefreely_
````

## File: templates/setup.yml
````yaml
- name: "Set up a self-hosted server"
  hosts: "{{ target if target is defined else 'mash_servers' }}"
  become: true

  roles:
    # role-specific:playbook_help
    - role: galaxy/playbook_help
      tags:
        - setup-all
        - install-all
    # /role-specific:playbook_help

    # No role-specific checks here. Local roles are always installed.
    - role: mash/playbook_base
    - role: mash/playbook_migration

    # role-specific:systemd_docker_base
    # This role has no tasks at all
    - role: galaxy/systemd_docker_base
    # /role-specific:systemd_docker_base

    # role-specific:docker
    - when: mash_playbook_docker_installation_enabled | bool
      role: galaxy/docker
      vars:
        docker_install_compose: false
        docker_install_compose_plugin: false
      tags:
        - setup-docker
        - setup-all
        - install-docker
        - install-all
    # /role-specific:docker

    # role-specific:docker_sdk_for_python
    - when: devture_docker_sdk_for_python_installation_enabled | bool
      role: galaxy/docker_sdk_for_python
      tags:
        - setup-docker
        - setup-all
        - install-docker
        - install-all
    # /role-specific:docker_sdk_for_python

    # role-specific:timesync
    - when: devture_timesync_installation_enabled | bool
      role: galaxy/timesync
      tags:
        - setup-timesync
        - setup-all
        - install-timesync
        - install-all
    # /role-specific:timesync

    # role-specific:swap
    - role: galaxy/swap
    # /role-specific:swap

    # role-specific:cleanup
    - role: galaxy/cleanup
    # /role-specific:cleanup

    # role-specific:ssh
    - when: system_security_ssh_enabled | bool
      role: galaxy/ssh
    # /role-specific:ssh

    # role-specific:fail2ban
    - when: system_security_fail2ban_enabled | bool
      role: galaxy/fail2ban
    # /role-specific:fail2ban

    # role-specific:postgres
    # This role exposes various tags (setup-postgres, setup-all, upgrade-postgres, import-postgres, etc.), so we don't tag it here.
    - role: galaxy/postgres
    # /role-specific:postgres

    # role-specific:postgres_backup
    - role: galaxy/postgres_backup
    # /role-specific:postgres_backup

    # role-specific:mongodb
    - role: galaxy/mongodb
    # /role-specific:mongodb

    # role-specific:container_socket_proxy
    - role: galaxy/container_socket_proxy
    # /role-specific:container_socket_proxy

    # role-specific:traefik
    - role: galaxy/traefik
    # /role-specific:traefik

    # role-specific:adguard_home
    - role: galaxy/adguard_home
    # /role-specific:adguard_home

    # role-specific:appsmith
    - role: galaxy/appsmith
    # /role-specific:appsmith

    # role-specific:apisix_dashboard
    - role: galaxy/apisix_dashboard
    # /role-specific:apisix_dashboard

    # role-specific:apisix_gateway
    - role: galaxy/apisix_gateway
    # /role-specific:apisix_gateway

    # role-specific:authelia
    - role: galaxy/authelia
    # /role-specific:authelia

    # role-specific:authentik
    - role: galaxy/authentik
    # /role-specific:authentik

    # role-specific:backup_borg
    - role: galaxy/backup_borg
    # /role-specific:backup_borg

    # role-specific:changedetection
    - role: galaxy/changedetection
    # /role-specific:changedetection

    # role-specific:wetty
    - role: galaxy/wetty
    # /role-specific:wetty

    # role-specific:calibre-web
    - role: galaxy/calibre-web
    # /role-specific:calibre-web

    # role-specific:clickhouse
    - role: galaxy/clickhouse
    # /role-specific:clickhouse

    # role-specific:collabora_online
    - role: galaxy/collabora_online
    # /role-specific:collabora_online

    # role-specific:couchdb
    - role: galaxy/couchdb
    # /role-specific:couchdb

    # role-specific:docker_registry
    - role: galaxy/docker_registry
    # /role-specific:docker_registry

    # role-specific:docker_registry_proxy
    - role: galaxy/docker_registry_proxy
    # /role-specific:docker_registry_proxy

    # role-specific:docker_registry_browser
    - role: galaxy/docker_registry_browser
    # /role-specific:docker_registry_browser

    # role-specific:docker_registry_purger
    - role: galaxy/docker_registry_purger
    # /role-specific:docker_registry_purger

    # role-specific:dokuwiki
    - role: galaxy/dokuwiki
    # /role-specific:dokuwiki

    # role-specific:echoip
    - role: galaxy/echoip
    # /role-specific:echoip

    # role-specific:endlessh
    - role: galaxy/endlessh
    # /role-specific:endlessh

    # role-specific:etcd
    - role: galaxy/etcd
    # /role-specific:etcd

    # role-specific:etherpad
    - role: galaxy/etherpad
    # /role-specific:etherpad

    # role-specific:exim_relay
    - role: galaxy/exim_relay
    # /role-specific:exim_relay

    # role-specific:firezone
    - role: galaxy/firezone
    # /role-specific:firezone

    # role-specific:focalboard
    - role: galaxy/focalboard
    # /role-specific:focalboard

    # role-specific:freshrss
    - role: galaxy/freshrss
    # /role-specific:freshrss

    # role-specific:funkwhale
    - role: galaxy/funkwhale
    # /role-specific:funkwhale

    # role-specific:gitea
    - role: galaxy/gitea
    # /role-specific:gitea

    # role-specific:gotosocial
    - role: galaxy/gotosocial
    # /role-specific:gotosocial

    # role-specific:grafana
    - role: galaxy/grafana
    # /role-specific:grafana

    # role-specific:mariadb
    - role: galaxy/mariadb
    # /role-specific:mariadb

    # role-specific:freescout
    - role: galaxy/freescout
    # /role-specific:freescout

    # role-specific:miniflux
    - role: galaxy/miniflux
    # /role-specific:miniflux

    # role-specific:mrs
    - role: galaxy/mrs
    # /role-specific:mrs

    # role-specific:n8n
    - role: galaxy/n8n
    # /role-specific:n8n

    # role-specific:healthchecks
    - role: galaxy/healthchecks
    # /role-specific:healthchecks

    # role-specific:infisical
    - role: galaxy/infisical
    # /role-specific:infisical

    # role-specific:headscale
    - role: galaxy/headscale
    # /role-specific:headscale

    # role-specific:hubsite
    - role: galaxy/hubsite
    # /role-specific:hubsite

    # role-specific:ilmo
    - role: galaxy/ilmo
    # /role-specific:ilmo

    # role-specific:influxdb
    - role: galaxy/influxdb
    # /role-specific:influxdb

    # role-specific:jackett
    - role: galaxy/jackett
    # /role-specific:jackett

    # role-specific:jitsi
    - role: galaxy/jitsi
    # /role-specific:jitsi

    # role-specific:keycloak
    - role: galaxy/keycloak
    # /role-specific:keycloak

    # role-specific:keydb
    - role: galaxy/keydb
    # /role-specific:keydb

    # role-specific:labelstudio
    - role: galaxy/labelstudio
    # /role-specific:labelstudio

    # role-specific:lago
    - role: galaxy/lago
    # /role-specific:lago

    # role-specific:languagetool
    - role: galaxy/languagetool
    # /role-specific:languagetool

    # role-specific:linkding
    - role: galaxy/linkding
    # /role-specific:linkding

    # role-specific:loki
    - role: galaxy/loki
    # /role-specific:loki

    # role-specific:mobilizon
    - role: galaxy/mobilizon
    # /role-specific:mobilizon

    # role-specific:mosquitto
    - role: galaxy/mosquitto
    # /role-specific:mosquitto

    # role-specific:navidrome
    - role: galaxy/navidrome
    # /role-specific:navidrome

    # role-specific:neko
    - role: galaxy/neko
    # /role-specific:neko

    # role-specific:netbox
    - role: galaxy/netbox
    # /role-specific:netbox

    # role-specific:nextcloud
    - role: galaxy/nextcloud
    # /role-specific:nextcloud

    # role-specific:notfellchen
    - role: galaxy/notfellchen
    # /role-specific:notfellchen

    # role-specific:ntfy
    - role: galaxy/ntfy
    # /role-specific:ntfy

    # role-specific:oauth2_proxy
    - role: galaxy/oauth2_proxy
    # /role-specific:oauth2_proxy

    # role-specific:overseerr
    - role: galaxy/overseerr
    # /role-specific:overseerr

    # role-specific:owncast
    - role: galaxy/owncast
    # /role-specific:owncast

    # role-specific:outline
    - role: galaxy/outline
    # /role-specific:outline

    # role-specific:oxitraffic
    - role: galaxy/oxitraffic
    # /role-specific:oxitraffic

    # role-specific:paperless
    - role: galaxy/paperless
    # /role-specific:paperless

    # role-specific:peertube
    - role: galaxy/peertube
    # /role-specific:peertube

    # role-specific:plausible
    - role: galaxy/plausible
    # /role-specific:plausible

    # role-specific:postgis
    - role: galaxy/postgis
    # /role-specific:postgis

    # role-specific:prometheus
    - role: galaxy/prometheus
    # /role-specific:prometheus

    # role-specific:prometheus_node_exporter
    - role: galaxy/prometheus_node_exporter
    # /role-specific:prometheus_node_exporter

    # role-specific:prometheus_blackbox_exporter
    - role: galaxy/prometheus_blackbox_exporter
    # /role-specific:prometheus_blackbox_exporter

    # role-specific:prometheus_postgres_exporter
    - role: galaxy/prometheus_postgres_exporter
    # /role-specific:prometheus_postgres_exporter

    # role-specific:prometheus_ssh_exporter
    - role: galaxy/prometheus_ssh_exporter
    # /role-specific:prometheus_ssh_exporter

    # role-specific:promtail
    - role: galaxy/promtail
    # /role-specific:promtail

    # role-specific:qbittorrent
    - role: galaxy/qbittorrent
    # /role-specific:qbittorrent

    # role-specific:radarr
    - role: galaxy/radarr
    # /role-specific:radarr

    # role-specific:radicale
    - role: galaxy/radicale
    # /role-specific:radicale

    # role-specific:readeck
    - role: galaxy/readeck
    # /role-specific:readeck

    # role-specific:redmine
    - role: galaxy/redmine
    # /role-specific:redmine

    # role-specific:redis
    - role: galaxy/redis
    # /role-specific:redis

    # role-specific:rumqttd
    - role: galaxy/rumqttd
    # /role-specific:rumqttd

    # role-specific:searxng
    - role: galaxy/searxng
    # /role-specific:searxng

    # role-specific:semaphore
    - role: galaxy/semaphore
    # /role-specific:semaphore

    # role-specific:soft_serve
    - role: galaxy/soft_serve
    # /role-specific:soft_serve

    # role-specific:sonarr
    - role: galaxy/sonarr
    # /role-specific:sonarr

    # role-specific:stirling_pdf
    - role: galaxy/stirling_pdf
    # /role-specific:stirling_pdf

    # role-specific:syncthing
    - role: galaxy/syncthing
    # /role-specific:syncthing

    # role-specific:tandoor
    - role: galaxy/tandoor
    # /role-specific:tandoor

    # role-specific:telegraf
    - role: galaxy/telegraf
    # /role-specific:telegraf

    # role-specific:tsdproxy
    - role: galaxy/tsdproxy
    # /role-specific:tsdproxy

    # role-specific:valkey
    - role: galaxy/valkey
    # /role-specific:valkey

    # role-specific:vaultwarden
    - role: galaxy/vaultwarden
    # /role-specific:vaultwarden

    # role-specific:uptime_kuma
    - role: galaxy/uptime_kuma
    # /role-specific:uptime_kuma

    # role-specific:wg_easy
    - role: galaxy/wg_easy
    # /role-specific:wg_easy

    # role-specific:forgejo
    - role: galaxy/forgejo
    # /role-specific:forgejo

    # role-specific:forgejo_runner
    - role: galaxy/forgejo_runner
    # /role-specific:forgejo_runner

    # role-specific:woodpecker_ci_server
    - role: galaxy/woodpecker_ci_server
    # /role-specific:woodpecker_ci_server

    # role-specific:versatiles
    - role: galaxy/versatiles
    # /role-specific:versatiles

    # role-specific:woodpecker_ci_agent
    - role: galaxy/woodpecker_ci_agent
    # /role-specific:woodpecker_ci_agent

    # role-specific:wordpress
    - role: galaxy/wordpress
    # /role-specific:wordpress

    # role-specific:writefreely
    - role: galaxy/writefreely
    # /role-specific:writefreely

    # role-specific:roundcube
    - role: galaxy/roundcube
    # /role-specific:roundcube

    # role-specific:auxiliary
    - role: galaxy/auxiliary
    # /role-specific:auxiliary

    # role-specific:matterbridge
    - role: galaxy/matterbridge
    # /role-specific:matterbridge

    # role-specific:systemd_service_manager
    - when: devture_systemd_service_manager_enabled | bool
      role: galaxy/systemd_service_manager
    # /role-specific:systemd_service_manager

    # role-specific:playbook_state_preserver
    # This is pretty much last, because we want it to better serve as a "last known good configuration".
    # See: https://github.com/spantaleev/matrix-docker-ansible-deploy/pull/2217#issuecomment-1301487601
    - when: devture_playbook_state_preserver_enabled | bool
      role: galaxy/playbook_state_preserver
      tags:
        - setup-all
        - install-all
    # /role-specific:playbook_state_preserver

    # role-specific:playbook_runtime_messages
    - role: galaxy/playbook_runtime_messages
    # /role-specific:playbook_runtime_messages
````

## File: .dockerignore
````
/inventory

# ignore roles pulled by ansible-galaxy
/roles/galaxy/*
!/roles/galaxy/.gitkeep

# ignores vscode file
.vscode

# ignores macos files
.DS_Store

/requirements.yml
/setup.yml
/group_vars/mash_servers

/run/*
!/run/.gitkeep

/group_vars/*
!/group_vars/.gitkeep
````

## File: .editorconfig
````
# This file is the top-most EditorConfig file
root = true

# All Files
[*]
charset = utf-8
end_of_line = lf
indent_style = tab
indent_size = 4
insert_final_newline = true
trim_trailing_whitespace = true

#########################
# File Extension Settings
#########################

# YAML Files
[*.{yml,yaml,log.config.j2,yaml.j2}]
indent_style = space
indent_size = 2

[*.py]
indent_style = space
indent_size = 4

[group_vars/mash_servers]
indent_style = space
indent_size = 2

[templates/group_vars_mash_servers]
indent_style = space
indent_size = 2

[justfile]
indent_style = space
indent_size = 4

# Markdown Files
#
# Two spaces at the end of a line in Markdown mean "new line",
# so trimming trailing whitespace for such files can cause breakage.
[*.md]
trim_trailing_whitespace = false
````

## File: .gitignore
````
/inventory

# ignore roles pulled by ansible-galaxy
/roles/galaxy/*
!/roles/galaxy/.gitkeep

# ignores vscode file
.vscode

# ignores macos files
.DS_Store

/requirements.yml
/setup.yml
/group_vars/mash_servers

/run/*
!/run/.gitkeep

/group_vars/*
!/group_vars/.gitkeep
````

## File: ansible.cfg
````
[defaults]
retry_files_enabled = False
stdout_callback = yaml

[connection]
pipelining = True
````

## File: CHANGELOG.md
````markdown
# 2025-03-08

## 6️⃣ IPv6 support enablement recommended by default

Our [default example configuration](./examples/vars.yml) and [Configuring DNS](./docs/configuring-dns.md) guides now recommend enabling [IPv6](https://en.wikipedia.org/wiki/IPv6) support. We recommend that everyone enables IPv6 support for their Matrix server, even if they don't have IPv6 connectivity yet.

Our new [Configuring IPv6](./docs/configuring-ipv6.md) documentation page has more details about the playbook's IPv6 support.

**Existing playbook users** will **need to do some manual work** to enable IPv6 support. This consists of:

- enabling IPv6 support for the Docker container networks:
	- add `devture_systemd_docker_base_ipv6_enabled: true` to their `vars.yml` configuration file
	- stop all services (`just stop-all`)
	- delete all container networks on the server: `docker network rm $(docker network ls -q)`
	- re-run the playbook fully: `just install-all`

- [configuring IPv6 (`AAAA`) DNS records](./docs/configuring-ipv6.md#configuring-dns-records-for-ipv6)

> [!WARNING]
> Not all mash-playbook Ansible roles respect the `devture_systemd_docker_base_ipv6_enabled` setting yet.
> Even if you enable this setting, you may still see that some container networks and services aren't IPv6-enabled.
> **Consider sending pull requests** for the playbook roles that do not respect the `devture_systemd_docker_base_ipv6_enabled` seting yet.

# 2025-02-21

## Docker daemon options are no longer adjusted when IPv6 is enabled

We landed initial IPv6 support in the past via a `devture_systemd_docker_base_ipv6_enabled` variable that one had to toggle to `true`.

This variable did **2 different things at once**:

- ensured that container networks were created with IPv6 being enabled
- adjusted the Docker daemon's configuration to set `experimental: true` and `ip6tables: true` (a necessary prerequisite for creating IPv6-enabled networks)

Since Docker 27.0.1's [changes to how it handles IPv6](https://docs.docker.com/engine/release-notes/27/#ipv6), **adjusting the Docker daemon's configuration is no longer necessary**, because:
- `ip6tables` defaults to `true` for everyone
- `ip6tables` is out of the experimental phase, so `experimental` is no longer necessary

In light of this, we're introducing a new variable (`devture_systemd_docker_base_ipv6_daemon_options_changing_enabled`) for controlling if IPv6 should be force-enabled in the Docker daemon's configuration options.
Since most people should be on a modern enough Docker daemon version which doesn't require such changes, this variable defaults to `false`.

This change affects you like this:

- ✅ if you're **not explicitly enabling IPv6** (via `devture_systemd_docker_base_ipv6_enabled` in your configuration): you're unaffected
- ❓ if you're **explicitly enabling IPv6** (via `devture_systemd_docker_base_ipv6_enabled` in your configuration):
  - ✅ .. and you're on a modern enough Docker version (which you most likely are): the playbook will no longer mess with your Docker daemon options. You're unaffected.
  - 🔧 .. and you're on an old Docker version, you **are affected** and need to use the following configuration to restore the old behavior:

    ```yml
    # Force-enable IPv6 by changing the Docker daemon's options.
    # This is necessary for Docker < 27.0.1, but not for newer versions.
    devture_systemd_docker_base_ipv6_daemon_options_changing_enabled: true

    # Request that individual container networks are created with IPv6 enabled.
    devture_systemd_docker_base_ipv6_enabled: true
    ```

# 2024-09-27

## (BC Break) Postgres, Traefik & Woodpecker CI roles have been relocated and variable names need adjustments

Various roles have been relocated from the [devture](https://github.com/devture) organization to the [mother-of-all-self-hosting](https://github.com/mother-of-all-self-hosting) organization.

Along with the relocation, the `devture_` prefix was dropped from their variable names, so you need to adjust your `vars.yml` configuration.

You need to do the following replacements:

- `devture_postgres_` -> `postgres_`
- `devture_traefik_` -> `traefik_`
- `devture_woodpecker_ci_` -> `woodpecker_ci_`

As always, the playbook would let you know about this and point out any variables you may have missed.


# 2024-07-06

## Traefik v3 and HTTP/3 are here now

**TLDR**: Traefik was migrated from v2 to v3. Minor changes were done to the playbook. Mostly everything else worked out of the box. Most people will not have to do any tweaks to their configuration. In addition, [HTTP/3](https://en.wikipedia.org/wiki/HTTP/3) support is now auto-enabled for the `web-secure` (port 443) and `matrix-federation` (port `8448`) entrypoints. If you have a firewall in front of your server and you wish to benefit from `HTTP3`, you will need to open the `443` and `8448` UDP ports in it.

### Traefik v3

The reverse-proxy that the playbook uses by default (Traefik) has recently been upgraded to v3 (see [this blog post](https://traefik.io/blog/announcing-traefik-proxy-v3-rc/) to learn about its new features). Version 3 includes some small breaking configuration changes requiring a [migration](https://doc.traefik.io/traefik/migration/v2-to-v3/).

We have **updated the playbook to Traefik v3** (make sure to run `just roles` / `make roles` to get it).

Most (all) MASH roles should not require any changes and should work with Traefik v3 by default.

**Most people using the playbook should not have to do any changes**.

If you're using the playbook's Traefik instance to reverse-proxy to some other services of your own (not managed by the playbook), you may wish to review their Traefik labels and make sure they're in line with the [Traefik v2 to v3 migration guide](https://doc.traefik.io/traefik/migration/v2-to-v3/).

If you've tweaked any of this playbook's `_path_prefix` variables and made them use a regular expression, you will now need to make additional adjustments. The playbook makes extensive use of `PathPrefix()` matchers in Traefik rules and `PathPrefix` does not support regular expressions anymore. To work around it, you may now need to override a whole `_traefik_rule` variable and switch it from [`PathPrefix` to `PathRegexp`](https://doc.traefik.io/traefik/routing/routers/#path-pathprefix-and-pathregexp).

You **may potentially downgrade to Traefik v2** (if necessary) by adding `traefik_verison: v2.11.4` to your configuration.


### HTTP/3 is enabled by default

In Traefik v3, [HTTP/3](https://en.wikipedia.org/wiki/HTTP/3) support is no longer considered experimental now.
Due to this, **the playbook auto-enables HTTP3** for the `web-secure` (port 443) entrypoint.

HTTP3 uses the UDP protocol and **the playbook (together with Docker) will make sure that the appropriate port** (`443` over UDP) **is exposed and whitelisted in your server's firewall**. However, **if you have another firewall in front of your server** (as is the case for many cloud providers), **you will need to manually open this UDP port**.

If you do not open the UDP port correctly or there is some other issue, clients (browsers, mostly) will fall-back to [HTTP/2](https://en.wikipedia.org/wiki/HTTP/2) or even [HTTP/1.1](https://en.wikipedia.org/wiki/HTTP).

Still, if HTTP/3 cannot function correctly in your setup, it's best to disable advertising support for it (and misleading clients into trying to use HTTP/3).

To **disable HTTP/3**, you can use the following configuration:

```yml
traefik_config_entrypoint_web_secure_http3_enabled: false
```


# 2023-10-18

## Postgres parameters are automatically tuned now

The playbook has provided some hints about [Tuning PostgreSQL](docs/maintenance-postgres.md#tuning-postgresql) for quite a while now.

From now on, the [Postgres Ansible role](https://github.com/devture/com.devture.ansible.role.postgres) automatically tunes your Postgres configuration with the same [calculation logic](https://github.com/le0pard/pgtune/blob/master/src/features/configuration/configurationSlice.js) that powers https://pgtune.leopard.in.ua/.

Our [Tuning PostgreSQL](docs/maintenance-postgres.md#tuning-postgresql) documentation page has details about how you can turn auto-tuning off or adjust the automatically-determined Postgres configuration parameters manually.


# 2023-04-23

## (Backward Compatibility Break) Authentik container variables renamed

For the authentik role there wehre initially two containers: `authentic_worker_container` and `authentic_server_container`. To simnplifiy the setup this was reduced to one container.
As the role is pretty young and to avoid confusion because of legacy and reverted design decisions all variables containing `authentik_server_container` will now start with authentik_container. This means you will have to renemae these variables in your `vars.yml` if you already use authentik. If you use a standard setup this only includes

* `authentic_server_container_additional_networks_custom` -> `authentik_container_additional_networks_custom`

# 2023-03-29

## (Backward Compatibility Break) Firezone database renamed

If you are running [Firezone](docs/services/firezone.md) with the default [Postgres](docs/services/postgres.md) integration the playbook automatically created the database with the name `mash-firezone`.
To be consistent with how this playbook names databases for all other services, going forward we've changed the database name to be just `firezone`. You will have to rename you database manually by running the following commands on your server:

1. Stop Firezone: `systemctl stop mash-firezone`
2. Run a Postgres `psql` shell: `/mash/postgres/bin/cli`
3. Execute this query: `ALTER DATABASE "mash-firezone" RENAME TO firezone;` and then quit the shell with `\q`

Then update the playbook (don't forget to run `just roles`), run `just install-all` and you should be good to go!

# 2023-03-26

## (Backward Compatibility Break) PeerTube is no longer wired to Redis automatically

As described in our [Redis](docs/services/redis.md) services docs, running a single instance of Redis to be used by multiple services is not a good practice.

For this reason, we're no longer auto-wiring PeerTube to Redis. If you're running other services (which may require Redis in the future) on the same host, it's recommended that you follow the [Creating a Redis instance dedicated to PeerTube](docs/services/peertube.md#creating-a-redis-instance-dedicated-to-peertube) documentation.

If you're only running PeerTube on a dedicated server (no other services that may need Redis) or you'd like to stick to what you've used until now (a single shared Redis instance), follow the [Using the shared Redis instance for PeerTube](docs/services/peertube.md#using-the-shared-redis-instance-for-peertube) documentation.


# 2023-03-25

## (Backward Compatibility Break) Docker no longer installed by default

The playbook used to install Docker and the Docker SDK for Python by default, unless you turned these off by setting `mash_playbook_docker_installation_enabled` and `devture_docker_sdk_for_python_installation_enabled` (respectively) to `false`.

From now on, both of these variables default to `false`. An empty inventory file will not install these components.

**Most** users will want to enable these, just like they would want to enable [Traefik](docs/services/traefik.md) and [Postgres](docs/services/postgres.md), so why default them to `false`? The answer is: it's cleaner to have "**everything** is off by default - enable as you wish" and just need to add stuff, as opposed to "**some** things are on, **some** are off - toggle as you wish".

To enable these components, you need to explicitly add something like this to your `vars.yml` file:

```yaml
########################################################################
#                                                                      #
# Docker                                                               #
#                                                                      #
########################################################################

mash_playbook_docker_installation_enabled: true

devture_docker_sdk_for_python_installation_enabled: true

########################################################################
#                                                                      #
# /Docker                                                              #
#                                                                      #
########################################################################
```

Our [example vars.yml](examples/vars.yml) file has been updated, so that new hosts created based on it will have this configuration by default.


# 2023-03-15

## Initial release

This is the initial release of this playbook.
````

## File: Dockerfile
````dockerfile
FROM alpine:3.20

ENV ANSIBLE_LOG_PATH=" "
WORKDIR /playbook
ENTRYPOINT ["/bin/sh"]

RUN apk add --no-cache ansible ansible-core py3-passlib git openssh-client just

COPY . /playbook

RUN git rev-parse HEAD > /playbook/source-commit && \
	rm -rf /playbook/.git && \
	just roles
````

## File: justfile
````
# Shows help
default:
    @{{ just_executable() }} --list --justfile {{ justfile() }}

run_directory_path := justfile_directory() + "/run"
templates_directory_path := justfile_directory() + "/templates"
optimization_vars_files_file_path := run_directory_path + "/optimization-vars-files.state"

# Pulls external Ansible roles
roles: _requirements-yml
    #!/usr/bin/env sh
    if [ -x "$(command -v agru)" ]; then
        echo "[NOTE] This command just updates the roles, but if you want to update everything at once (playbook, roles, etc.) - use 'just update'"
        agru -r {{ justfile_directory() }}/requirements.yml
    else
        echo "[NOTE] You are using the standard ansible-galaxy tool to install roles, which is slow and lacks other features. We recommend installing the 'agru' tool to speed up the process: https://github.com/etkecc/agru#where-to-get"
        echo "[NOTE] This command just updates the roles, but if you want to update everything at once (playbook, roles, etc.) - use 'just update'"
        rm -rf roles/galaxy
        ansible-galaxy install -r {{ justfile_directory() }}/requirements.yml -p roles/galaxy/ --force
    fi

# Optimizes the playbook based on stored configuration (vars.yml paths)
optimize-restore:
    #!/usr/bin/env sh
    if [ -f "{{ optimization_vars_files_file_path }}" ]; then
        {{ just_executable() }} --justfile {{ justfile() }} \
        _optimize-for-var-paths \
        $(cat {{ optimization_vars_files_file_path }})
    else
        echo "Cannot restore optimization state from a file ($optimization_vars_files_file_path), because it doesn't exist"
        exit 1
    fi

# Clears optimizations and resets the playbook to a non-optimized state
optimize-reset: && _clean_template_derived_files
    #!/usr/bin/env sh
    rm -f {{ run_directory_path }}/*.srchash
    rm -f {{ optimization_vars_files_file_path }}

# Optimizes the playbook based on the enabled components for all hosts in the inventory
optimize inventory_path='inventory': _reconfigure-for-all-hosts

_reconfigure-for-all-hosts inventory_path='inventory':
    #!/usr/bin/env sh
    {{ just_executable() }} --justfile {{ justfile() }} \
    _optimize-for-var-paths \
    $(find {{ inventory_path }}/host_vars/ -maxdepth 2 -name 'vars.yml' -exec readlink -f {} \;)

# Optimizes the playbook based on the enabled components for a single host
optimize-for-host hostname inventory_path='inventory':
    #!/usr/bin/env sh
    {{ just_executable() }} --justfile {{ justfile() }} \
    _optimize-for-var-paths \
    $(find {{ inventory_path }}/host_vars/{{ hostname }} -maxdepth 1 -name 'vars.yml' -exec readlink -f {} \;)

# Optimizes the playbook based on the enabled components found in the given vars.yml files
_optimize-for-var-paths +PATHS:
    #!/usr/bin/env sh
    echo '{{ PATHS }}' > {{ optimization_vars_files_file_path }}

    {{ just_executable() }} --justfile {{ justfile() }} _save_hash_for_file {{ templates_directory_path }}/requirements.yml {{ justfile_directory() }}/requirements.yml
    {{ just_executable() }} --justfile {{ justfile() }} _save_hash_for_file {{ templates_directory_path }}/setup.yml {{ justfile_directory() }}/setup.yml
    {{ just_executable() }} --justfile {{ justfile() }} _save_hash_for_file {{ templates_directory_path }}/group_vars_mash_servers {{ justfile_directory() }}/group_vars/mash_servers

    /usr/bin/env python {{ justfile_directory() }}/bin/optimize.py \
    --vars-paths='{{ PATHS }}' \
    --src-requirements-yml-path={{ templates_directory_path }}/requirements.yml \
    --dst-requirements-yml-path={{ justfile_directory() }}/requirements.yml \
    --src-setup-yml-path={{ templates_directory_path }}/setup.yml \
    --dst-setup-yml-path={{ justfile_directory() }}/setup.yml \
    --src-group-vars-yml-path={{ templates_directory_path }}/group_vars_mash_servers \
    --dst-group-vars-yml-path={{ justfile_directory() }}/group_vars/mash_servers

# Updates the playbook and installs the necessary Ansible roles pinned in requirements.yml. If a -u flag is passed, also updates the requirements.yml file with new role versions (if available)
update *flags: update-playbook-only
    #!/usr/bin/env sh
    if [ -x "$(command -v agru)" ]; then
        echo {{ if flags == "" { "Installing roles pinned in requirements.yml..." } else if flags == "-u" { "Updating roles and pinning new versions in requirements.yml..." } else { "Unknown flags passed" } }}
        agru -r {{ templates_directory_path }}/requirements.yml {{ flags }}
    else
        echo "[NOTE] You are using the standard ansible-galaxy tool to install roles, which is slow and lacks other features. We recommend installing the 'agru' tool to speed up the process: https://github.com/etkecc/agru#where-to-get"
        echo "Installing roles..."
        rm -rf roles/galaxy
        ansible-galaxy install -r requirements.yml -p roles/galaxy/ --force
    fi

    if [[ "{{ flags }}" == "-u" ]]; then
        {{ just_executable() }} --justfile {{ justfile() }} versions
        {{ just_executable() }} --justfile {{ justfile() }} opml
    fi

# Updates the playbook without installing/updating Ansible roles
update-playbook-only:
    @echo "Updating playbook..."
    @git stash -q
    @git pull -q
    @-git stash pop -q

# Runs ansible-lint against all roles in the playbook
lint:
    ansible-lint

# dumps an OPML file with extracted git feeds for roles
opml:
    @echo "generating opml..."
    @python bin/feeds.py . dump

# dumps versions of the components found in the roles to the VERSIONS.md file
versions:
    @echo "generating versions..."
    @python bin/versions.py

# Runs the playbook with --tags=install-all,start and optional arguments
install-all *extra_args: (run-tags "install-all,start" extra_args)

# Runs installation tasks for a single service
install-service service *extra_args:
    {{ just_executable() }} --justfile {{ justfile() }} run \
    --tags=install-{{ service }},start-group \
    --extra-vars=group={{ service }} \
    --extra-vars=devture_systemd_service_manager_service_restart_mode=one-by-one {{ extra_args }}

# Runs the playbook with --tags=setup-all,start and optional arguments
setup-all *extra_args: (run-tags "setup-all,start" extra_args)

# Runs setup tasks for a single service
setup-service service *extra_args:
    {{ just_executable() }} --justfile {{ justfile() }} run \
    --tags=setup-{{ service }},start-group \
    --extra-vars=group={{ service }} \
    --extra-vars=devture_systemd_service_manager_service_restart_mode=one-by-one {{ extra_args }}

# Runs the playbook with the given list of arguments
run +extra_args: _requirements-yml _setup-yml _group-vars-mash-servers
    ansible-playbook -i inventory/hosts setup.yml {{ extra_args }}

# Runs the playbook with the given list of comma-separated tags and optional arguments
run-tags tags *extra_args:
    {{ just_executable() }} --justfile {{ justfile() }} run --tags={{ tags }} {{ extra_args }}

# Starts all services
start-all *extra_args: (run-tags "start-all" extra_args)

# Starts a specific service group
start-group group *extra_args:
    @{{ just_executable() }} --justfile {{ justfile() }} run-tags start-group --extra-vars="group={{ group }}" {{ extra_args }}

# Stops all services
stop-all *extra_args: (run-tags "stop-all" extra_args)

# Stops a specific service group
stop-group group *extra_args:
    @{{ just_executable() }} --justfile {{ justfile() }} run-tags stop-group --extra-vars="group={{ group }}" {{ extra_args }}

# Prepares the requirements.yml file
_requirements-yml:
    @{{ just_executable() }} --justfile {{ justfile() }} _ensure_file_prepared {{ templates_directory_path }}/requirements.yml {{ justfile_directory() }}/requirements.yml

# Prepares the setup.yml file
_setup-yml:
    @{{ just_executable() }} --justfile {{ justfile() }} _ensure_file_prepared {{ templates_directory_path }}/setup.yml {{ justfile_directory() }}/setup.yml

# Prepares the group_vars/mash_servers file
_group-vars-mash-servers:
    @{{ just_executable() }} --justfile {{ justfile() }} _ensure_file_prepared {{ templates_directory_path }}/group_vars_mash_servers {{ justfile_directory() }}/group_vars/mash_servers

_ensure_file_prepared src_path dst_path:
    #!/usr/bin/env sh
    dst_file_name=$(basename "{{ dst_path }}")
    hash_path={{ run_directory_path }}"/"$dst_file_name".srchash"
    src_hash=$(md5sum {{ src_path }} | cut -d ' ' -f 1)

    if [ ! -f "{{ dst_path }}" ] || [ ! -f "$hash_path" ]; then
        cp {{ src_path }} {{ dst_path }}
        echo $src_hash > $hash_path
    else
        current_hash=$(cat $hash_path)

        if [ "$current_hash" != "$src_hash" ]; then
            cp {{ src_path }} {{ dst_path }}
            echo $src_hash > $hash_path

            if [ -f "{{ optimization_vars_files_file_path }}" ]; then
                {{ just_executable() }} --justfile {{ justfile() }} \
                _optimize-for-var-paths \
                $(cat {{ optimization_vars_files_file_path }})
            fi
        fi
    fi

_save_hash_for_file src_path dst_path:
    #!/usr/bin/env sh
    dst_file_name=$(basename "{{ dst_path }}")
    hash_path={{ run_directory_path }}"/"$dst_file_name".srchash"
    src_hash=$(md5sum {{ src_path }} | cut -d ' ' -f 1)
    echo $src_hash > $hash_path

_clean_template_derived_files:
    #!/usr/bin/env sh
    if [ -f "{{ justfile_directory() }}/requirements.yml" ]; then
        rm {{ justfile_directory() }}/requirements.yml
    fi

    if [ -f "{{ justfile_directory() }}/setup.yml" ]; then
        rm {{ justfile_directory() }}/setup.yml
    fi

    if [ -f "{{ justfile_directory() }}/group_vars/mash_servers" ]; then
        rm {{ justfile_directory() }}/group_vars/mash_servers
    fi
````

## File: LICENSE
````
GNU AFFERO GENERAL PUBLIC LICENSE
                       Version 3, 19 November 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU Affero General Public License is a free, copyleft license for
software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
our General Public Licenses are intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

  A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate.  Many developers of free software are heartened and
encouraged by the resulting cooperation.  However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

  The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

  An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals.  This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU Affero General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Remote Network Interaction; Use with the GNU General Public License.

  Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users
interacting with it remotely through a computer network (if your version
supports such interaction) an opportunity to receive the Corresponding
Source of your version by providing access to the Corresponding Source
from a network server at no charge, through some standard or customary
means of facilitating copying of software.  This Corresponding Source
shall include the Corresponding Source for any work covered by version 3
of the GNU General Public License that is incorporated pursuant to the
following paragraph.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the work with which it is combined will remain governed by version
3 of the GNU General Public License.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time.  Such new versions
will be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU Affero General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a "Source" link that leads users to an archive
of the code.  There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU AGPL, see
<https://www.gnu.org/licenses/>.
````

## File: README.md
````markdown
[![Support room on Matrix](https://img.shields.io/matrix/mash-playbook:devture.com.svg?label=%23mash-playbook%3Adevture.com&logo=matrix&style=for-the-badge&server_fqdn=matrix.devture.com)](https://matrix.to/#/#mash-playbook:devture.com) [![donate](https://liberapay.com/assets/widgets/donate.svg)](https://liberapay.com/mother-of-all-self-hosting/donate)

# Mother-of-All-Self-Hosting Ansible playbook

**MASH** (**M**other-of-**A**ll-**S**elf-**H**osting) is an [Ansible](https://www.ansible.com/) playbook that helps you self-host services as [Docker](https://www.docker.com/) containers on your own server.

By running services in containers, we can have a predictable and up-to-date setup, across multiple supported distros and CPU architectures.

This project allows self-hosting of a [large number of services](docs/supported-services.md) and will continue to grow by adding support for [FOSS](https://en.wikipedia.org/wiki/Free_and_open-source_software).

[Installation](docs/README.md) (upgrades) and some maintenance tasks are automated using [Ansible](https://www.ansible.com/) (see [our Ansible guide](docs/ansible.md)).


## Supported services

See the [full list of supported services here](docs/supported-services.md).


## Installation

To configure and install services on your own server, follow the [README in the docs/ directory](docs/README.md).


## Changes

This playbook evolves over time, sometimes with backward-incompatible changes.

When updating the playbook, refer to [the changelog](CHANGELOG.md) to catch up with what's new.


## Support

- Matrix room: [#mash-playbook:devture.com](https://matrix.to/#/#mash-playbook:devture.com). To join Matrix, [use a public server](https://app.element.io) like `matrix.org` or self-host Matrix yourself using the related [matrix-docker-ansible-deploy](https://github.com/spantaleev/matrix-docker-ansible-deploy) Ansible playbook

- GitHub issues: [mother-of-all-self-hosting/mash-playbook/issues](https://github.com/mother-of-all-self-hosting/mash-playbook/issues)


## Why create such a mega playbook?

We used to maintain separate playbooks for various services ([Matrix](https://github.com/spantaleev/matrix-docker-ansible-deploy), [Nextcloud](https://github.com/spantaleev/nextcloud-docker-ansible-deploy), [Gitea](https://github.com/spantaleev/gitea-docker-ansible-deploy), [Gitlab](https://github.com/spantaleev/gitlab-docker-ansible-deploy), [Vaultwarden](https://github.com/spantaleev/vaultwarden-docker-ansible-deploy), [PeerTube](https://github.com/spantaleev/peertube-docker-ansible-deploy), ..). They re-used Ansible roles (for [Postgres](https://github.com/devture/com.devture.ansible.role.postgres), [Traefik](https://github.com/devture/com.devture.ansible.role.traefik), etc.), but were still hard to maintain due to the large duplication of effort.

Most of these playbooks hosted services which require a Postgres database, a Traefik reverse-proxy, a backup solution, etc. All of them needed to come with documentation, etc.
All these things need to be created and kept up-to-date in each and every playbook.

Having to use a dedicated Ansible playbook for each and every piece of software means that you have to juggle many playbooks and make sure they don't conflict with one another when installing services on the same server. All these related playbooks interoperated nicely, but still required at least a bit of manual configuration to achieve this interoperability.

Using specialized Ansible playbooks also means that trying out new software is difficult. Despite the playbooks being similar (which eases the learning curve), each one is still a new git repository you need to clone and maintain, etc.

Furthermore, not all pieces of software are large enough to justify having their own dedicated Ansible playbook. They have no home, so no one uses them.

We're finding the need for a playbook which combines all of this into one, so that:

- you don't need to juggle multiple Ansible playbooks
- you can try out various services easily - a few lines of extra configuration and you're ready to go
- small pieces of software (like [Miniflux](https://miniflux.app/), powered by the [miniflux](https://gitlab.com/etke.cc/roles/miniflux) Ansible role) which don't have their own playbook can finally find a home
- you can use a single playbook with the quality you know and trust
- shared services (like Postgres) are maintained in one single place
- backups are made easy, because everything lives together (same base data path, same Postgres instance)

Having one large playbook with all services does not necessarily mean you need to host everything on the same server though. Feel free to use as many servers as you see fit. While containers provide some level of isolation, it's still better to not put all your eggs in one basket and create a single point of failure.

All of the aforementioned playbooks have been absorbed into this one. See the [full list of supported services here](docs/supported-services.md).
The [Matrix playbook](https://github.com/spantaleev/matrix-docker-ansible-deploy) will remain separate, because it contains a huge number of components and will likely grow even more. It deserves to stand on its own.


## What's with the name?

Our goal is to create a large Ansible playbook which can be your all-in-one-toolkit for self-hosting services in a clean and reliable way.

We like the MASH acronym, and [mashing](https://en.wikipedia.org/wiki/Mashing) is popular in the alcohol brewing industry. The result of all that mash is an enjoyable (at least by some) product.

Then, there's mixing and mashing stuff, which is also what this Ansible playbook is all about - you can mix and mash various pieces of software to create the self-hosted stack of your dreams!
````

## File: releases.opml
````
<?xml version='1.0' encoding='UTF-8'?>
<opml version="1.0">
  <head>
    <title>Release feeds for roles</title>
  </head>
  <body>
    <outline text="adguard_home" title="adguard_home" type="rss" htmlUrl="https://github.com/AdguardTeam/AdguardHome" xmlUrl="https://github.com/AdguardTeam/AdguardHome/releases.atom" />
    <outline text="apisix_dashboard" title="apisix_dashboard" type="rss" htmlUrl="https://github.com/apache/apisix-dashboard" xmlUrl="https://github.com/apache/apisix-dashboard/releases.atom" />
    <outline text="apisix_gateway" title="apisix_gateway" type="rss" htmlUrl="https://github.com/apache/apisix" xmlUrl="https://github.com/apache/apisix/releases.atom" />
    <outline text="appsmith" title="appsmith" type="rss" htmlUrl="https://github.com/appsmithorg/appsmith" xmlUrl="https://github.com/appsmithorg/appsmith/releases.atom" />
    <outline text="authelia" title="authelia" type="rss" htmlUrl="https://github.com/authelia/authelia" xmlUrl="https://github.com/authelia/authelia/releases.atom" />
    <outline text="authentik" title="authentik" type="rss" htmlUrl="https://github.com/goauthentik/authentik" xmlUrl="https://github.com/goauthentik/authentik/releases.atom" />
    <outline text="backup_borg" title="backup_borg" type="rss" htmlUrl="https://gitlab.com/etke.cc/borgmatic" xmlUrl="https://gitlab.com/etke.cc/borgmatic/-/tags?format=atom" />
    <outline text="calibre-web" title="calibre-web" type="rss" htmlUrl="https://github.com/calibre_web/calibre_web" xmlUrl="https://github.com/calibre_web/calibre_web/releases.atom" />
    <outline text="changedetection" title="changedetection" type="rss" htmlUrl="https://github.com/dgtlmoon/changedetection.io" xmlUrl="https://github.com/dgtlmoon/changedetection.io/releases.atom" />
    <outline text="clickhouse" title="clickhouse" type="rss" htmlUrl="https://github.com/ClickHouse/ClickHouse" xmlUrl="https://github.com/ClickHouse/ClickHouse/releases.atom" />
    <outline text="container_socket_proxy" title="container_socket_proxy" type="rss" htmlUrl="https://github.com/Tecnativa/docker-socket-proxy" xmlUrl="https://github.com/Tecnativa/docker-socket-proxy/releases.atom" />
    <outline text="couchdb" title="couchdb" type="rss" htmlUrl="https://github.com/apache/couchdb" xmlUrl="https://github.com/apache/couchdb/releases.atom" />
    <outline text="docker_registry" title="docker_registry" type="rss" htmlUrl="https://github.com/distribution/distribution/" xmlUrl="https://github.com/distribution/distribution//releases.atom" />
    <outline text="docker_registry_proxy" title="docker_registry_proxy" type="rss" htmlUrl="https://github.com/etkecc/docker-registry-proxy" xmlUrl="https://github.com/etkecc/docker-registry-proxy/releases.atom" />
    <outline text="dokuwiki" title="dokuwiki" type="rss" htmlUrl="https://github.com/dokuwiki/docker" xmlUrl="https://github.com/dokuwiki/docker/releases.atom" />
    <outline text="echoip" title="echoip" type="rss" htmlUrl="https://github.com/mpolden/echoip" xmlUrl="https://github.com/mpolden/echoip/releases.atom" />
    <outline text="etcd" title="etcd" type="rss" htmlUrl="https://github.com/etcd-io/etcd" xmlUrl="https://github.com/etcd-io/etcd/releases.atom" />
    <outline text="etherpad" title="etherpad" type="rss" htmlUrl="https://github.com/ether/etherpad-lite" xmlUrl="https://github.com/ether/etherpad-lite/releases.atom" />
    <outline text="exim_relay" title="exim_relay" type="rss" htmlUrl="https://github.com/devture/exim-relay" xmlUrl="https://github.com/devture/exim-relay/releases.atom" />
    <outline text="firezone" title="firezone" type="rss" htmlUrl="https://github.com/firezone/firezone" xmlUrl="https://github.com/firezone/firezone/releases.atom" />
    <outline text="focalboard" title="focalboard" type="rss" htmlUrl="https://github.com/mattermost/focalboard" xmlUrl="https://github.com/mattermost/focalboard/releases.atom" />
    <outline text="forgejo" title="forgejo" type="rss" htmlUrl="https://codeberg.org/forgejo/forgejo" xmlUrl="https://codeberg.org/forgejo/forgejo/releases.atom" />
    <outline text="forgejo_runner" title="forgejo_runner" type="rss" htmlUrl="https://code.forgejo.org/forgejo/runner" xmlUrl="https://code.forgejo.org/forgejo/runner/releases.atom" />
    <outline text="freescout" title="freescout" type="rss" htmlUrl="https://github.com/tiredofit/docker-freescout" xmlUrl="https://github.com/tiredofit/docker-freescout/releases.atom" />
    <outline text="freshrss" title="freshrss" type="rss" htmlUrl="https://github.com/freshrss/freshrss" xmlUrl="https://github.com/freshrss/freshrss/releases.atom" />
    <outline text="funkwhale" title="funkwhale" type="rss" htmlUrl="https://dev.funkwhale.audio/funkwhale/funkwhale" xmlUrl="https://dev.funkwhale.audio/funkwhale/funkwhale/-/tags?format=atom" />
    <outline text="gotosocial" title="gotosocial" type="rss" htmlUrl="https://github.com/superseriousbusiness/gotosocial" xmlUrl="https://github.com/superseriousbusiness/gotosocial/releases.atom" />
    <outline text="grafana" title="grafana" type="rss" htmlUrl="https://github.com/grafana/grafana" xmlUrl="https://github.com/grafana/grafana/releases.atom" />
    <outline text="headscale" title="headscale" type="rss" htmlUrl="https://github.com/juanfont/headscale" xmlUrl="https://github.com/juanfont/headscale/releases.atom" />
    <outline text="healthchecks" title="healthchecks" type="rss" htmlUrl="https://github.com/healthchecks/healthchecks" xmlUrl="https://github.com/healthchecks/healthchecks/releases.atom" />
    <outline text="infisical" title="infisical" type="rss" htmlUrl="https://github.com/Infisical/infisical" xmlUrl="https://github.com/Infisical/infisical/releases.atom" />
    <outline text="influxdb" title="influxdb" type="rss" htmlUrl="https://github.com/influxdata/influxdb" xmlUrl="https://github.com/influxdata/influxdb/releases.atom" />
    <outline text="jackett" title="jackett" type="rss" htmlUrl="https://github.com/Jackett/Jackett" xmlUrl="https://github.com/Jackett/Jackett/releases.atom" />
    <outline text="jitsi" title="jitsi" type="rss" htmlUrl="https://github.com/jitsi/docker-jitsi-meet" xmlUrl="https://github.com/jitsi/docker-jitsi-meet/releases.atom" />
    <outline text="keycloak" title="keycloak" type="rss" htmlUrl="https://github.com/keycloak/keycloak" xmlUrl="https://github.com/keycloak/keycloak/releases.atom" />
    <outline text="keydb" title="keydb" type="rss" htmlUrl="https://github.com/Snapchat/KeyDB" xmlUrl="https://github.com/Snapchat/KeyDB/releases.atom" />
    <outline text="labelstudio" title="labelstudio" type="rss" htmlUrl="https://github.com/labelstudio/labelstudio" xmlUrl="https://github.com/labelstudio/labelstudio/releases.atom" />
    <outline text="lago" title="lago" type="rss" htmlUrl="https://github.com/lago/lago" xmlUrl="https://github.com/lago/lago/releases.atom" />
    <outline text="languagetool" title="languagetool" type="rss" htmlUrl="https://github.com/Erikvl87/docker-languagetool" xmlUrl="https://github.com/Erikvl87/docker-languagetool/releases.atom" />
    <outline text="linkding" title="linkding" type="rss" htmlUrl="https://github.com/sissbruecker/linkding" xmlUrl="https://github.com/sissbruecker/linkding/releases.atom" />
    <outline text="loki" title="loki" type="rss" htmlUrl="https://github.com/grafana/loki" xmlUrl="https://github.com/grafana/loki/releases.atom" />
    <outline text="miniflux" title="miniflux" type="rss" htmlUrl="https://github.com/miniflux/v2" xmlUrl="https://github.com/miniflux/v2/releases.atom" />
    <outline text="mobilizon" title="mobilizon" type="rss" htmlUrl="https://framagit.org/framasoft/mobilizon/" xmlUrl="https://framagit.org/framasoft/mobilizon//-/tags?format=atom" />
    <outline text="mongodb" title="mongodb" type="rss" htmlUrl="https://github.com/mongodb/mongo" xmlUrl="https://github.com/mongodb/mongo/releases.atom" />
    <outline text="n8n" title="n8n" type="rss" htmlUrl="https://github.com/n8n-io/n8n" xmlUrl="https://github.com/n8n-io/n8n/releases.atom" />
    <outline text="navidrome" title="navidrome" type="rss" htmlUrl="https://github.com/navidrome/navidrome" xmlUrl="https://github.com/navidrome/navidrome/releases.atom" />
    <outline text="netbox" title="netbox" type="rss" htmlUrl="https://github.com/netbox-community/netbox-docker/" xmlUrl="https://github.com/netbox-community/netbox-docker//releases.atom" />
    <outline text="nextcloud" title="nextcloud" type="rss" htmlUrl="https://github.com/nextcloud/server" xmlUrl="https://github.com/nextcloud/server/releases.atom" />
    <outline text="ntfy" title="ntfy" type="rss" htmlUrl="https://github.com/binwiederhier/ntfy" xmlUrl="https://github.com/binwiederhier/ntfy/releases.atom" />
    <outline text="oauth2_proxy" title="oauth2_proxy" type="rss" htmlUrl="https://github.com/oauth2-proxy/oauth2-proxy" xmlUrl="https://github.com/oauth2-proxy/oauth2-proxy/releases.atom" />
    <outline text="outline" title="outline" type="rss" htmlUrl="https://github.com/outline/outline" xmlUrl="https://github.com/outline/outline/releases.atom" />
    <outline text="overseerr" title="overseerr" type="rss" htmlUrl="https://github.com/sct/overseerr" xmlUrl="https://github.com/sct/overseerr/releases.atom" />
    <outline text="owncast" title="owncast" type="rss" htmlUrl="https://github.com/owncast/owncast" xmlUrl="https://github.com/owncast/owncast/releases.atom" />
    <outline text="paperless" title="paperless" type="rss" htmlUrl="https://github.com/paperless-ngx/paperless-ngx" xmlUrl="https://github.com/paperless-ngx/paperless-ngx/releases.atom" />
    <outline text="paperless-2" title="paperless-2" type="rss" htmlUrl="https://github.com/apache/tika" xmlUrl="https://github.com/apache/tika/releases.atom" />
    <outline text="paperless-3" title="paperless-3" type="rss" htmlUrl="https://github.com/gotenberg/gotenberg" xmlUrl="https://github.com/gotenberg/gotenberg/releases.atom" />
    <outline text="peertube" title="peertube" type="rss" htmlUrl="https://github.com/Chocobozzz/PeerTube" xmlUrl="https://github.com/Chocobozzz/PeerTube/releases.atom" />
    <outline text="plausible" title="plausible" type="rss" htmlUrl="https://github.com/plausible/analytics" xmlUrl="https://github.com/plausible/analytics/releases.atom" />
    <outline text="postgis" title="postgis" type="rss" htmlUrl="https://git.osgeo.org/gitea/postgis/postgis" xmlUrl="https://git.osgeo.org/gitea/postgis/postgis.atom" />
    <outline text="postgres" title="postgres" type="rss" htmlUrl="https://github.com/postgres/postgres" xmlUrl="https://github.com/postgres/postgres/releases.atom" />
    <outline text="postgres_backup" title="postgres_backup" type="rss" htmlUrl="https://github.com/prodrigestivill/docker-postgres-backup-local" xmlUrl="https://github.com/prodrigestivill/docker-postgres-backup-local/releases.atom" />
    <outline text="prometheus" title="prometheus" type="rss" htmlUrl="https://github.com/prometheus/prometheus" xmlUrl="https://github.com/prometheus/prometheus/releases.atom" />
    <outline text="prometheus_blackbox_exporter" title="prometheus_blackbox_exporter" type="rss" htmlUrl="https://github.com/prometheus/blackbox_exporter" xmlUrl="https://github.com/prometheus/blackbox_exporter/releases.atom" />
    <outline text="prometheus_node_exporter" title="prometheus_node_exporter" type="rss" htmlUrl="https://github.com/prometheus/node_exporter" xmlUrl="https://github.com/prometheus/node_exporter/releases.atom" />
    <outline text="prometheus_postgres_exporter" title="prometheus_postgres_exporter" type="rss" htmlUrl="https://github.com/prometheus-community/postgres_exporter" xmlUrl="https://github.com/prometheus-community/postgres_exporter/releases.atom" />
    <outline text="prometheus_ssh_exporter" title="prometheus_ssh_exporter" type="rss" htmlUrl="https://github.com/treydock/ssh_exporter" xmlUrl="https://github.com/treydock/ssh_exporter/releases.atom" />
    <outline text="promtail" title="promtail" type="rss" htmlUrl="https://github.com/grafana/loki" xmlUrl="https://github.com/grafana/loki/releases.atom" />
    <outline text="qbittorrent" title="qbittorrent" type="rss" htmlUrl="https://github.com/qBittorrent/qBittorrent" xmlUrl="https://github.com/qBittorrent/qBittorrent/releases.atom" />
    <outline text="radarr" title="radarr" type="rss" htmlUrl="https://github.com/Radarr/Radarr" xmlUrl="https://github.com/Radarr/Radarr/releases.atom" />
    <outline text="radicale" title="radicale" type="rss" htmlUrl="https://github.com/tomsquest/docker-radicale" xmlUrl="https://github.com/tomsquest/docker-radicale/releases.atom" />
    <outline text="readeck" title="readeck" type="rss" htmlUrl="https://codeberg.org/readeck/readeck" xmlUrl="https://codeberg.org/readeck/readeck/releases.atom" />
    <outline text="redis" title="redis" type="rss" htmlUrl="https://github.com/redis/redis" xmlUrl="https://github.com/redis/redis/releases.atom" />
    <outline text="redmine" title="redmine" type="rss" htmlUrl="https://github.com/redmine/redmine" xmlUrl="https://github.com/redmine/redmine/releases.atom" />
    <outline text="roundcube" title="roundcube" type="rss" htmlUrl="https://github.com/roundcube/roundcubemail" xmlUrl="https://github.com/roundcube/roundcubemail/releases.atom" />
    <outline text="searxng" title="searxng" type="rss" htmlUrl="https://github.com/searxng/searxng" xmlUrl="https://github.com/searxng/searxng/releases.atom" />
    <outline text="semaphore" title="semaphore" type="rss" htmlUrl="https://github.com/ansible-semaphore/semaphore" xmlUrl="https://github.com/ansible-semaphore/semaphore/releases.atom" />
    <outline text="soft_serve" title="soft_serve" type="rss" htmlUrl="https://github.com/charmbracelet/soft-serve" xmlUrl="https://github.com/charmbracelet/soft-serve/releases.atom" />
    <outline text="sonarr" title="sonarr" type="rss" htmlUrl="https://github.com/Sonarr/Sonarr" xmlUrl="https://github.com/Sonarr/Sonarr/releases.atom" />
    <outline text="stirling_pdf" title="stirling_pdf" type="rss" htmlUrl="https://github.com/Stirling-Tools/Stirling-PDF" xmlUrl="https://github.com/Stirling-Tools/Stirling-PDF/releases.atom" />
    <outline text="syncthing" title="syncthing" type="rss" htmlUrl="https://github.com/syncthing/syncthing/" xmlUrl="https://github.com/syncthing/syncthing//releases.atom" />
    <outline text="tandoor" title="tandoor" type="rss" htmlUrl="https://github.com/TandoorRecipes/recipes" xmlUrl="https://github.com/TandoorRecipes/recipes/releases.atom" />
    <outline text="traefik" title="traefik" type="rss" htmlUrl="https://github.com/traefik/traefik" xmlUrl="https://github.com/traefik/traefik/releases.atom" />
    <outline text="tsdproxy" title="tsdproxy" type="rss" htmlUrl="https://github.com/almeidapaulopt/tsdproxy" xmlUrl="https://github.com/almeidapaulopt/tsdproxy/releases.atom" />
    <outline text="uptime_kuma" title="uptime_kuma" type="rss" htmlUrl="https://github.com/louislam/uptime-kuma" xmlUrl="https://github.com/louislam/uptime-kuma/releases.atom" />
    <outline text="valkey" title="valkey" type="rss" htmlUrl="https://github.com/valkey-io/valkey" xmlUrl="https://github.com/valkey-io/valkey/releases.atom" />
    <outline text="vaultwarden" title="vaultwarden" type="rss" htmlUrl="https://github.com/dani-garcia/vaultwarden" xmlUrl="https://github.com/dani-garcia/vaultwarden/releases.atom" />
    <outline text="wetty" title="wetty" type="rss" htmlUrl="https://github.com/butlerx/wetty" xmlUrl="https://github.com/butlerx/wetty/releases.atom" />
    <outline text="wg_easy" title="wg_easy" type="rss" htmlUrl="https://github.com/wg-easy/wg-easy" xmlUrl="https://github.com/wg-easy/wg-easy/releases.atom" />
    <outline text="woodpecker_ci_agent" title="woodpecker_ci_agent" type="rss" htmlUrl="https://github.com/woodpecker-ci/woodpecker" xmlUrl="https://github.com/woodpecker-ci/woodpecker/releases.atom" />
    <outline text="woodpecker_ci_server" title="woodpecker_ci_server" type="rss" htmlUrl="https://github.com/woodpecker-ci/woodpecker" xmlUrl="https://github.com/woodpecker-ci/woodpecker/releases.atom" />
    <outline text="writefreely" title="writefreely" type="rss" htmlUrl="https://github.com/writefreely/writefreely" xmlUrl="https://github.com/writefreely/writefreely/releases.atom" />
  </body>
</opml>
````

## File: VERSIONS.md
````markdown
* Adguard Home: v0.107.57
* Apisix Dashboard: 3.0.1
* Apisix Gateway: 3.8.0
* Appsmith: v1.9.50
* Authelia: 4.37.5
* Authentik: 2025.2.1
* Borg: 1.4.0
* Borgmatic: 1.9.13
* Calibre Web: 0.6.24
* Changedetection: 0.48.04
* Changedetection Playwright Driver: latest
* Clickhouse: 24.8.4.13
* Collabora Online: 24.04.3.1.1
* Container Socket Proxy: 0.3.0
* Couchdb: 3.4.2
* Docker Compose: v2.32.1
* Docker Registry: 2.8.3
* Docker Registry Browser: 1.7.2
* Docker Registry Proxy: v1.2.4
* Docker Registry Purger: 1.0.0
* Dokuwiki: 2024-02-06b
* Echoip: latest
* Endlessh: 2024.0119.1
* Etcd: 3.5.11
* Etherpad: 2.2.7
* Exim Relay: 4.98.1-r0-2
* Firezone: 0.7.36
* Focalboard: 7.10.4
* Forgejo: 10.0.1
* Forgejo Runner: 6.2.0
* Freescout: 1.17.110
* Freshrss: 1.25.0
* Funkwhale: 1.4.0
* Gitea: 1.23.5
* Gotosocial: 0.17.4
* Grafana: 11.5.2
* Headscale: v0.25.1
* Healthchecks: v3.9
* Hubsite Nginx: 1.27.4
* Ilmo: 1.0.4
* Infisical: v0.43.19
* Influxdb: 2.7.6
* Jackett: 0.22.1377
* Jitsi: stable-10078-1
* Jitsi Ldap: 3
* Jitsi Prosody Auth Matrix User Verification Repo: 2839499cb03894d8cfc3e5b2219441427cb133d8
* Keycloak: 26.1.3
* Keydb: 6.3.4
* Labelstudio: latest
* Lago: v0.50.0-beta
* Languagetool: 6.5-dockerupdate-1
* Linkding: 1.39.1
* Loki: 2.9.4
* Matterbridge: 1.26.0
* Miniflux: 2.2.4
* Mobilizon: 4.1.0
* Mongodb: 7.0.4
* Mosquitto: 2.0.15
* Mrs: v0.1.0
* N8N: next
* Navidrome: 0.55.0
* Neko: firefox
* Netbox: v3.7.0-2.8.0
* Netbox Container Image Customizations Keycloak Sso Expiration Middleware: a2ac39b1c73a50742c6e834e89162f87528c7f73
* Nextcloud: 30.0.6
* Notfellchen: 0.1.1
* Notfellchen Sws: 2
* Ntfy: v2.11.0
* Oauth2 Proxy: v7.6.0
* Outline: 0.82.0
* Overseerr: 1.33.2
* Owncast: 0.2.1
* Oxitraffic: 0.10.1
* Paperless: 2.13.2
* Paperless Gotenberg: 8.12.0
* Paperless Tika: 3.0.0.0
* Peertube: v7.0.1
* Plausible: v2.1.5
* Prometheus: v2.55.1
* Prometheus Blackbox Exporter: v0.25.0
* Prometheus Node Exporter: v1.8.2
* Prometheus Postgres Exporter: v0.14.0
* Prometheus Ssh Exporter: v1.5.0
* Promtail: 2.9.5
* Qbittorrent: 5.0.4
* Radarr: 5.18.4
* Radicale: 3.4.1.1
* Readeck: 0.17.1
* Redis: 7.4.2
* Redmine: 6.0.4
* Roundcube: 1.6.10
* Rumqttd: 0.21.0
* Searxng: latest
* Semaphore: 2.9.56
* Soft Serve: v0.4.7
* Sonarr: 4.0.12
* Stirling Pdf: 0.42.0-fat
* Syncthing: 1.29.3
* Tandoor Api: 1.5.31
* Tandoor Frontend: 1.27-alpine
* Telegraf: 1.30.2
* Traefik: v3.3.4
* Tsdproxy: 1.4.4
* Uptime Kuma: 1.23.16
* Valkey: 8.0.1
* Vaultwarden: 1.33.2
* Versatiles: 0.12.6
* Wetty: 2.5
* Wg Easy: 15.0.0-beta.7
* Woodpecker Ci Agent: v3.1.0
* Woodpecker Ci Server: v3.1.0
* Wordpress: 6.6.2
* Writefreely: v0.15.0
````
